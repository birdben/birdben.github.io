<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>birdben</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="birdben">
<meta property="og:url" content="https://github.com/birdben/page/7/index.html">
<meta property="og:site_name" content="birdben">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="birdben">
  
    <link rel="alternative" href="/atom.xml" title="birdben" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
<script type="text/javascript">
var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1260188951'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s4.cnzz.com/z_stat.php%3Fid%3D1260188951' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/images/logo.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">birdben</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						<li>Links</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/AWK/" style="font-size: 10.83px;">AWK</a> <a href="/tags/Akka/" style="font-size: 10.83px;">Akka</a> <a href="/tags/Dockerfile/" style="font-size: 20px;">Dockerfile</a> <a href="/tags/Docker命令/" style="font-size: 19.17px;">Docker命令</a> <a href="/tags/Docker环境/" style="font-size: 15px;">Docker环境</a> <a href="/tags/ELK/" style="font-size: 16.67px;">ELK</a> <a href="/tags/ElasticSearch/" style="font-size: 10.83px;">ElasticSearch</a> <a href="/tags/Elasticsearch/" style="font-size: 12.5px;">Elasticsearch</a> <a href="/tags/Flume/" style="font-size: 17.5px;">Flume</a> <a href="/tags/Git命令/" style="font-size: 13.33px;">Git命令</a> <a href="/tags/Go/" style="font-size: 14.17px;">Go</a> <a href="/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/tags/HDFS/" style="font-size: 18.33px;">HDFS</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Hadoop原理架构体系/" style="font-size: 13.33px;">Hadoop原理架构体系</a> <a href="/tags/Hive/" style="font-size: 16.67px;">Hive</a> <a href="/tags/JVM/" style="font-size: 11.67px;">JVM</a> <a href="/tags/Java-Web，Socket，Python/" style="font-size: 10px;">Java Web，Socket，Python</a> <a href="/tags/Jenkins环境/" style="font-size: 10px;">Jenkins环境</a> <a href="/tags/Kafka/" style="font-size: 15.83px;">Kafka</a> <a href="/tags/Kibana/" style="font-size: 14.17px;">Kibana</a> <a href="/tags/Linux命令/" style="font-size: 12.5px;">Linux命令</a> <a href="/tags/Logstash/" style="font-size: 15.83px;">Logstash</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/MapReduce/" style="font-size: 11.67px;">MapReduce</a> <a href="/tags/Maven配置/" style="font-size: 11.67px;">Maven配置</a> <a href="/tags/MongoDB/" style="font-size: 11.67px;">MongoDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Nginx/" style="font-size: 10px;">Nginx</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/Shell/" style="font-size: 16.67px;">Shell</a> <a href="/tags/Spring/" style="font-size: 10.83px;">Spring</a> <a href="/tags/Storm/" style="font-size: 12.5px;">Storm</a> <a href="/tags/Zookeeper/" style="font-size: 12.5px;">Zookeeper</a> <a href="/tags/其他/" style="font-size: 10px;">其他</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://blog.csdn.net/birdben">我的CSDN的博客</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">birdben</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="/images/logo.png" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">birdben</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-Kafka/Kafka学习（一）Kafka环境搭建" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/09/Kafka/Kafka学习（一）Kafka环境搭建/" class="article-date">
  	<time datetime="2016-10-09T02:57:43.000Z" itemprop="datePublished">2016-10-09</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/09/Kafka/Kafka学习（一）Kafka环境搭建/">Kafka学习（一）Kafka环境搭建</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Kafka安装"><a href="#Kafka安装" class="headerlink" title="Kafka安装"></a>Kafka安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ wget http://apache.fayea.com/kafka/0.10.0.0/kafka_2.11-0.10.0.0.tgz</div><div class="line">$ tar -xzf kafka_2.11-0.10.0.0.tgz</div><div class="line">$ mv kafka_2.11-0.10.0.0 kafka_2.11</div><div class="line">$ cd kafka_2.11</div></pre></td></tr></table></figure>
<h3 id="启动Kafka单节点模式"><a href="#启动Kafka单节点模式" class="headerlink" title="启动Kafka单节点模式"></a>启动Kafka单节点模式</h3><p>在启动Kafka之前需要先启动Zookeeper，因为Kafka集群是依赖于Zookeeper服务的。如果没有外置的Zookeeper集群服务可以使用Kafka内置的Zookeeper实例</p>
<h5 id="启动Kafka内置的Zookeeper"><a href="#启动Kafka内置的Zookeeper" class="headerlink" title="启动Kafka内置的Zookeeper"></a>启动Kafka内置的Zookeeper</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ ./bin/zookeeper-server-start.sh config/zookeeper.properties</div><div class="line">[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)</div><div class="line">...</div></pre></td></tr></table></figure>
<p>这里我们使用我们自己的Zookeeper集群，所以直接启动我们搭建好的Zookeeper集群</p>
<h5 id="启动外置的Zookeeper集群"><a href="#启动外置的Zookeeper集群" class="headerlink" title="启动外置的Zookeeper集群"></a>启动外置的Zookeeper集群</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 分别启动Hadoop1，Hadoop2，Hadoop3三台服务器的Zookeeper服务</div><div class="line">$ ./bin/zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /data/zookeeper-3.4.8/bin/../conf/zoo.cfgStarting zookeeper ... already running as process 4468.</div><div class="line"></div><div class="line"># 分别查看一下Zookeeper服务的状态</div><div class="line">$ ./bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /data/zookeeper-3.4.8/bin/../conf/zoo.cfgMode: leader</div></pre></td></tr></table></figure>
<h5 id="修改server-properties配置文件"><a href="#修改server-properties配置文件" class="headerlink" title="修改server.properties配置文件"></a>修改server.properties配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># 添加外置的Zookeeper集群配置zookeeper.connect=10.10.1.64:2181,10.10.1.94:2181,10.10.1.95:2181</div></pre></td></tr></table></figure>
<h5 id="启动Kafka"><a href="#启动Kafka" class="headerlink" title="启动Kafka"></a>启动Kafka</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/kafka-server-start.sh config/server.properties</div></pre></td></tr></table></figure>
<h5 id="创建Topic"><a href="#创建Topic" class="headerlink" title="创建Topic"></a>创建Topic</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 创建Topic test1</div><div class="line">$ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test1Created topic &quot;test1&quot;.</div><div class="line"></div><div class="line"># 查看我们所有的Topic，可以看到test1</div><div class="line">$ ./bin/kafka-topics.sh --list --zookeeper localhost:2181__consumer_offsetsconnect-testkafka_testmy-replicated-topicstreams-file-inputtest1</div><div class="line"></div><div class="line"># 通过ZK的客户端连接到Zookeeper服务，localhost可以替换成Zookeeper集群的任意节点（10.10.1.64，10.10.1.94，10.10.1.95），当前localhost是10.10.1.64机器</div><div class="line">$ ./bin/zkCli.sh -server localhost:2181</div><div class="line"></div><div class="line"># 可以在Zookeeper中查看到新创建的Topic test1</div><div class="line">[zk: localhost:2181(CONNECTED) 5] ls /brokers/topics[kafka_test, test1, streams-file-input, __consumer_offsets, connect-test, my-replicated-topic]</div></pre></td></tr></table></figure>
<h5 id="启动producer服务，向test1的Topic中发送消息"><a href="#启动producer服务，向test1的Topic中发送消息" class="headerlink" title="启动producer服务，向test1的Topic中发送消息"></a>启动producer服务，向test1的Topic中发送消息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test1</div><div class="line">this is a message</div><div class="line">this is another message</div><div class="line">still a message</div></pre></td></tr></table></figure>
<h5 id="启动consumer服务，从test1的Topic中接收消息"><a href="#启动consumer服务，从test1的Topic中接收消息" class="headerlink" title="启动consumer服务，从test1的Topic中接收消息"></a>启动consumer服务，从test1的Topic中接收消息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test1 --from-beginningthis is a messagethis is another messagestill a message</div></pre></td></tr></table></figure>
<h3 id="启动Kafka集群模式"><a href="#启动Kafka集群模式" class="headerlink" title="启动Kafka集群模式"></a>启动Kafka集群模式</h3><p>以上是Kafka单节点模式启动，集群模式启动只需要启动多个Kafka broker，我们这里部署了三个Kafka<br>broker，分别在10.10.1.64，10.10.1.94，10.10.1.95三台机器上</p>
<h5 id="修改server-properties配置文件-1"><a href="#修改server-properties配置文件-1" class="headerlink" title="修改server.properties配置文件"></a>修改server.properties配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 分别在10.10.1.64，10.10.1.94，10.10.1.95三台机器上的配置文件设置broker.id为0，1，2</div><div class="line"># broker.id是用来唯一标识Kafka集群节点的</div><div class="line">broker.id=1</div></pre></td></tr></table></figure>
<h5 id="分别启动三台机器的Kafka服务"><a href="#分别启动三台机器的Kafka服务" class="headerlink" title="分别启动三台机器的Kafka服务"></a>分别启动三台机器的Kafka服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/kafka-server-start.sh config/server.properties &amp;</div></pre></td></tr></table></figure>
<h5 id="创建Topic-1"><a href="#创建Topic-1" class="headerlink" title="创建Topic"></a>创建Topic</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 创建新的Topic kafka_cluster_topic</div><div class="line">$ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic kafka_cluster_topic</div><div class="line"></div><div class="line"># 查看Topic kafka_cluster_topic的状态，发现Leader是1（broker.id=1）,有三个备份分别是0，1，2</div><div class="line">$ ./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic kafka_cluster_topicTopic:kafka_cluster_topic	PartitionCount:1	ReplicationFactor:3	Configs:	Topic: kafka_cluster_topic	Partition: 0	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</div><div class="line">	</div><div class="line"># 再次查看原来的Topic test1，发现Leader是0（broker.id=0）,因为我们之前单节点是在broker.id=0这台服务器（10.10.1.64）上运行的，因为当时只有这一个节点，所以leader一定是0</div><div class="line">$ ./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test1Topic:test1	PartitionCount:1	ReplicationFactor:1	Configs:	Topic: test1	Partition: 0	Leader: 0	Replicas: 0	Isr: 0</div><div class="line">	</div><div class="line"># leader：是随机挑选出来的</div><div class="line"># replicas：是负责同步leader的log的备份节点列表</div><div class="line"># isr：是备份节点列表的子集，表示正在进行同步log的工作状态的节点列表</div></pre></td></tr></table></figure>
<h5 id="启动producer服务，向kafka-cluster-topic的Topic中发送消息"><a href="#启动producer服务，向kafka-cluster-topic的Topic中发送消息" class="headerlink" title="启动producer服务，向kafka_cluster_topic的Topic中发送消息"></a>启动producer服务，向kafka_cluster_topic的Topic中发送消息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic kafka_cluster_topic</div><div class="line">this is a message</div><div class="line">my name is birdben</div></pre></td></tr></table></figure>
<h5 id="启动consumer服务，从kafka-cluster-topic的Topic中接收消息"><a href="#启动consumer服务，从kafka-cluster-topic的Topic中接收消息" class="headerlink" title="启动consumer服务，从kafka_cluster_topic的Topic中接收消息"></a>启动consumer服务，从kafka_cluster_topic的Topic中接收消息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic kafka_cluster_topic --from-beginningthis is a message</div><div class="line">my name is birdben</div></pre></td></tr></table></figure>
<h5 id="停止leader-1的Kafka服务（10-10-1-94）"><a href="#停止leader-1的Kafka服务（10-10-1-94）" class="headerlink" title="停止leader=1的Kafka服务（10.10.1.94）"></a>停止leader=1的Kafka服务（10.10.1.94）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># 停止leader的Kafka服务之后，再次查看Topic kafka_cluster_topic的状态</div><div class="line"># 这时候会发现Leader已经变成0了，而且Isr列表中已经没有1了，说明1的Kafka的备份服务已经停止不工作了</div><div class="line">$ ./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic kafka_cluster_topicTopic:kafka_cluster_topic	PartitionCount:1	ReplicationFactor:3	Configs:	Topic: kafka_cluster_topic	Partition: 0	Leader: 0	Replicas: 1,0,2	Isr: 0,2</div><div class="line">	</div><div class="line"># 但是此时我们仍然可以在0，2两个Kafka节点接收消息</div><div class="line">$ ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic kafka_cluster_topic</div><div class="line">this is a messagebirdben</div></pre></td></tr></table></figure>
<p>刚开始接触Kafka，所以只是按照官网的示例简单安装了环境，后续会随着深入使用更新复杂的配置和用法</p>
<p>参考文章：</p>
<ul>
<li><a href="http://kafka.apache.org/quickstart" target="_blank" rel="external">http://kafka.apache.org/quickstart</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/MQ/">MQ</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Flume/Flume学习（十）Flume整合HDFS（二）" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/23/Flume/Flume学习（十）Flume整合HDFS（二）/" class="article-date">
  	<time datetime="2016-09-23T06:09:26.000Z" itemprop="datePublished">2016-09-23</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/23/Flume/Flume学习（十）Flume整合HDFS（二）/">Flume学习（十）Flume整合HDFS（二）</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上一篇介绍了Flume整合HDFS，但是没有对HDFS Sink进行配置上的优化，本篇重点介绍HDFS Sink的相关配置。</p>
<p>上一篇中我们用Flume采集的日志直接输出到HDFS文件中，但是文件的输出的文件大小</p>
<h4 id="优化后的flume-collector-hdfs-conf配置文件"><a href="#优化后的flume-collector-hdfs-conf配置文件" class="headerlink" title="优化后的flume_collector_hdfs.conf配置文件"></a>优化后的flume_collector_hdfs.conf配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div></pre></td><td class="code"><pre><div class="line">agentX.sources = flume-avro-sink</div><div class="line">agentX.channels = chX</div><div class="line">agentX.sinks = flume-hdfs-sink</div><div class="line"></div><div class="line">agentX.sources.flume-avro-sink.channels = chX</div><div class="line">agentX.sources.flume-avro-sink.type = avro</div><div class="line">agentX.sources.flume-avro-sink.bind = 127.0.0.1</div><div class="line">agentX.sources.flume-avro-sink.port = 41414</div><div class="line">agentX.sources.flume-avro-sink.threads = 8</div><div class="line"></div><div class="line"># 定义拦截器，为消息添加时间戳和Host地址</div><div class="line">agentX.sources.flume-avro-sink.interceptors = i1 i2</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i1.type = timestamp</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.type = host</div><div class="line"># 如果不指定hostHeader，就是用%&#123;host&#125;。但是指定了hostHeader=hostname，就需要使用%&#123;hostname&#125;</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.hostHeader = hostname</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.preserveExisting = true</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.useIP = true</div><div class="line"></div><div class="line">agentX.channels.chX.type = memory</div><div class="line">agentX.channels.chX.capacity = 1000</div><div class="line">agentX.channels.chX.transactionCapacity = 100</div><div class="line"></div><div class="line">agentX.sinks.flume-hdfs-sink.type = hdfs</div><div class="line">agentX.sinks.flume-hdfs-sink.channel = chX</div><div class="line"></div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.path = hdfs://10.10.1.64:8020/flume/events/</div><div class="line"># 使用时间作为分割目录</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.path = hdfs://10.10.1.64:8020/flume/events/%Y%m%d/</div><div class="line"></div><div class="line"># HdfsEventSink中，hdfs.fileType默认为SequenceFile，将其改为DataStream就可以按照采集的文件原样输入到hdfs，加一行agentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStream</div><div class="line"># 设置文件格式， 有3种格式可选择：SequenceFile, DataStream or CompressedStream</div><div class="line"># 当使用DataStream时候，文件不会被压缩，不需要设置hdfs.codeC</div><div class="line"># 当使用CompressedStream时候，必须设置一个正确的hdfs.codeC值</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStream</div><div class="line"></div><div class="line"># 写入hdfs的文件名前缀，可以使用flume提供的日期及%&#123;host&#125;表达式。默认值：FlumeData</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.filePrefix = events-%&#123;hostname&#125;-</div><div class="line"># 写入hdfs的文件名后缀，比如：.lzo .log等。</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.fileSuffix = .log</div><div class="line"></div><div class="line"># 临时文件的文件名前缀，hdfs sink会先往目标目录中写临时文件，再根据相关规则重命名成最终目标文件</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.inUsePrefix</div><div class="line"># 临时文件的文件名后缀。默认值：.tmp</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.inUseSuffix</div><div class="line"></div><div class="line"># 当目前被打开的临时文件在该参数指定的时间（秒）内，没有任何数据写入，则将该临时文件关闭并重命名成目标文件。默认值是0</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.idleTimeout = 0</div><div class="line"># 文件压缩格式，包括：gzip, bzip2, lzo, lzop, snappy</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.codeC = gzip</div><div class="line"># 每个批次刷新到HDFS上的events数量。默认值：100</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.batchSize = 100</div><div class="line"></div><div class="line"># 不想每次Flume将日志写入到HDFS文件中都分成很多个碎小的文件，这里控制HDFS的滚动</div><div class="line"># 注：滚动（roll）指的是，hdfs sink将临时文件重命名成最终目标文件，并新打开一个临时文件来写入数据；</div><div class="line"># 设置间隔多长将临时文件滚动成最终目标文件。单位是秒，默认30秒。</div><div class="line"># 如果设置为0的话表示不根据时间滚动hdfs文件</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollInterval = 0</div><div class="line"># 当临时文件达到该大小（单位：bytes）时，滚动成目标文件。默认值1024，单位是字节。</div><div class="line"># 如果设置为0的话表示不基于文件大小滚动hdfs文件</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollSize = 0</div><div class="line"># 设置当events数据达到该数量时候，将临时文件滚动成目标文件。默认值是10个。</div><div class="line"># 如果设置为0的话表示不基于事件个数滚动hdfs文件</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollCount = 300</div><div class="line"></div><div class="line"># 是否启用时间上的”舍弃”，这里的”舍弃”，类似于”四舍五入”，后面再介绍。如果启用，则会影响除了%t的其他所有时间表达式</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.round = true</div><div class="line"># 时间上进行“舍弃”的值。默认值：1</div><div class="line"># 举例：当时间为2015-10-16 17:38:59时候，hdfs.path依然会被解析为：/flume/events/20151016/17:30/00</div><div class="line"># 因为设置的是舍弃10分钟内的时间，因此，该目录每10分钟新生成一个。</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.roundValue = 10</div><div class="line"># 时间上进行”舍弃”的单位，包含：second,minute,hour。默认值：seconds</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.roundUnit = minute</div><div class="line"></div><div class="line"># 写入HDFS文件块的最小副本数。默认值：HDFS副本数</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.minBlockReplicas</div><div class="line"># 最大允许打开的HDFS文件数，当打开的文件数达到该值，最早打开的文件将会被关闭。默认值：5000</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.maxOpenFiles</div><div class="line"># 执行HDFS操作的超时时间（单位：毫秒）。默认值：10000</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.callTimeout</div><div class="line"># hdfs sink启动的操作HDFS的线程数。默认值：10</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.threadsPoolSize</div><div class="line"># 时区。默认值：Local Time</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.timeZone</div><div class="line"># 是否使用当地时间。默认值：flase</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.useLocalTimeStamp</div><div class="line"># hdfs sink关闭文件的尝试次数。默认值：0</div><div class="line"># 如果设置为1，当一次关闭文件失败后，hdfs sink将不会再次尝试关闭文件，这个未关闭的文件将会一直留在那，并且是打开状态。</div><div class="line"># 设置为0，当一次关闭失败后，hdfs sink会继续尝试下一次关闭，直到成功。</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.closeTries</div><div class="line"># hdfs sink尝试关闭文件的时间间隔，如果设置为0，表示不尝试，相当于于将hdfs.closeTries设置成1。默认值：180（秒）</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.retryInterval</div><div class="line"># 序列化类型。其他还有：avro_event或者是实现了EventSerializer.Builder的类名。默认值：TEXT</div><div class="line"># agentX.sinks.flume-hdfs-sink.hdfs.serializer</div></pre></td></tr></table></figure>
<p>注意：hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount这3个参数尤为重要，因为这三个参数是控制HDFS文件滚动的，如果想要按照自己的方式做HDFS文件滚动必须三个参数都需要设置，我这里是按照300个Event来做HDFS文件滚动的，如果仅仅设置hdfs.rollCount一个参数是不起作用的，因为其他两个参数按照默认值还是会生效，如果只希望其中某些参数起作用，最好禁用其他的参数。</p>
<h4 id="在HDFS中查看"><a href="#在HDFS中查看" class="headerlink" title="在HDFS中查看"></a>在HDFS中查看</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">$ hdfs dfs -ls /flume/events/</div><div class="line">16/09/23 14:43:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">Found 1 items</div><div class="line">drwxr-xr-x   - yunyu supergroup          0 2016-09-23 14:42 /flume/events/20160923</div><div class="line"></div><div class="line">$ hdfs dfs -ls /flume/events/20160923/</div><div class="line">16/09/23 14:43:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   1 yunyu supergroup      92900 2016-09-23 14:42 /flume/events/20160923/events-.1474612925442</div><div class="line">-rw-r--r--   1 yunyu supergroup       5880 2016-09-23 14:42 /flume/events/20160923/events-.1474612925443.tmp</div><div class="line">-rw-r--r--   1 yunyu supergroup      92900 2016-09-23 14:42 /flume/events/20160923/events-.1474612930367</div><div class="line">-rw-r--r--   1 yunyu supergroup      19193 2016-09-23 14:42 /flume/events/20160923/events-.1474612930368.tmp</div><div class="line"></div><div class="line"># 使用hostname作为前缀，这里的127.0.0.1应该是从/etc/hosts配置文件中读取的</div><div class="line">$ hdfs dfs -ls /flume/events/20160923</div><div class="line">16/09/23 18:01:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   1 yunyu supergroup      92900 2016-09-23 18:00 /flume/events/20160923/events-127.0.0.1-.1474624778493</div><div class="line">-rw-r--r--   1 yunyu supergroup      25083 2016-09-23 18:00 /flume/events/20160923/events-127.0.0.1-.1474624778494.tmp</div><div class="line">-rw-r--r--   1 yunyu supergroup      92900 2016-09-23 18:00 /flume/events/20160923/events-127.0.0.1-.1474624788628</div><div class="line">-rw-r--r--   1 yunyu supergroup       5881 2016-09-23 18:00 /flume/events/20160923/events-127.0.0.1-.1474624788629.tmp</div></pre></td></tr></table></figure>
<h4 id="遇到的问题和解决方法"><a href="#遇到的问题和解决方法" class="headerlink" title="遇到的问题和解决方法"></a>遇到的问题和解决方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">2016-09-23 14:40:16,810 (SinkRunner-PollingRunner-DefaultSinkProcessor) [ERROR - org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:160)] Unable to deliver event. Exception follows.</div><div class="line">org.apache.flume.EventDeliveryException: java.lang.NullPointerException: Expected timestamp in the Flume event headers, but it was null</div><div class="line">	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:463)</div><div class="line">	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)</div><div class="line">	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)</div><div class="line">	at java.lang.Thread.run(Thread.java:745)</div><div class="line">Caused by: java.lang.NullPointerException: Expected timestamp in the Flume event headers, but it was null</div><div class="line">	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:226)</div><div class="line">	at org.apache.flume.formatter.output.BucketPath.replaceShorthand(BucketPath.java:228)</div><div class="line">	at org.apache.flume.formatter.output.BucketPath.escapeString(BucketPath.java:432)</div><div class="line">	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:380)</div><div class="line">	... 3 more</div></pre></td></tr></table></figure>
<p>遇到上面的问题是因为写入到HDFS时，使用到了时间戳来区分目录结构，Flume的消息组件Event在接受到之后在Header中没有发现时间戳参数，导致该错误发生，有三种方法可以解决这个错误；</p>
<ul>
<li>在Source中设置拦截器，为每条Event头中加入时间戳（效率会慢一些）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">agentX.sources.flume-avro-sink.interceptors = i1</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i1.type = timestamp</div></pre></td></tr></table></figure>
<ul>
<li>设置使用本地的时间戳（如果客户端和flume集群时间不一致数据时间会不准确）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 为sink指定该参数为true</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.useLocalTimeStamp = true</div></pre></td></tr></table></figure>
<ul>
<li>在数据源头解决，在日志Event的Head中添加时间戳再再送到Flume（推荐使用）</li>
</ul>
<p>在向Source发送Event时，将时间戳参数添加到Event的Header中即可，Header是一个Map，添加时MapKey为timestamp</p>
<p>参考文章：</p>
<ul>
<li><a href="http://flume.apache.org/FlumeUserGuide.html#hdfs-sink" target="_blank" rel="external">http://flume.apache.org/FlumeUserGuide.html#hdfs-sink</a></li>
<li><a href="http://lxw1234.com/archives/2015/10/527.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/10/527.htm</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_db77b3c60102vrzt.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_db77b3c60102vrzt.html</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Flume/Flume学习（九）Flume整合HDFS（一）" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/22/Flume/Flume学习（九）Flume整合HDFS（一）/" class="article-date">
  	<time datetime="2016-09-22T10:35:32.000Z" itemprop="datePublished">2016-09-22</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/22/Flume/Flume学习（九）Flume整合HDFS（一）/">Flume学习（九）Flume整合HDFS（一）</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="环境简介"><a href="#环境简介" class="headerlink" title="环境简介"></a>环境简介</h3><ul>
<li>JDK1.7.0_79</li>
<li>Flume1.6.0</li>
<li>Hadoop2.7.1</li>
</ul>
<p>之前介绍了Flume整合ES，本篇主要介绍Flume整合HDFS，将日志内容通过Flume传输给Hadoop，并且保存成文件存储在HDFS上。</p>
<h3 id="需要依赖Hadoop的jar包"><a href="#需要依赖Hadoop的jar包" class="headerlink" title="需要依赖Hadoop的jar包"></a>需要依赖Hadoop的jar包</h3><p>下面的jar包路径根据自己的实际环境情况修改。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">cp ~/Downloads/develop/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar ~/dev/flume-1.6.0/lib</div><div class="line">cp ~/Downloads/develop/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar ~/dev/flume-1.6.0/lib</div><div class="line">cp ~/Downloads/develop/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar ~/dev/flume-1.6.0/lib</div><div class="line">cp ~/Downloads/develop/hadoop-2.7.1/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/hadoop-hdfs-2.7.1.jar ~/dev/flume-1.6.0/lib</div><div class="line">cp ~/Downloads/develop/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar ~/dev/flume-1.6.0/lib</div><div class="line"># 覆盖已有的commons-io.jar</div><div class="line">cp ~/Downloads/develop/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar ~/dev/flume-1.6.0/lib</div></pre></td></tr></table></figure>
<h3 id="command-log日志文件"><a href="#command-log日志文件" class="headerlink" title="command.log日志文件"></a>command.log日志文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div></pre></td></tr></table></figure>
<h3 id="Flume相关配置"><a href="#Flume相关配置" class="headerlink" title="Flume相关配置"></a>Flume相关配置</h3><h4 id="Flume-Agent端的flume-agent-file-conf配置"><a href="#Flume-Agent端的flume-agent-file-conf配置" class="headerlink" title="Flume Agent端的flume_agent_file.conf配置"></a>Flume Agent端的flume_agent_file.conf配置</h4><p>这里是采集/Users/yunyu/Downloads/command.log日志文件的内容，并且上报到127.0.0.1:41414服务器上（也就是Flume Collector端）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">agent3.sources = command-logfile-source</div><div class="line">agent3.channels = ch3</div><div class="line">agent3.sinks = flume-avro-sink</div><div class="line"></div><div class="line">agent3.sources.command-logfile-source.channels = ch3</div><div class="line">agent3.sources.command-logfile-source.type = exec</div><div class="line">agent3.sources.command-logfile-source.command = tail -F /Users/yunyu/Downloads/command.log</div><div class="line"></div><div class="line">agent3.channels.ch3.type = memory</div><div class="line">agent3.channels.ch3.capacity = 1000</div><div class="line">agent3.channels.ch3.transactionCapacity = 100</div><div class="line"></div><div class="line">agent3.sinks.flume-avro-sink.channel = ch3</div><div class="line">agent3.sinks.flume-avro-sink.type = avro</div><div class="line">agent3.sinks.flume-avro-sink.hostname = 127.0.0.1</div><div class="line">agent3.sinks.flume-avro-sink.port = 41414</div></pre></td></tr></table></figure>
<h4 id="Flume-Collector端的flume-collector-hdfs-conf配置"><a href="#Flume-Collector端的flume-collector-hdfs-conf配置" class="headerlink" title="Flume Collector端的flume_collector_hdfs.conf配置"></a>Flume Collector端的flume_collector_hdfs.conf配置</h4><p>这里监听到127.0.0.1:41414上报的内容，并且输出到HDFS中，这里需要指定HDFS的文件路径。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">agentX.sources = flume-avro-sink</div><div class="line">agentX.channels = chX</div><div class="line">agentX.sinks = flume-hdfs-sink</div><div class="line"></div><div class="line">agentX.sources.flume-avro-sink.channels = chX</div><div class="line">agentX.sources.flume-avro-sink.type = avro</div><div class="line">agentX.sources.flume-avro-sink.bind = 127.0.0.1</div><div class="line">agentX.sources.flume-avro-sink.port = 41414</div><div class="line">agentX.sources.flume-avro-sink.threads = 8</div><div class="line"></div><div class="line">agentX.channels.chX.type = memory</div><div class="line">agentX.channels.chX.capacity = 1000</div><div class="line">agentX.channels.chX.transactionCapacity = 100</div><div class="line"></div><div class="line">agentX.sinks.flume-hdfs-sink.type = hdfs</div><div class="line">agentX.sinks.flume-hdfs-sink.channel = chX</div><div class="line">#agentX.sinks.flume-hdfs-sink.hdfs.path = hdfs://10.10.1.64:8020/flume/events/%y-%m-%d/%H%M/%S</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.path = hdfs://10.10.1.64:8020/flume/events/</div><div class="line"># HdfsEventSink中，hdfs.fileType默认为SequenceFile，将其改为DataStream就可以按照采集的文件原样输入到hdfs，加一行agentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStream</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStream</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.filePrefix = events-</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.round = true</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.roundValue = 10</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.roundUnit = minute</div></pre></td></tr></table></figure>
<h4 id="启动Flume"><a href="#启动Flume" class="headerlink" title="启动Flume"></a>启动Flume</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 启动Flume收集端</div><div class="line">$ ./bin/flume-ng agent --conf ./conf/ -f conf/flume_collector_hdfs.conf -Dflume.root.logger=DEBUG,console -n agentX</div><div class="line"></div><div class="line"># 启动Flume采集端，发送数据到Collector测试</div><div class="line">$ ./bin/flume-ng agent --conf ./conf/ -f conf/flume_agent_file.conf -Dflume.root.logger=DEBUG,console -n agent3</div></pre></td></tr></table></figure>
<p>这里遇到个小问题，就是Flume收集的日志文件到HDFS上查看有乱码，具体查看HDFS文件内容如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ hdfs dfs -cat /flume/events/events-.1474337184903</div><div class="line">SEQ!org.apache.hadoop.io.LongWritable&quot;org.apache.hadoop.io.BytesWritable�w�x0�\����WEX&quot;Ds &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Fs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Gs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Gs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Hs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Hs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Hs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Is &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Is &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;</div></pre></td></tr></table></figure>
<p>解决方式：HdfsEventSink中，hdfs.fileType默认为SequenceFile，将其改为DataStream就可以按照采集的文件原样输入到hdfs，加一行agentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStream，如果不改就会出现HDFS文件乱码问题。</p>
<h4 id="在HDFS中查看日志文件"><a href="#在HDFS中查看日志文件" class="headerlink" title="在HDFS中查看日志文件"></a>在HDFS中查看日志文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># 之前我们在Flume中配置了采集到的日志输出到HDFS的保存路径是hdfs://10.10.1.64:8020/flume/events/</div><div class="line"></div><div class="line"># 查看HDFS文件存储路径</div><div class="line">$ hdfs dfs -ls /flume/events/</div><div class="line">Found 2 items-rw-r--r--   3 yunyu supergroup       1134 2016-09-19 23:43 /flume/events/events-.1474353822776-rw-r--r--   3 yunyu supergroup        126 2016-09-19 23:44 /flume/events/events-.1474353822777</div><div class="line"></div><div class="line"># 查看HDFS文件内容</div><div class="line">$ hdfs dfs -cat /flume/events/events-.1474353822776</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">  565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">  565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="http://blog.csdn.net/cnbird2008/article/details/18967449" target="_blank" rel="external">http://blog.csdn.net/cnbird2008/article/details/18967449</a></li>
<li><a href="http://blog.csdn.net/lifuxiangcaohui/article/details/49949865" target="_blank" rel="external">http://blog.csdn.net/lifuxiangcaohui/article/details/49949865</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hive/Hive学习（二）使用Hive进行离线分析日志" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/20/Hive/Hive学习（二）使用Hive进行离线分析日志/" class="article-date">
  	<time datetime="2016-09-20T07:28:15.000Z" itemprop="datePublished">2016-09-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/20/Hive/Hive学习（二）使用Hive进行离线分析日志/">Hive学习（二）使用Hive进行离线分析日志</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>继上一篇把Hive环境安装好之后，我们要做具体的日志分析处理，这里我们的架构是使用Flume + HDFS + Hive离线分析日志。通过Flume收集日志文件中的日志，然后存储到HDFS中，在通过Hive在HDFS之上建立数据库表，进行SQL的查询分析（其实底层是mapreduce任务）。</p>
<p>这里我们还是处理之前一直使用的command.log命令行日志，先来看一下具体的日志文件格式</p>
<h3 id="command-log日志文件"><a href="#command-log日志文件" class="headerlink" title="command.log日志文件"></a>command.log日志文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div></pre></td></tr></table></figure>
<h3 id="Flume相关配置"><a href="#Flume相关配置" class="headerlink" title="Flume相关配置"></a>Flume相关配置</h3><h4 id="Flume-Agent端的flume-agent-file-conf配置"><a href="#Flume-Agent端的flume-agent-file-conf配置" class="headerlink" title="Flume Agent端的flume_agent_file.conf配置"></a>Flume Agent端的flume_agent_file.conf配置</h4><p>这里是采集/Users/yunyu/Downloads/command.log日志文件的内容，并且上报到127.0.0.1:41414服务器上（也就是Flume Collector端）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">agent3.sources = command-logfile-source</div><div class="line">agent3.channels = ch3</div><div class="line">agent3.sinks = flume-avro-sink</div><div class="line"></div><div class="line">agent3.sources.command-logfile-source.channels = ch3</div><div class="line">agent3.sources.command-logfile-source.type = exec</div><div class="line">agent3.sources.command-logfile-source.command = tail -F /Users/yunyu/Downloads/command.log</div><div class="line"></div><div class="line">agent3.channels.ch3.type = memory</div><div class="line">agent3.channels.ch3.capacity = 1000</div><div class="line">agent3.channels.ch3.transactionCapacity = 100</div><div class="line"></div><div class="line">agent3.sinks.flume-avro-sink.channel = ch3</div><div class="line">agent3.sinks.flume-avro-sink.type = avro</div><div class="line">agent3.sinks.flume-avro-sink.hostname = 127.0.0.1</div><div class="line">agent3.sinks.flume-avro-sink.port = 41414</div></pre></td></tr></table></figure>
<h4 id="Flume-Collector端的flume-collector-hdfs-conf配置"><a href="#Flume-Collector端的flume-collector-hdfs-conf配置" class="headerlink" title="Flume Collector端的flume_collector_hdfs.conf配置"></a>Flume Collector端的flume_collector_hdfs.conf配置</h4><p>这里监听到127.0.0.1:41414上报的内容，并且输出到HDFS中，这里需要指定HDFS的文件路径。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">agentX.sources = flume-avro-sink</div><div class="line">agentX.channels = chX</div><div class="line">agentX.sinks = flume-hdfs-sink</div><div class="line"></div><div class="line">agentX.sources.flume-avro-sink.channels = chX</div><div class="line">agentX.sources.flume-avro-sink.type = avro</div><div class="line">agentX.sources.flume-avro-sink.bind = 127.0.0.1</div><div class="line">agentX.sources.flume-avro-sink.port = 41414</div><div class="line">agentX.sources.flume-avro-sink.threads = 8</div><div class="line"></div><div class="line">agentX.channels.chX.type = memory</div><div class="line">agentX.channels.chX.capacity = 1000</div><div class="line">agentX.channels.chX.transactionCapacity = 100</div><div class="line"></div><div class="line">agentX.sinks.flume-hdfs-sink.type = hdfs</div><div class="line">agentX.sinks.flume-hdfs-sink.channel = chX</div><div class="line">#agentX.sinks.flume-hdfs-sink.hdfs.path = hdfs://10.10.1.64:8020/flume/events/%y-%m-%d/%H%M/%S</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.path = hdfs://10.10.1.64:8020/flume/events/</div><div class="line"># HdfsEventSink中，hdfs.fileType默认为SequenceFile，将其改为DataStream就可以按照采集的文件原样输入到hdfs，加一行agentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStream</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStream</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.filePrefix = events-</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.round = true</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.roundValue = 10</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.roundUnit = minute</div></pre></td></tr></table></figure>
<h4 id="启动Flume"><a href="#启动Flume" class="headerlink" title="启动Flume"></a>启动Flume</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 启动Flume收集端</div><div class="line">$ ./bin/flume-ng agent --conf ./conf/ -f conf/flume_collector_hdfs.conf -Dflume.root.logger=DEBUG,console -n agentX</div><div class="line"></div><div class="line"># 启动Flume采集端，发送数据到Collector测试</div><div class="line">$ ./bin/flume-ng agent --conf ./conf/ -f conf/flume_agent_file.conf -Dflume.root.logger=DEBUG,console -n agent3</div></pre></td></tr></table></figure>
<p>这里遇到个小问题，就是Flume收集的日志文件到HDFS上查看有乱码，具体查看HDFS文件内容如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ hdfs dfs -cat /flume/events/events-.1474337184903</div><div class="line">SEQ!org.apache.hadoop.io.LongWritable&quot;org.apache.hadoop.io.BytesWritable�w�x0�\����WEX&quot;Ds &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Fs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Gs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Gs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Hs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Hs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Hs &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Is &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;WEX&quot;Is &#123;&quot;TIME&quot;:&quot;2016-09-20 10:05:30&quot;,&quot;HOSTNAME&quot;:&quot;hadoop1&quot;,&quot;LI&quot;:&quot;:0&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;tailf command.log &quot;&#125;</div></pre></td></tr></table></figure>
<p>解决方式：HdfsEventSink中，hdfs.fileType默认为SequenceFile，将其改为DataStream就可以按照采集的文件原样输入到hdfs，加一行agentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStream，如果不改就会出现HDFS文件乱码问题。</p>
<p>参考文章：</p>
<ul>
<li><a href="http://blog.csdn.net/cnbird2008/article/details/18967449" target="_blank" rel="external">http://blog.csdn.net/cnbird2008/article/details/18967449</a></li>
<li><a href="http://blog.csdn.net/lifuxiangcaohui/article/details/49949865" target="_blank" rel="external">http://blog.csdn.net/lifuxiangcaohui/article/details/49949865</a></li>
</ul>
<h3 id="Hive中创建表"><a href="#Hive中创建表" class="headerlink" title="Hive中创建表"></a>Hive中创建表</h3><p>下面是具体如何在Hive中基于HDFS文件创建表的</p>
<h4 id="启动相关服务"><a href="#启动相关服务" class="headerlink" title="启动相关服务"></a>启动相关服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"># 启动hdfs服务</div><div class="line">$ ./sbin/start-dfs.sh</div><div class="line"></div><div class="line"># 启动yarn服务</div><div class="line">$ ./sbin/start-yarn.sh</div><div class="line"></div><div class="line"># 进入hive安装目录</div><div class="line">$ cd /data/hive-1.2.1</div><div class="line"></div><div class="line"># 启动metastore</div><div class="line">$ ./bin/hive --service metastore &amp;</div><div class="line"></div><div class="line"># 启动hiveserver2</div><div class="line">$ ./bin/hive --service hiveserver2 &amp;</div><div class="line"></div><div class="line"># 启动hive shell</div><div class="line">$ ./bin/hive shell</div><div class="line">hive&gt;</div><div class="line">hive&gt; show databases;</div><div class="line">OK</div><div class="line">default</div><div class="line">Time taken: 1.323 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure>
<p>如果看过上一篇Hive环境搭建的同学，到这里应该是一切正常的。如果启动metastore或者hiveserver2服务的时候遇到’MySQL: ERROR 1071 (42000): Specified key was too long; max key length is 767 bytes’错误，将MySQL元数据的hive数据库编码方式改成latin1就好了。</p>
<p>参考文章</p>
<ul>
<li><a href="http://blog.csdn.net/cindy9902/article/details/6215769" target="_blank" rel="external">http://blog.csdn.net/cindy9902/article/details/6215769</a></li>
</ul>
<h4 id="在HDFS中查看日志文件"><a href="#在HDFS中查看日志文件" class="headerlink" title="在HDFS中查看日志文件"></a>在HDFS中查看日志文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># 之前我们在Flume中配置了采集到的日志输出到HDFS的保存路径是hdfs://10.10.1.64:8020/flume/events/</div><div class="line"></div><div class="line"># 查看HDFS文件存储路径</div><div class="line">$ hdfs dfs -ls /flume/events/</div><div class="line">Found 2 items-rw-r--r--   3 yunyu supergroup       1134 2016-09-19 23:43 /flume/events/events-.1474353822776-rw-r--r--   3 yunyu supergroup        126 2016-09-19 23:44 /flume/events/events-.1474353822777</div><div class="line"></div><div class="line"># 查看HDFS文件内容</div><div class="line">$ hdfs dfs -cat /flume/events/events-.1474353822776</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">  565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">  565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div></pre></td></tr></table></figure>
<h4 id="使用org-apache-hadoop-hive-contrib-serde2-RegexSerDe解析日志"><a href="#使用org-apache-hadoop-hive-contrib-serde2-RegexSerDe解析日志" class="headerlink" title="使用org.apache.hadoop.hive.contrib.serde2.RegexSerDe解析日志"></a>使用org.apache.hadoop.hive.contrib.serde2.RegexSerDe解析日志</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"># 确认日志写入HDFS成功之后，我们需要在Hive中创建table</div><div class="line"># 启动hive shell</div><div class="line">$ ./bin/hive shell</div><div class="line"></div><div class="line"># 创建新的数据库test_hdfs</div><div class="line">hive&gt; create database test_hdfs;</div><div class="line">OKTime taken: 0.205 seconds</div><div class="line"></div><div class="line"># 使用数据库test_hdfs</div><div class="line">hive&gt; use test_hdfs;</div><div class="line"></div><div class="line"># 新建表command_test_table并且使用正则表达式提取日志文件中的字段信息</div><div class="line"># ROW FORMAT SERDE：这里使用的是正则表达式匹配</div><div class="line"># input.regex：指定配置日志的正则表达式</div><div class="line"># output.format.string：指定提取匹配正则表达式的字段</div><div class="line"># LOCATION：指定HDFS文件的存储路径</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_test_table(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hadoop.hive.contrib.serde2.RegexSerDe&apos;</div><div class="line">WITH SERDEPROPERTIES (</div><div class="line">&quot;input.regex&quot; = &apos;&quot;TIME&quot;:(.*),&quot;HOSTNAME&quot;:(.*),&quot;LI&quot;:(.*),&quot;LU&quot;:(.*),&quot;NU&quot;:(.*),&quot;CMD&quot;:(.*)&apos;,</div><div class="line">&quot;output.format.string&quot; = &quot;%1$s %2$s %3$s %4$s %5$s %6$s&quot;</div><div class="line">)</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div><div class="line"></div><div class="line"># 创建成功之后，查看表中的数据发现全都是NULL，说明正则表达式没有提取到对应的字段信息</div><div class="line">hive&gt; select * from command_test_table;OKNULL	NULL	NULL	NULL	NULL	NULLNULL	NULL	NULL	NULL	NULL	NULLNULL	NULL	NULL	NULL	NULL	NULLNULL	NULL	NULL	NULL	NULL	NULLNULL	NULL	NULL	NULL	NULL	NULLNULL	NULL	NULL	NULL	NULL	NULLNULL	NULL	NULL	NULL	NULL	NULLNULL	NULL	NULL	NULL	NULL	NULLNULL	NULL	NULL	NULL	NULL	NULLNULL	NULL	NULL	NULL	NULL	NULLTime taken: 0.087 seconds, Fetched: 10 row(s)</div></pre></td></tr></table></figure>
<p>这里因为我们的日志是字符串内含有json，想要通过正则表达式提取json的字段属性，通过Flume的Interceptors或者Logstash的Grok表达式很容易做到，可能是我对于Hive这块研究的还不够深入，所以没有深入去研究org.apache.hadoop.hive.contrib.serde2.RegexSerDe是否支持这种正则表达式的匹配，我又尝试了一下只用空格拆分的普通字符串日志格式。</p>
<p>日志格式如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1 2 3</div><div class="line">4 5 6</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS test_table(aa STRING, bb STRING, cc STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hadoop.hive.contrib.serde2.RegexSerDe&apos;</div><div class="line">WITH SERDEPROPERTIES (</div><div class="line">&quot;input.regex&quot; = &apos;([^ ]*) ([^ ]*) ([^ ]*)&apos;,</div><div class="line">&quot;output.format.string&quot; = &quot;%1$s %2$s %3$s&quot;</div><div class="line">)</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div><div class="line"></div><div class="line">hive&gt; select * from test_table;</div><div class="line">OK</div><div class="line">1	2	3</div><div class="line">4	5	6Time taken: 0.035 seconds, Fetched: 2 row(s)</div></pre></td></tr></table></figure>
<p>发现用这种方式能够用正则表达式解析出来我们需要提取的字段信息。不知道是不是org.apache.hadoop.hive.contrib.serde2.RegexSerDe不支持这种带有json字符串的正则表达式匹配方式。这里我换了另一种做法，修改我们的日志格式尝试一下，我把command.log的日志内容修改成纯json字符串，然后使用org.apache.hive.hcatalog.data.JsonSerDe解析json字符串的匹配。下面是修改后的command.log日志文件内容。</p>
<h4 id="command-log日志文件-1"><a href="#command-log日志文件-1" class="headerlink" title="command.log日志文件"></a>command.log日志文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">&#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">&#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">&#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">&#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">&#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">&#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">&#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">&#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div></pre></td></tr></table></figure>
<h4 id="使用org-apache-hive-hcatalog-data-JsonSerDe解析日志"><a href="#使用org-apache-hive-hcatalog-data-JsonSerDe解析日志" class="headerlink" title="使用org.apache.hive.hcatalog.data.JsonSerDe解析日志"></a>使用org.apache.hive.hcatalog.data.JsonSerDe解析日志</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"># Flume重新写入新的command.log日志到HDFS中</div><div class="line"># 启动hive shell</div><div class="line">$ ./bin/hive shell</div><div class="line"></div><div class="line"># 使用数据库test_hdfs</div><div class="line">hive&gt; use test_hdfs;</div><div class="line"></div><div class="line"># 新建表command_json_table并且使用json解析器提取日志文件中的字段信息</div><div class="line"># ROW FORMAT SERDE：这里使用的是json解析器匹配</div><div class="line"># LOCATION：指定HDFS文件的存储路径</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_json_table(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hive.hcatalog.data.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div><div class="line"></div><div class="line"># 这创建还是会报错，查看hive.log日志文件的错误信息，发现是缺少org.apache.hive.hcatalog.data.JsonSerDe类所在的jar包</div><div class="line">Caused by: java.lang.ClassNotFoundException: Class org.apache.hive.hcatalog.data.JsonSerDe not found</div><div class="line"></div><div class="line"># 查了下Hive的官网wiki，发现需要先执行add jar操作，将hive-hcatalog-core.jar添加到classpath（具体的jar包地址根据自己实际的Hive安装路径修改）</div><div class="line">add jar /usr/local/hive/hcatalog/share/hcatalog/hive-hcatalog-core-1.2.1.jar;</div><div class="line"></div><div class="line"># 为了避免每次启动hive shell都重新执行一下add jar操作，我们这里在$&#123;HIVE_HOME&#125;/conf/hive-env.sh启动脚本中添加如下信息</div><div class="line">export HIVE_AUX_JARS_PATH=/usr/local/hive/hcatalog/share/hcatalog</div><div class="line"></div><div class="line"># 重启Hive服务之后，再次创建command_json_table表成功</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_json_table(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hive.hcatalog.data.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div><div class="line"></div><div class="line"># 查看command_json_table表中的内容，json字段成功的解析出我们要的字段</div><div class="line">hive&gt; select * from command_json_table;</div><div class="line">OK2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 13:10:43	localhost	783	yunyu	yunyu	ssh yunyu@10.10.1.152016-09-06 13:10:43	localhost	783	yunyu	yunyu	ssh yunyu@10.10.1.152016-09-06 13:10:43	localhost	783	yunyu	yunyu	ssh yunyu@10.10.1.15Time taken: 0.09 seconds, Fetched: 10 row(s)</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Json+SerDe" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/Json+SerDe</a></li>
<li><a href="https://my.oschina.net/cjun/blog/494692" target="_blank" rel="external">https://my.oschina.net/cjun/blog/494692</a></li>
<li><a href="http://blog.csdn.net/bluishglc/article/details/46005269" target="_blank" rel="external">http://blog.csdn.net/bluishglc/article/details/46005269</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_604c7cdd0102wbzz.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_604c7cdd0102wbzz.html</a></li>
<li><a href="http://blog.csdn.net/xiao_jun_0820/article/details/38119123" target="_blank" rel="external">http://blog.csdn.net/xiao_jun_0820/article/details/38119123</a></li>
</ul>
<h4 id="使用select-count-验证Hive可以调用MapReduce进行离线任务处理"><a href="#使用select-count-验证Hive可以调用MapReduce进行离线任务处理" class="headerlink" title="使用select count(*)验证Hive可以调用MapReduce进行离线任务处理"></a>使用select count(*)验证Hive可以调用MapReduce进行离线任务处理</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># 使用数据库test_hdfs</div><div class="line">hive&gt; use test_hdfs;</div><div class="line"></div><div class="line"># 统计command_json_table表的行数，执行失败</div><div class="line">hive&gt; select count(*) from command_json_table;</div><div class="line"></div><div class="line"># 查看yarn的log发现执行对应的mapreduce提示Connection Refused</div><div class="line"># 因为Hive最终是调用Hadoop的MapReduce来执行任务的，所以需要查看的是yarn的log日志</div><div class="line">appattempt_1474251946149_0003_000002. Got exception: java.net.ConnectException: Call From ubuntu/127.0.1.1 to ubuntu:50060 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</div></pre></td></tr></table></figure>
<p>这里我自己分析了一下原因，我们之前搭建的Hadoop集群配置是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Hadoop1节点是namenode</div><div class="line">Hadoop2和Hadoop3这两个节点是datanode</div></pre></td></tr></table></figure>
<p>仔细看了一下报错的信息，我们现在在Hadoop1上安装的Hive，ubuntu:50060这个发现是连接的Hadoop1节点的50060端口，但是50060端口是NodeManager服务的端口，但这里Hadoop1不是datanode所以没有启动NodeManager服务，需要在slaves文件中把Hadoop1节点添加上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 修改好之后重启dfs和yarn服务，再次执行sql语句</div><div class="line">hive&gt; select count(*) from command_json_table;</div><div class="line"></div><div class="line"># 又报如下的错误</div><div class="line">Application application_1474265561006_0002 failed 2 times due to Error launching appattempt_1474265561006_0002_000002. Got exception: java.net.ConnectException: Call From ubuntu/127.0.1.1 to ubuntu:52990 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused</div></pre></td></tr></table></figure>
<p>这个问题可把我坑惨了，后来自己分析了一下，原因一定是哪里的配置是我配置错了hostname是ubuntu了，但是找了一圈的配置文件也没找到，后来看网上说在namenode节点上用yarn node -list -all查看不健康的节点，发现没有问题。又尝试hdfs dfsadmin -report语句检查 DataNode 是否正常启动，让我查出来我的/etc/hosts默认配置带有’127.0.0.1 ubuntu’，这样Hadoop可能会用ubuntu这个hostname</p>
<p>重试之后还是不对，使用hostname命令查看ubuntu系统的hostname果然是’ubuntu’，ubuntu系统永久修改hostname是在/etc/hostname文件中修改，我这里对应修改成Hadoop1,hadoop2,hadoop3</p>
<p>修改/etc/hostname文件后，重新检查Hadoop集群的所有主机的hostname都已经不再是ubuntu了，都改成对应的hadoop1，hadoop2，hadoop3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hdfs dfsadmin -report Configured Capacity: 198290427904 (184.67 GB)Present Capacity: 159338950656 (148.40 GB)DFS Remaining: 159084933120 (148.16 GB)DFS Used: 254017536 (242.25 MB)DFS Used%: 0.16%Under replicated blocks: 8Blocks with corrupt replicas: 0Missing blocks: 0Missing blocks (with replication factor 1): 0-------------------------------------------------Live datanodes (3):Name: 10.10.1.94:50010 (hadoop2)Hostname: hadoop2Decommission Status : NormalConfigured Capacity: 66449108992 (61.89 GB)DFS Used: 84217856 (80.32 MB)Non DFS Used: 8056225792 (7.50 GB)DFS Remaining: 58308665344 (54.30 GB)DFS Used%: 0.13%DFS Remaining%: 87.75%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Tue Sep 20 02:23:19 PDT 2016Name: 10.10.1.64:50010 (hadoop1)Hostname: hadoop1Decommission Status : NormalConfigured Capacity: 65392209920 (60.90 GB)DFS Used: 84488192 (80.57 MB)Non DFS Used: 22853742592 (21.28 GB)DFS Remaining: 42453979136 (39.54 GB)DFS Used%: 0.13%DFS Remaining%: 64.92%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Tue Sep 20 02:23:18 PDT 2016Name: 10.10.1.95:50010 (hadoop3)Hostname: hadoop3Decommission Status : NormalConfigured Capacity: 66449108992 (61.89 GB)DFS Used: 85311488 (81.36 MB)Non DFS Used: 8041508864 (7.49 GB)DFS Remaining: 58322288640 (54.32 GB)DFS Used%: 0.13%DFS Remaining%: 87.77%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Tue Sep 20 02:23:20 PDT 2016</div></pre></td></tr></table></figure>
<p>重启系统之后，检查hostname都已经修改正确，再次启动dfs，yarn，hive服务，重试执行select count(*) from command_json_table;终于正确了。。。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hive&gt; select count(*) from command_json_table;</div><div class="line">Query ID = yunyu_20160920020204_544583fc-b872-44c8-95a6-a7b0c9611da7Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes):  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers:  set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers:  set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1474274066864_0003, Tracking URL = http://hadoop1:8088/proxy/application_1474274066864_0003/Kill Command = /data/hadoop-2.7.1/bin/hadoop job  -kill job_1474274066864_0003Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12016-09-20 02:02:13,090 Stage-1 map = 0%,  reduce = 0%2016-09-20 02:02:19,318 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.14 sec2016-09-20 02:02:26,575 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.51 secMapReduce Total cumulative CPU time: 2 seconds 510 msecEnded Job = job_1474274066864_0003MapReduce Jobs Launched: Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.51 sec   HDFS Read: 8187 HDFS Write: 3 SUCCESSTotal MapReduce CPU Time Spent: 2 seconds 510 msecOK10Time taken: 23.155 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="http://www.powerxing.com/install-hadoop-cluster/" target="_blank" rel="external">http://www.powerxing.com/install-hadoop-cluster/</a></li>
<li><a href="http://www.th7.cn/Program/java/201609/968295.shtml" target="_blank" rel="external">http://www.th7.cn/Program/java/201609/968295.shtml</a></li>
<li><a href="http://blog.csdn.net/ruglcc/article/details/7802077" target="_blank" rel="external">http://blog.csdn.net/ruglcc/article/details/7802077</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hive/Hive学习（一）Hive环境搭建" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/18/Hive/Hive学习（一）Hive环境搭建/" class="article-date">
  	<time datetime="2016-09-18T08:48:04.000Z" itemprop="datePublished">2016-09-18</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/18/Hive/Hive学习（一）Hive环境搭建/">Hive学习（一）Hive环境搭建</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Hive必须运行在Hadoop之上，则需要先安装Hadoop环境，而且还需要MySQL数据库，具体Hadoop安装请参考Hadoop系列文章</p>
<h3 id="Hive环境安装"><a href="#Hive环境安装" class="headerlink" title="Hive环境安装"></a>Hive环境安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 下载Hive</div><div class="line">$ wget http://apache.mirrors.ionfish.org/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz</div><div class="line"></div><div class="line"># 解压Hive压缩包</div><div class="line">$ tar -zxvf apache-hive-1.2.1-bin.tar.gz</div><div class="line"></div><div class="line"># 下载MySQL驱动包</div><div class="line">$ wget http://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.38.tar.gz</div><div class="line"></div><div class="line"># 解压MySQL驱动压缩包</div><div class="line">$ tar -zxvf mysql-connector-java-5.1.38.tar.gz</div></pre></td></tr></table></figure>
<h3 id="Hive相关的配置文件"><a href="#Hive相关的配置文件" class="headerlink" title="Hive相关的配置文件"></a>Hive相关的配置文件</h3><p>注意：以下配置请根据自己的实际环境修改</p>
<h5 id="配置环境变量-etc-profile"><a href="#配置环境变量-etc-profile" class="headerlink" title="配置环境变量/etc/profile"></a>配置环境变量/etc/profile</h5><pre><code>HIVE_HOME=/usr/local/hive
export HIVE_HOME
HIVE_JARS=$HIVE_HOME/lib
export HIVE_JARS
PATH=$JAVA_HOME/bin:$HIVE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$MAVEN_HOME/bin:$PATH
export PATH
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">##### 配置HIVE_HOME/conf/hive-env.sh（默认不存在，将hive-env.sh.template复制并改名为hive-env.sh）</div></pre></td></tr></table></figure>

# 这里使用此路径是因为安装Hadoop环境的时候，设置了环境变量PATH
HADOOP_HOME=/usr/local/hadoop
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">##### 配置HIVE_HOME/conf/hive-log4j.properties（默认不存在，将hive-log4j.properties.template复制并改名为hive-log4j.properties）</div><div class="line"></div><div class="line">这里使用默认配置即可，不需要修改</div><div class="line"></div><div class="line">##### 配置HIVE_HOME/conf/hdfs-site.xml（默认不存在，将hive-default.xml.template复制并改名为hive-site.xml）</div><div class="line"></div><div class="line">这里的Hadoop1是我们Hadoop集群的namenode主机的hostname，mysql安装在另外一台机器10.10.1.46上</div></pre></td></tr></table></figure>

&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;

&lt;configuration&gt;
   &lt;property&gt;
       &lt;!-- metastore我的mysql不是在该server上，是在另一台Docker镜像中 --&gt;
        &lt;name&gt;hive.metastore.local&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
        &lt;!-- mysql服务的ip和端口号 --&gt;
        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
        &lt;value&gt;jdbc:mysql://10.10.1.46:3306/hive&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionDriveName&lt;/name&gt;
        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
        &lt;value&gt;root&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
        &lt;value&gt;root&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
        &lt;!-- hive的仓库目录，需要在HDFS上创建，并修改权限 --&gt;
        &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
        &lt;value&gt;/hive/warehouse&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
        &lt;!-- 运行hive得主机地址及端口，即本机ip和端口号，启动metastore服务 --&gt;
        &lt;name&gt;hive.metastore.uris&lt;/name&gt;
        &lt;value&gt;thrift://Hadoop1:9083&lt;/value&gt;
   &lt;/property&gt;
&lt;/configuration&gt;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">##### 控制台终端</div></pre></td></tr></table></figure>

# 初始化namenode
#（这一步根据自己的实际情况选择是否初始化，如果初始化过了就不需要再初始化了）
$ ./bin/hdfs namenode -format

# 启动hdfs服务
$ ./sbin/start-dfs.sh
Starting namenodes on [hadoop1]
hadoop1: starting namenode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-namenode-ubuntu.out
hadoop2: starting datanode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-datanode-ubuntu.out
hadoop3: starting datanode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-datanode-ubuntu.out
Starting secondary namenodes [hadoop1]
hadoop1: starting secondarynamenode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-secondarynamenode-ubuntu.out

# 启动yarn服务
$ ./sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /data/hadoop-2.7.1/logs/yarn-yunyu-resourcemanager-ubuntu.out
hadoop3: starting nodemanager, logging to /data/hadoop-2.7.1/logs/yarn-yunyu-nodemanager-ubuntu.out
hadoop2: starting nodemanager, logging to /data/hadoop-2.7.1/logs/yarn-yunyu-nodemanager-ubuntu.out

# 进入hive安装目录
$ cd /data/hive-1.2.1

# 启动metastore
# 注意：启动metastore之前一定要检查hive-site.xml配置文件中配置的mysql数据库地址10.10.1.46中是否有配置的hive数据库，如果没有启动会报错，需要事先创建好空的数据库，启动metastore后会自动初始化hive的元数据表
$ ./bin/hive --service metastore &amp;

# 启动的时候可能会遇到下面的错误，是因为没有找到mysql驱动包
Caused by: java.sql.SQLException: No suitable driver found for jdbc:mysql://10.10.1.46:3306/hive
    at java.sql.DriverManager.getConnection(DriverManager.java:596)
    at java.sql.DriverManager.getConnection(DriverManager.java:187)
    at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
    at com.jolbox.bonecp.BoneCP.&lt;init&gt;(BoneCP.java:416)
    ... 48 more

# 把下载的mysql驱动包copy到hive/lib目录下重启即可
$ cp mysql-connector-java-5.1.38-bin.jar /data/hive-1.2.1/lib/

# 启动hiveserver2
$ ./bin/hive --service hiveserver2 &amp;

# 此时重新启动hive shell，就可以成功登录hive了
$ ./bin/hive shell
hive&gt;
hive&gt; show databases;
OK
default
Time taken: 1.323 seconds, Fetched: 1 row(s)

# 注意：这里使用的MySQL的root账号需要处理更改密码和远程登录授权问题，所以这里没有涉及这些问题，具体设置可以参考之前的Docker安装MySQL镜像的文章

# 我们需要预先在mysql中创建一个hive的数据库，因为hive-site.xml是连接到这个hive数据库的，所有的hive元数据都是存在这个hive数据库中的
# 我们在hive中创建新的数据库和表来验证hive的元数据都存储在mysql了

# 在hive中创建一个新的数据库test_hive，test_hive这个数据库会对应mysql中的hive数据库中的DBS表中的一条记录
hive&gt; CREATE DATABASE test_hive;

# 在hive中创建一个新的表test_person，test_person这个表会对应mysql中的hive数据库中的TBLS表中的一条记录
hive&gt; USE test_hive;
hive&gt; CREATE TABLE test_person (id INT,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;


# 在hive创建表的时候可能会遇到如下问题，是因为MySQL数据库字符集设置的utf-8导致的
# Specified key was too long; max key length is 767 bytes
# 修改MySQL的hive数据库的字符集为latin1就好用了
$ alter database hive character set latin1;
# 参考：http://blog.163.com/zhangjie_0303/blog/static/990827062013112623615941/

# test_person.txt
1    John
2    Ben
3    Allen
4    Jimmy
5    Will
6    Jackson

# 导入数据到test_person.txt到test_person表
hive&gt; LOAD DATA LOCAL INPATH &apos;/data/test_person.txt&apos; OVERWRITE INTO TABLE test_person;
Loading data to table test_hive.test_person
Table test_hive.test_person stats: [numFiles=1, numRows=0, totalSize=45, rawDataSize=0]
OK
Time taken: 2.885 seconds

# 查看test_person表数据
hive&gt; select * from test_person;
OK
1    John
2    Ben
3    Allen
4    Jimmy
5    Will
6    Jackson
Time taken: 0.7 seconds, Fetched: 6 row(s)

# 查看test_hive数据库在HDFS中存储的目录
$ cd /data/hadoop-2.7.1/bin

# 查看HDFS中/hive/warehouse目录下的所有文件，此目录是在hive-site.xml中hive.metastore.warehouse.dir参数配置的路径/hive/warehouse
$ ./bin/hdfs dfs -ls /hive/warehouse/
Found 1 items
drwxr-xr-x   - admin supergroup          0 2016-06-25 11:39 /hive/warehouse/test_hive.db

# 查看test_person表在HDFS中存储的目录
$ ./bin/hdfs dfs -ls /hive/warehouse/test_hive.db/
Found 1 items
drwxr-xr-x   - admin supergroup          0 2016-06-25 11:52 /hive/warehouse/test_hive.db/test_person

# 在深入一层就能看到我们导入的文件test_person.txt了
$ ./bin/hdfs dfs -ls /hive/warehouse/test_hive.db/test_person/
Found 1 items
-rwxr-xr-x   3 admin supergroup         45 2016-06-25 11:52 /hive/warehouse/test_hive.db/test_person/test_person.txt

# 查看test_person.txt文件里的内容，就是我们导入的内容
$ ./bin/hdfs dfs -cat /hive/warehouse/test_hive.db/test_person/test_person.txt
1    John
2    Ben
3    Allen
4    Jimmy
5    Will
6    Jackson
</code></pre><p>参考文章：</p>
<ul>
<li><a href="http://my.oschina.net/u/204498/blog/522772" target="_blank" rel="external">http://my.oschina.net/u/204498/blog/522772</a></li>
<li><a href="http://blog.fens.me/hadoop-hive-intro/" target="_blank" rel="external">http://blog.fens.me/hadoop-hive-intro/</a></li>
<li><a href="http://www.mincoder.com/article/5809.shtml" target="_blank" rel="external">http://www.mincoder.com/article/5809.shtml</a></li>
<li><a href="http://blog.163.com/zhangjie_0303/blog/static/990827062013112623615941/" target="_blank" rel="external">http://blog.163.com/zhangjie_0303/blog/static/990827062013112623615941/</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hadoop/Hadoop学习（二）Hadoop架构及原理" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/12/Hadoop/Hadoop学习（二）Hadoop架构及原理/" class="article-date">
  	<time datetime="2016-09-12T02:29:27.000Z" itemprop="datePublished">2016-09-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/12/Hadoop/Hadoop学习（二）Hadoop架构及原理/">Hadoop学习（二）Hadoop架构及原理</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>通过上一章Hadoop完全分布式集群的环境搭建，遇到了各种各样的疑问，包括MRv1代和MRv2代的区别，对很多Hadoop进程的意义都进行了详细的了解，包括一些Hadoop组件的原理</p>
<p>先来说说我的疑问</p>
<ul>
<li>有NameNode和DataNode，有ResourceManager和NodeManager，为什么没有网上说的JobTracker和TaskTracker</li>
<li>SecondaryNameNode是做什么的</li>
<li>MRv1和MRv2有什么区别</li>
<li>YARN是做什么的</li>
<li>core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml这几个配置文件是配置什么的</li>
<li>HDFS默认的端口号是8020还是9000</li>
</ul>
<p>看了上面的疑问，我想如果对Hadoop有研究的人应该都知道答案了。但是对于我这样的初学者理解起来还是费了点时间，下面我们将逐渐解答上面的疑团。</p>
<h3 id="MRv1和MRv2的区别"><a href="#MRv1和MRv2的区别" class="headerlink" title="MRv1和MRv2的区别"></a>MRv1和MRv2的区别</h3><p>先说说MRv1和MRv2，MRv1就是MapReduce v1版本和MapReduce v2版本，从 Hadoop 0.23.0 版本开始，Hadoop 的 MapReduce 框架完全重构，发生了根本的变化。新的 Hadoop MapReduce 框架命名为 MapReduceV2 或者叫 Yarn。这里我们使用的Hadoop-2.7.1版本，也就是用的YARN版本。</p>
<h4 id="YARN的好处"><a href="#YARN的好处" class="headerlink" title="YARN的好处"></a>YARN的好处</h4><p>与MRv1相比，YARN不再是一个单纯的计算框架，而是一个框架管理器，用户可以将各种各样的计算框架移植到YARN之上，由YARN进行统一管理和资源分配，由于将现有框架移植到YARN之上需要一定的工作量，当前YARN仅可运行MapReduce这种离线计算框架。</p>
<p>我们知道，不存在一种统一的计算框架适合所有应用场景，也就是说，如果你想设计一种计算框架，可以高效地进行离线计算、在线计算、流式计算、内存计算等，是不可能的。既然没有全能的计算框架，为什么不开发一个容纳和管理各种计算框架的框架管理平台（实际上是资源管理平台）呢，而YARN正是干这件事情的东西。</p>
<p>YANR本质上是一个资源统一管理系统，这一点与几年前的mesos（<a href="http://www.mesosproject.org/），更早的Torque（http://www.adaptivecomputing.com/products/open-source/torque/）基本一致。将各种框架运行在YARN之上，可以实现框架的资源统一管理和分配，使他们共享一个集群，而不是“一个框架一个集群”，这可大大降低运维成本和硬件成本。" target="_blank" rel="external">http://www.mesosproject.org/），更早的Torque（http://www.adaptivecomputing.com/products/open-source/torque/）基本一致。将各种框架运行在YARN之上，可以实现框架的资源统一管理和分配，使他们共享一个集群，而不是“一个框架一个集群”，这可大大降低运维成本和硬件成本。</a></p>
<p>下面列举了比较流行的多计算框架</p>
<ul>
<li>MapReduce:  这个框架人人皆知，它是一种离线计算框架，将一个算法抽象成Map和Reduce两个阶段进行处理，非常适合数据密集型计算。</li>
<li>Spark:  我们知道，MapReduce计算框架不适合（不是不能做，是不适合，效率太低）迭代计算（常见于machine learning领域，比如PageRank）和交互式计算（data mining领域，比如SQL查询），MapReduce是一种磁盘计算框架，而Spark则是一种内存计算框架，它将数据尽可能放到内存中以提高迭代应用和交互式应用的计算效率。官方首页：<a href="http://spark-project.org/" target="_blank" rel="external">http://spark-project.org/</a></li>
<li>Storm:  MapReduce也不适合进行流式计算、实时分析，比如广告点击计算等，而Storm则更擅长这种计算、它在实时性要远远好于MapReduce计算框架。官方首页：<a href="http://storm-project.net/" target="_blank" rel="external">http://storm-project.net/</a></li>
</ul>
<p>在YARN中，各种计算框架不再是作为一个服务部署到集群的各个节点上（比如MapReduce框架，不再需要部署JobTracler、TaskTracker等服务），而是被封装成一个用户程序库（lib）存放在客户端，当需要对计算框架进行升级时，只需升级用户程序库即可，多么容易！</p>
<p>再来看看MRv1和MRv2的对比。</p>
<h4 id="MRv1结构图"><a href="#MRv1结构图" class="headerlink" title="MRv1结构图"></a>MRv1结构图</h4><ul>
<li>NameNode : HDFS分发节点</li>
<li>DataNode : HDFS数据节点</li>
<li>JobTracker : 首先用户程序 (JobClient) 提交了一个 job，job 的信息会发送到 Job Tracker 中，Job Tracker 是 Map-reduce 框架的中心，他需要与集群中的机器定时通信 (heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理所有 job 失败、重启等操作。</li>
<li>TaskTracker : TaskTracker 是 Map-reduce 集群中每台机器都有的一个部分，他做的事情主要是监视自己所在机器的资源情况。TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat 发送给 JobTracker，JobTracker 会搜集这些信息以给新提交的 job 分配运行在哪些机器上。</li>
</ul>
<p><img src="http://img.blog.csdn.net/20160924114720210?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="MRv1"></p>
<p>当一个客户端向一个 Hadoop 集群发出一个请求时，此请求由 JobTracker 管理。JobTracker 与 NameNode 联合将工作分发到离它所处理的数据尽可能近的位置。NameNode 是文件系统的主系统，提供元数据服务来执行数据分发和复制。JobTracker 将 Map 和 Reduce 任务安排到一个或多个 TaskTracker 上的可用插槽中。TaskTracker 与 DataNode（分布式文件系统）一起对来自 DataNode 的数据执行 Map 和 Reduce 任务。当 Map 和 Reduce 任务完成时，TaskTracker 会告知 JobTracker，后者确定所有任务何时完成并最终告知客户作业已完成。</p>
<p>从 图 1 中可以看到，MRv1 实现了一个相对简单的集群管理器来执行 MapReduce 处理。MRv1 提供了一种分层的集群管理模式，其中大数据作业以单个 Map 和 Reduce 任务的形式渗入一个集群，并最后聚合成作业来报告给用户。但这种简单性有一些隐秘，不过也不是很隐秘的问题。 </p>
<h5 id="MRv1-的缺陷"><a href="#MRv1-的缺陷" class="headerlink" title="MRv1 的缺陷"></a>MRv1 的缺陷</h5><ul>
<li>JobTracker 是 Map-reduce 的集中处理点，存在单点故障。</li>
<li>JobTracker 完成了太多的任务，造成了过多的资源消耗，当 map-reduce job 非常多的时候，会造成很大的内存开销，潜在来说，也增加了 JobTracker fail 的风险，这也是业界普遍总结出老 Hadoop 的 Map-Reduce 只能支持 4000 节点主机的上限。</li>
<li>在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/ 内存的占用情况，如果两个大内存消耗的 task 被调度到了一块，很容易出现 OOM。</li>
<li>在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task slot, 如果当系统中只有 map task 或者只有 reduce task 的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。</li>
<li>源代码层面分析的时候，会发现代码非常的难读，常常因为一个 class 做了太多的事情，代码量达 3000 多行，，造成 class 的任务不清晰，增加 bug 修复和版本维护的难度。</li>
<li>从操作的角度来看，现在的 Hadoop MapReduce 框架在有任何重要的或者不重要的变化 ( 例如 bug 修复，性能提升和特性化 ) 时，都会强制进行系统级别的升级更新。更糟的是，它不管用户的喜好，强制让分布式集群系统的每一个用户端同时更新。这些更新会让用户为了验证他们之前的应用程序是不是适用新的 Hadoop 版本而浪费大量时间。</li>
</ul>
<h4 id="MRv2结构图"><a href="#MRv2结构图" class="headerlink" title="MRv2结构图"></a>MRv2结构图</h4><ul>
<li>NameNode : HDFS分发节点</li>
<li>DataNode : HDFS数据节点</li>
<li>ResourceManager : MR资源管理</li>
<li>NodeManager : NodeManager是执行应用程序的容器，监控应用程序的资源使用情况 (CPU，内存，硬盘，网络 ) 并且向调度器汇报。</li>
<li>ApplicationMaster : ApplicationMaster是向调度器索要适当的资源容器，运行任务，跟踪应用程序的状态和监控它们的进程，处理任务的失败原因。 </li>
<li>SecondaryNameNode</li>
</ul>
<p><img src="http://img.blog.csdn.net/20160924114749368?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="MRv2"></p>
<p>重构根本的思想是将 JobTracker 两个主要的功能分离成单独的组件，这两个功能是资源管理和任务调度 / 监控。新的资源管理器全局管理所有应用程序计算资源的分配，每一个应用的 ApplicationMaster 负责相应的调度和协调。一个应用程序无非是一个单独的传统的 MapReduce 任务或者是一个 DAG( 有向无环图 ) 任务。ResourceManager 和每一台机器的节点管理服务器能够管理用户在那台机器上的进程并能对计算进行组织。</p>
<p>事实上，每一个应用的 ApplicationMaster 是一个详细的框架库，它结合从 ResourceManager 获得的资源和 NodeManager 协同工作来运行和监控任务。</p>
<p>上图中 ResourceManager 支持分层级的应用队列，这些队列享有集群一定比例的资源。从某种意义上讲它就是一个纯粹的调度器，它在执行过程中不对应用进行监控和状态跟踪。同样，它也不能重启因应用失败或者硬件错误而运行失败的任务。</p>
<p>ResourceManager 是基于应用程序对资源的需求进行调度的 ; 每一个应用程序需要不同类型的资源因此就需要不同的容器。资源包括：内存，CPU，磁盘，网络等等。可以看出，这同现 Mapreduce 固定类型的资源使用模型有显著区别，它给集群的使用带来负面的影响。资源管理器提供一个调度策略的插件，它负责将集群资源分配给多个队列和应用程序。调度插件可以基于现有的能力调度和公平调度模型。</p>
<p>上图中 NodeManager 是每一台机器框架的代理，是执行应用程序的容器，监控应用程序的资源使用情况 (CPU，内存，硬盘，网络 ) 并且向调度器汇报。</p>
<p>每一个应用的 ApplicationMaster 的职责有：向调度器索要适当的资源容器，运行任务，跟踪应用程序的状态和监控它们的进程，处理任务的失败原因。 </p>
<h4 id="新旧-Hadoop-MapReduce-框架比对"><a href="#新旧-Hadoop-MapReduce-框架比对" class="headerlink" title="新旧 Hadoop MapReduce 框架比对"></a>新旧 Hadoop MapReduce 框架比对</h4><p>让我们来对新旧 MapReduce 框架做详细的分析和对比，可以看到有以下几点显著变化：<br>首先客户端不变，其调用 API 及接口大部分保持兼容，这也是为了对开发使用者透明化，使其不必对原有代码做大的改变 ( 详见 2.3 Demo 代码开发及详解)，但是原框架中核心的 JobTracker 和 TaskTracker 不见了，取而代之的是 ResourceManager, ApplicationMaster 与 NodeManager 三个部分。</p>
<p>我们来详细解释这三个部分，首先 ResourceManager 是一个中心的服务，它做的事情是调度、启动每一个 Job 所属的 ApplicationMaster、另外监控 ApplicationMaster 的存在情况。细心的读者会发现：Job 里面所在的 task 的监控、重启等等内容不见了。这就是 AppMst 存在的原因。ResourceManager 负责作业与资源的调度。接收 JobSubmitter 提交的作业，按照作业的上下文 (Context) 信息，以及从 NodeManager 收集来的状态信息，启动调度过程，分配一个 Container 作为 App Mstr<br>NodeManager 功能比较专一，就是负责 Container 状态的维护，并向 RM 保持心跳。</p>
<p>ApplicationMaster 负责一个 Job 生命周期内的所有工作，类似老的框架中 JobTracker。但注意每一个 Job（不是每一种）都有一个 ApplicationMaster，它可以运行在 ResourceManager 以外的机器上。</p>
<p>Yarn 框架相对于老的 MapReduce 框架什么优势呢？我们可以看到：<br>这个设计大大减小了 JobTracker（也就是现在的 ResourceManager）的资源消耗，并且让监测每一个 Job 子任务 (tasks) 状态的程序分布式化了，更安全、更优美。</p>
<p>在新的 Yarn 中，ApplicationMaster 是一个可变更的部分，用户可以对不同的编程模型写自己的 AppMst，让更多类型的编程模型能够跑在 Hadoop 集群中，可以参考 hadoop Yarn 官方配置模板中的 mapred-site.xml 配置。</p>
<p>对于资源的表示以内存为单位 ( 在目前版本的 Yarn 中，没有考虑 cpu 的占用 )，比之前以剩余 slot 数目更合理。</p>
<p>老的框架中，JobTracker 一个很大的负担就是监控 job 下的 tasks 的运行状况，现在，这个部分就扔给 ApplicationMaster 做了，而 ResourceManager 中有一个模块叫做 ApplicationsMasters( 注意不是 ApplicationMaster)，它是监测 ApplicationMaster 的运行状况，如果出问题，会将其在其他机器上重启。</p>
<p>Container 是 Yarn 为了将来作资源隔离而提出的一个框架。这一点应该借鉴了 Mesos 的工作，目前是一个框架，仅仅提供 java 虚拟机内存的隔离 ,hadoop 团队的设计思路应该后续能支持更多的资源调度和控制 , 既然资源表示成内存量，那就没有了之前的 map slot/reduce slot 分开造成集群资源闲置的尴尬情况。</p>
<p>引入YARN作为通用资源调度平台后，Hadoop得以支持多种计算框架，如MapReduce、Spark、Storm等。MRv1是Hadoop1中的MapReduce，MRv2是Hadoop2中的MapReduce。下面是MRv1和MRv2之间的一些基本变化：</p>
<p>MRv1包括三个部分：运行时环境（jobtracker和tasktracker）、编程模型（MapReduce）、数据处理引擎（Map任务和Reduce任务）<br>MRv2中，重用了MRv1中的编程模型和数据处理引擎。但是运行时环境被重构了。jobtracker被拆分成了通用的资源调度平台YARN和负责各个计算框架的任务调度模型AM。<br>MRv1中任务是运行在Map slot和Reduce slot中的，计算节点上的Map slot资源和Reduce slot资源不能重用。而MRv2中任务是运行在container中的，map任务结束后，相应container结束，空闲出来的资源可以让reduce使用。</p>
<p>参考文章：</p>
<ul>
<li><a href="http://dongxicheng.org/mapreduce-nextgen/what-can-we-benifit-from-yarn/" target="_blank" rel="external">http://dongxicheng.org/mapreduce-nextgen/what-can-we-benifit-from-yarn/</a></li>
</ul>
<h3 id="Hadoop的默认端口"><a href="#Hadoop的默认端口" class="headerlink" title="Hadoop的默认端口"></a>Hadoop的默认端口</h3><p>Hadoop集群的各部分一般都会使用到多个端口，有些是daemon之间进行交互之用，有些是用于RPC访问以及HTTP访问。而随着Hadoop周边组件的增多，完全记不住哪个端口对应哪个应用，特收集记录如此，以便查询。</p>
<p>这里包含我们使用到的组件：HDFS, YARN, HBase, Hive, ZooKeeper。</p>
<table>
<thead>
<tr>
<th>组件</th>
<th>Daemon</th>
<th>端口</th>
<th>配置</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>50010</td>
<td>dfs.datanode.address</td>
<td>datanode服务端口，用于数据传输</td>
</tr>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>50075</td>
<td>dfs.datanode.http.address</td>
<td>http服务的端口</td>
</tr>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>50475</td>
<td>dfs.datanode.https.address</td>
<td>https服务的端口</td>
</tr>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>50020</td>
<td>dfs.datanode.ipc.address</td>
<td>ipc服务的端口</td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>50070</td>
<td>dfs.namenode.http-address</td>
<td>http服务的端口</td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>50470</td>
<td>dfs.namenode.https-address</td>
<td>https服务的端口</td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>8020</td>
<td>fs.defaultFS</td>
<td>接收Client连接的RPC端口，用于获取文件系统metadata信息。</td>
</tr>
<tr>
<td>HDFS</td>
<td>journalnode</td>
<td>8485</td>
<td>dfs.journalnode.rpc-address</td>
<td>RPC服务</td>
</tr>
<tr>
<td>HDFS</td>
<td>journalnode</td>
<td>8480</td>
<td>dfs.journalnode.http-address    HTTP服务</td>
</tr>
<tr>
<td>HDFS</td>
<td>ZKFC</td>
<td>8019</td>
<td>dfs.ha.zkfc.port</td>
<td>ZooKeeper FailoverController，用于NN HA</td>
</tr>
<tr>
<td>YARN</td>
<td>ResourceManager</td>
<td>8032</td>
<td>yarn.resourcemanager.address</td>
<td>RM的applications manager(ASM)端口</td>
</tr>
<tr>
<td>YARN</td>
<td>ResourceManager</td>
<td>8030</td>
<td>yarn.resourcemanager.scheduler.address</td>
<td>scheduler组件的IPC端口</td>
</tr>
<tr>
<td>YARN</td>
<td>ResourceManager</td>
<td>8031</td>
<td>yarn.resourcemanager.resource-tracker.address</td>
<td>IPC</td>
</tr>
<tr>
<td>YARN</td>
<td>ResourceManager</td>
<td>8033</td>
<td>yarn.resourcemanager.admin.address</td>
<td>IPC</td>
</tr>
<tr>
<td>YARN</td>
<td>ResourceManager</td>
<td>8088</td>
<td>yarn.resourcemanager.webapp.address</td>
<td>http服务端口</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>8040</td>
<td>yarn.nodemanager.localizer.address</td>
<td>localizer IPC</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>8042</td>
<td>yarn.nodemanager.webapp.address</td>
<td>http服务端口</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>8041</td>
<td>yarn.nodemanager.address</td>
<td>NM中container manager的端口</td>
</tr>
<tr>
<td>YARN</td>
<td>JobHistory Server</td>
<td>10020</td>
<td>mapreduce.jobhistory.address</td>
<td>IPC</td>
</tr>
<tr>
<td>YARN</td>
<td>JobHistory Server</td>
<td>19888</td>
<td>mapreduce.jobhistory.webapp.address</td>
<td>http服务端口</td>
</tr>
<tr>
<td>HBase</td>
<td>Master</td>
<td>60000</td>
<td>hbase.master.port</td>
<td>IPC</td>
</tr>
<tr>
<td>HBase</td>
<td>Master</td>
<td>60010</td>
<td>hbase.master.info.port</td>
<td>http服务端口</td>
</tr>
<tr>
<td>HBase</td>
<td>RegionServer</td>
<td>60020</td>
<td>hbase.regionserver.port</td>
<td>IPC</td>
</tr>
<tr>
<td>HBase</td>
<td>RegionServer</td>
<td>60030</td>
<td>hbase.regionserver.info.port</td>
<td>http服务端口</td>
</tr>
<tr>
<td>HBase</td>
<td>HQuorumPeer</td>
<td>2181</td>
<td>hbase.zookeeper.property.clientPort</td>
<td>HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。</td>
</tr>
<tr>
<td>HBase</td>
<td>HQuorumPeer</td>
<td>2888</td>
<td>hbase.zookeeper.peerport</td>
<td>HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。</td>
</tr>
<tr>
<td>HBase</td>
<td>HQuorumPeer</td>
<td>3888</td>
<td>hbase.zookeeper.leaderport</td>
<td>HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。</td>
</tr>
<tr>
<td>Hive</td>
<td>Metastore</td>
<td>9083</td>
<td>/etc/default/hive-metastore中export PORT=<port>来更新默认端口</port></td>
</tr>
<tr>
<td>Hive</td>
<td>HiveServer</td>
<td>10000</td>
<td>/etc/hive/conf/hive-env.sh中export HIVE_SERVER2_THRIFT_PORT=<port>来更新默认端口</port></td>
</tr>
<tr>
<td>ZooKeeper</td>
<td>Server</td>
<td>2181</td>
<td>/etc/zookeeper/conf/zoo.cfg中clientPort=<port></port></td>
<td>对客户端提供服务的端口</td>
</tr>
<tr>
<td>ZooKeeper</td>
<td>Server</td>
<td>2888</td>
<td>/etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分</td>
<td>follower用来连接到leader，只在leader上监听该端口。</td>
</tr>
<tr>
<td>ZooKeeper</td>
<td>Server</td>
<td>3888</td>
<td>/etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分</td>
<td>用于leader选举的。只在electionAlg是1,2或3(默认)时需要。</td>
</tr>
</tbody>
</table>
<p>参考文章：</p>
<ul>
<li><a href="http://blog.csdn.net/zzhongcy/article/details/19912577" target="_blank" rel="external">http://blog.csdn.net/zzhongcy/article/details/19912577</a></li>
<li><a href="http://blog.csdn.net/wulantian/article/details/46341043" target="_blank" rel="external">http://blog.csdn.net/wulantian/article/details/46341043</a></li>
</ul>
<h3 id="Hadoop的IPC机制"><a href="#Hadoop的IPC机制" class="headerlink" title="Hadoop的IPC机制"></a>Hadoop的IPC机制</h3><p>参考文章：</p>
<ul>
<li><a href="http://www.cnblogs.com/xuxm2007/archive/2012/06/22/2558599.html" target="_blank" rel="external">http://www.cnblogs.com/xuxm2007/archive/2012/06/22/2558599.html</a></li>
<li><a href="https://my.oschina.net/zavakid/blog/119020" target="_blank" rel="external">https://my.oschina.net/zavakid/blog/119020</a></li>
<li><a href="http://seandeng888.iteye.com/blog/2160714" target="_blank" rel="external">http://seandeng888.iteye.com/blog/2160714</a></li>
</ul>
<h3 id="YARN在Hadoop中的作用"><a href="#YARN在Hadoop中的作用" class="headerlink" title="YARN在Hadoop中的作用"></a>YARN在Hadoop中的作用</h3><h3 id="YARN的ResourceManager"><a href="#YARN的ResourceManager" class="headerlink" title="YARN的ResourceManager"></a>YARN的ResourceManager</h3><h3 id="SecondaryNameNode节点"><a href="#SecondaryNameNode节点" class="headerlink" title="SecondaryNameNode节点"></a>SecondaryNameNode节点</h3><p>参考文章：</p>
<ul>
<li><a href="http://www.cnblogs.com/likehua/p/4023777.html" target="_blank" rel="external">http://www.cnblogs.com/likehua/p/4023777.html</a></li>
</ul>
<h3 id="MapReduce-shuffle过程原理"><a href="#MapReduce-shuffle过程原理" class="headerlink" title="MapReduce shuffle过程原理"></a>MapReduce shuffle过程原理</h3><h3 id="MapReduce原理分析"><a href="#MapReduce原理分析" class="headerlink" title="MapReduce原理分析"></a>MapReduce原理分析</h3><p>最简单的MapReduce应用程序至少包含 3 个部分：一个 Map 函数、一个 Reduce 函数和一个 main 函数。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。main 函数将作业控制和文件输入/输出结合起来。</p>
<p>所以我们编写了下面的三个Java类</p>
<ul>
<li>WordCount：MapReduce程序入口类</li>
<li>TokenizerMapper：Map并行读取文本，对读取的单词进行map操作，每个词都以<key,value>形式生成。</key,value></li>
<li>IntSumReducer：Reduce操作是对map的结果进行排序，合并，最后得出词频。</li>
</ul>
<p>简单来说，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。</p>
<h4 id="MapReduce的执行过程"><a href="#MapReduce的执行过程" class="headerlink" title="MapReduce的执行过程"></a>MapReduce的执行过程</h4><p><img src="http://img.blog.csdn.net/20161030164443369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="MapReduce执行过程"></p>
<p>MapReduce的执行过程主要包含是三个阶段：Map阶段、Shuffle阶段、Reduce阶段</p>
<ul>
<li><p>Map阶段：</p>
<ul>
<li>分片（Split）：map阶段的输入通常是HDFS上文件，在运行Mapper前，FileInputFormat会将输入文件分割成多个split ——1个split至少包含1个HDFS的Block（默认为64M）；然后每一个分片运行一个map进行处理。</li>
<li><p>执行（Map）：对输入分片中的每个键值对调用map()函数进行运算，然后输出一个结果键值对。</p>
<p>  Partitioner：对map()的输出进行partition，即根据key或value及reduce的数量来决定当前的这对键值对最终应该交由哪个reduce处理。默认是对key哈希后再以reduce task数量取模，默认的取模方式只是为了避免数据倾斜。然后该key/value对以及partitionIdx的结果都会被写入环形缓冲区。</p>
</li>
<li><p>溢写（Spill）：map输出写在内存中的环形缓冲区，默认当缓冲区满80%，启动溢写线程，将缓冲的数据写出到磁盘。</p>
<p>  Sort：在溢写到磁盘之前，使用快排对缓冲区数据按照partitionIdx, key排序。（每个partitionIdx表示一个分区，一个分区对应一个reduce）</p>
<p>  Combiner：如果设置了Combiner，那么在Sort之后，还会对具有相同key的键值对进行合并，减少溢写到磁盘的数据量。</p>
</li>
<li><p>合并（Merge）：溢写可能会生成多个文件，这时需要将多个文件合并成一个文件。合并的过程中会不断地进行 sort &amp; combine 操作，最后合并成了一个已分区且已排序的文件。</p>
</li>
</ul>
</li>
<li><p>Shuffle阶段：广义上Shuffle阶段横跨Map端和Reduce端，在Map端包括Spill过程，在Reduce端包括copy和merge/sort过程。通常认为Shuffle阶段就是将map的输出作为reduce的输入的过程</p>
<ul>
<li>Copy过程：Reduce端启动一些copy线程，通过HTTP方式将map端输出文件中属于自己的部分拉取到本地。Reduce会从多个map端拉取数据，并且每个map的数据都是有序的。</li>
<li>Merge过程：Copy过来的数据会先放入内存缓冲区中，这里的缓冲区比较大；当缓冲区数据量达到一定阈值时，将数据溢写到磁盘（与map端类似，溢写过程会执行 sort &amp; combine）。如果生成了多个溢写文件，它们会被merge成一个有序的最终文件。这个过程也会不停地执行 sort &amp; combine 操作。</li>
</ul>
</li>
<li><p>Reduce阶段：Shuffle阶段最终生成了一个有序的文件作为Reduce的输入，对于该文件中的每一个键值对调用reduce()方法，并将结果写到HDFS。</p>
</li>
</ul>
<p>参考文章：</p>
<ul>
<li><a href="http://langyu.iteye.com/blog/992916" target="_blank" rel="external">http://langyu.iteye.com/blog/992916</a></li>
<li><a href="http://blog.itpub.net/29754888/viewspace-1704959/" target="_blank" rel="external">http://blog.itpub.net/29754888/viewspace-1704959/</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop原理架构体系/">Hadoop原理架构体系</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Git/Git的SSH-Key用法" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/11/Git/Git的SSH-Key用法/" class="article-date">
  	<time datetime="2016-09-11T08:51:32.000Z" itemprop="datePublished">2016-09-11</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/11/Git/Git的SSH-Key用法/">Git的SSH-Key用法</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>之前GitHub提交代码的时候总是不知道该使用SSH方式还是Https方式，后来看了GitHub官网上建议使用Https方式，但Https方式有些麻烦，因为每次使用Https方式提交代码的时候都需要输入用户名和密码，而用SSH方式就有免密码登录的方式，只是需要在GitHub服务器添加我们本地的公钥就可以了。但是之前有一点令我一直都不解，虽然我用Https方式提交代码但是并没有让我输入用户名密码，后来找了好久原因才发现是因为我用的Mac笔记本，Mac系统有个钥匙串记录的功能，会将GitHub的用户名密码保存下来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 首先在本地生成公钥/私钥的键值对</div><div class="line">$ ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot;</div><div class="line"># 输入公钥/私钥的文件路径</div><div class="line">Enter a file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]</div><div class="line">Enter passphrase (empty for no passphrase): [Type a passphrase]</div><div class="line">Enter same passphrase again: [Type passphrase again]</div><div class="line"># 复制公钥文件中的内容，并且添加到GitHub上即可</div><div class="line">$ cat ~/.ssh/id_dsa.pub</div></pre></td></tr></table></figure>
<p>同样的道理，如果我们想在本地免密码登录测试服务器，那么我们也可以用这样的方式来设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 首先在本地生成公钥/私钥的键值对</div><div class="line">$ ssh-keygen -t dsa -P &apos;&apos; -f ~/.ssh/id_dsa</div><div class="line">$ cat ~/.ssh/id_dsa.pub</div><div class="line"></div><div class="line"># 将本地的公钥上传到测试服务器</div><div class="line">$ scp root@LocalServer:~/.ssh/id_dsa.pub  ~/.ssh/master_dsa.pub</div><div class="line"># 将上传的本地公钥添加到authorized_keys中</div><div class="line">$ cat ~/.ssh/master_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="https://help.github.com/articles/which-remote-url-should-i-use/" target="_blank" rel="external">https://help.github.com/articles/which-remote-url-should-i-use/</a></li>
<li><a href="http://www.jianshu.com/p/1ac06bcd8ab5" target="_blank" rel="external">http://www.jianshu.com/p/1ac06bcd8ab5</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Git命令/">Git命令</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Git/">Git</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hadoop/Hadoop学习（一）Hadoop完全分布式环境搭建" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/10/Hadoop/Hadoop学习（一）Hadoop完全分布式环境搭建/" class="article-date">
  	<time datetime="2016-09-10T08:09:30.000Z" itemprop="datePublished">2016-09-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/10/Hadoop/Hadoop学习（一）Hadoop完全分布式环境搭建/">Hadoop学习（一）Hadoop完全分布式环境搭建</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天学习的信息量有点大收获不少，一时之间不知道从哪里开始写，希望尽量把我今天学习到的东西记录下来，因为内容太多可能会分几篇记录。其实之前有写过一篇用Docker搭建Hadoop环境的文章，当时其实搭建的是单机伪分布式的环境，今天这里搭建的是Hadoop完全分布式环境。今天又看了许多文章，对于Hadoop的体系架构又有了一定新的理解，包括1.x版本和2.x版本的不同。</p>
<h3 id="Hadoop集群环境"><a href="#Hadoop集群环境" class="headerlink" title="Hadoop集群环境"></a>Hadoop集群环境</h3><p>我这里使用的三台虚拟机，每台虚拟机有自己的独立IP</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">192.168.1.119   hadoop1</div><div class="line">192.168.1.150   hadoop2</div><div class="line">192.168.1.149   hadoop3</div></pre></td></tr></table></figure>
<p>相关环境信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">操作系统: Ubuntu 14.04.5 LTS</div><div class="line">JDK版本: 1.7.0_79</div><div class="line">Hadoop版本: 2.7.1</div></pre></td></tr></table></figure>
<h3 id="JDK安装"><a href="#JDK安装" class="headerlink" title="JDK安装"></a>JDK安装</h3><p>省略</p>
<h3 id="Hadoop安装"><a href="#Hadoop安装" class="headerlink" title="Hadoop安装"></a>Hadoop安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 下载Hadoop安装包</div><div class="line">$ curl -O http://mirrors.cnnic.cn/apache/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz</div><div class="line"></div><div class="line"># 解压Hadoop压缩包</div><div class="line">$ tar -zxvf hadoop-2.7.1.tar.gz</div></pre></td></tr></table></figure>
<h3 id="Hadoop集群配置"><a href="#Hadoop集群配置" class="headerlink" title="Hadoop集群配置"></a>Hadoop集群配置</h3><p>注意：以下配置请根据自己的实际环境修改</p>
<h5 id="配置环境变量-etc-profile"><a href="#配置环境变量-etc-profile" class="headerlink" title="配置环境变量/etc/profile"></a>配置环境变量/etc/profile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">JAVA_HOME=/usr/local/java</div><div class="line">export JAVA_HOME</div><div class="line">HADOOP_HOME=/usr/local/hadoop</div><div class="line">export HADOOP_HOME</div><div class="line">PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</div><div class="line">export PATH</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-hadoop-env-sh，添加以下内容"><a href="#配置HADOOP-HOME-etc-hadoop-hadoop-env-sh，添加以下内容" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/hadoop-env.sh，添加以下内容"></a>配置HADOOP_HOME/etc/hadoop/hadoop-env.sh，添加以下内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export JAVA_HOME=/usr/local/java</div><div class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-yarn-env-sh，添加以下内容"><a href="#配置HADOOP-HOME-etc-hadoop-yarn-env-sh，添加以下内容" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/yarn-env.sh，添加以下内容"></a>配置HADOOP_HOME/etc/hadoop/yarn-env.sh，添加以下内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export JAVA_HOME=/usr/local/java</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-core-site-xml"><a href="#配置HADOOP-HOME-etc-hadoop-core-site-xml" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/core-site.xml"></a>配置HADOOP_HOME/etc/hadoop/core-site.xml</h5><p>这里我使用Hadoop1这台虚拟机作为NameNode节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">    &lt;value&gt;hdfs://Hadoop1:9000&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-hdfs-site-xml"><a href="#配置HADOOP-HOME-etc-hadoop-hdfs-site-xml" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/hdfs-site.xml"></a>配置HADOOP_HOME/etc/hadoop/hdfs-site.xml</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;!-- 分布式文件系统数据块复制数，我们这里是Hadoop2和Hadoop3两个节点 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</div><div class="line">    &lt;value&gt;2&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- DFS namenode存放name table的目录 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</div><div class="line">    &lt;value&gt;file:/data/hdfs/name&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- DFS datanode存放数据block的目录 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</div><div class="line">    &lt;value&gt;file:/data/hdfs/data&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- SecondaryNameNode的端口号，默认端口号是50090 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</div><div class="line">    &lt;value&gt;hadoop1:50090&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-mapred-site-xml-默认不存在，需要自建"><a href="#配置HADOOP-HOME-etc-hadoop-mapred-site-xml-默认不存在，需要自建" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/mapred-site.xml,默认不存在，需要自建"></a>配置HADOOP_HOME/etc/hadoop/mapred-site.xml,默认不存在，需要自建</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;!-- 第三方MapReduce框架，我们这里使用的yarn --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</div><div class="line">    &lt;value&gt;yarn&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- MapReduce JobHistory Server的IPC通信地址，默认端口号是10020 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">     &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</div><div class="line">     &lt;value&gt;hadoop1:10020&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- MapReduce JobHistory Server的Web服务器访问地址，默认端口号是19888 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">     &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</div><div class="line">     &lt;value&gt;hadoop1:19888&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- MapReduce已完成作业信息 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;</div><div class="line">    &lt;value&gt;/data/history/done&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- MapReduce正在运行作业信息 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;</div><div class="line">    &lt;value&gt;/data/history/done_intermediate&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-yarn-site-xml"><a href="#配置HADOOP-HOME-etc-hadoop-yarn-site-xml" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/yarn-site.xml"></a>配置HADOOP_HOME/etc/hadoop/yarn-site.xml</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;!-- 为MapReduce设置洗牌服务 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</div><div class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</div><div class="line">    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- NodeManager与ResourceManager通信的接口地址，默认端口是8032 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</div><div class="line">    &lt;value&gt;hadoop1:8032&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- NodeManger需要知道ResourceManager主机的scheduler调度服务接口地址，默认端口是8030 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</div><div class="line">    &lt;value&gt;hadoop1:8030&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- NodeManager需要向ResourceManager报告任务运行状态供Resouce跟踪，因此NodeManager节点主机需要知道ResourceManager主机的tracker接口地址，默认端口是8031 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</div><div class="line">    &lt;value&gt;hadoop1:8031&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- resourcemanager.admin，默认端口是8033 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;</div><div class="line">    &lt;value&gt;hadoop1:8033&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- 各个task的资源调度及运行状况通过通过该web界面访问，默认端口是8088 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</div><div class="line">    &lt;value&gt;hadoop1:8088&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h5 id="配置slaves节点，修改HADOOP-HOME-etc-hadoop-slaves"><a href="#配置slaves节点，修改HADOOP-HOME-etc-hadoop-slaves" class="headerlink" title="配置slaves节点，修改HADOOP_HOME/etc/hadoop/slaves"></a>配置slaves节点，修改HADOOP_HOME/etc/hadoop/slaves</h5><p>如果slaves配置中也添加Hadoop1节点，那么Hadoop1节点就既是namenode，又是datanode，这里没有这么配置，所以Hadoop1节点只是namenode，所以下面启动Hadoop1的服务之后，jps查看只有namenode服务器而没有datanode服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Hadoop2</div><div class="line">Hadoop3</div></pre></td></tr></table></figure>
<h5 id="配置-etc-hostname"><a href="#配置-etc-hostname" class="headerlink" title="配置/etc/hostname"></a>配置/etc/hostname</h5><p>Hadoop1,2,3分别修改自己的/etc/hostname文件，如果这里不修改的话，后面使用Hive做离线查询会遇到问题，具体问题请参考《Hive学习（二）使用Hive进行离线分析日志》</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop1</div></pre></td></tr></table></figure>
<h5 id="配置主机名-etc-hosts"><a href="#配置主机名-etc-hosts" class="headerlink" title="配置主机名/etc/hosts"></a>配置主机名/etc/hosts</h5><p>这里Hadoop1是namenode，Hadoop2和Hadoop3是datanode</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">192.168.1.119   hadoop1</div><div class="line">192.168.1.150   hadoop2</div><div class="line">192.168.1.149   hadoop3</div></pre></td></tr></table></figure>
<h5 id="配置SSH免密码登录"><a href="#配置SSH免密码登录" class="headerlink" title="配置SSH免密码登录"></a>配置SSH免密码登录</h5><p>在Hadoop1节点中生成新的SSH Key，并且将新生成的SSH Key添加到Hadoop1，2，3的authorized_keys免密码访问的配置中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># 创建authorized_keys文件</div><div class="line">$ vi ~/.ssh/authorized_keys</div><div class="line"></div><div class="line"># 注意：这里authorized_keys文件的权限设置为600。（这点很重要，网没有设置600权限会导致登录失败）因为我这里用的root账户没有这个问题，但是如果用自己创建的其他hadoop账户，不设置600权限就会导致登录失败</div><div class="line"></div><div class="line"># Hadoop1中执行</div><div class="line">$ ssh-keygen -t dsa -P &apos;&apos; -f ~/.ssh/id_dsa</div><div class="line"># 将Hadoop1中的公钥复制进去</div><div class="line">$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</div><div class="line"></div><div class="line"># Hadoop2，3中执行</div><div class="line">$ scp root@Hadoop1:~/.ssh/id_dsa.pub  ~/.ssh/master_dsa.pub</div><div class="line"># 将Hadoop2，3中的公钥复制进去</div><div class="line">$ cat ~/.ssh/master_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</div><div class="line"></div><div class="line"># 在Hadoop1中测试是否可以免密码登录Hadoop1，2，3（第一次应该只需要输入yes）</div><div class="line">$ ssh root@Hadoop1</div><div class="line">$ ssh root@Hadoop2</div><div class="line">$ ssh root@Hadoop3</div></pre></td></tr></table></figure>
<h5 id="配置好Hadoop1之后，将Hadoop1的配置copy到Hadoop2和Hadoop3"><a href="#配置好Hadoop1之后，将Hadoop1的配置copy到Hadoop2和Hadoop3" class="headerlink" title="配置好Hadoop1之后，将Hadoop1的配置copy到Hadoop2和Hadoop3"></a>配置好Hadoop1之后，将Hadoop1的配置copy到Hadoop2和Hadoop3</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 在Hadoop1中执行</div><div class="line">$ scp -r /data/hadoop-2.7.1 root@Hadoop2:/data/</div><div class="line">$ scp -r /data/hadoop-2.7.1 root@Hadoop3:/data/</div></pre></td></tr></table></figure>
<h5 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"># 初始化namenode</div><div class="line">$ ./bin/hdfs namenode -format</div><div class="line"></div><div class="line"># 初始化好namenode后，hadoop会自动建好对应hdfs-site.xml的namenode配置的文件路径</div><div class="line">$ ll /data/hdfs/name/current/</div><div class="line">total 24</div><div class="line">drwxrwxr-x 2 yunyu yunyu 4096 Sep 10 18:07 ./</div><div class="line">drwxrwxr-x 3 yunyu yunyu 4096 Sep 10 18:07 ../</div><div class="line">-rw-rw-r-- 1 yunyu yunyu  352 Sep 10 18:07 fsimage_0000000000000000000</div><div class="line">-rw-rw-r-- 1 yunyu yunyu   62 Sep 10 18:07 fsimage_0000000000000000000.md5</div><div class="line">-rw-rw-r-- 1 yunyu yunyu    2 Sep 10 18:07 seen_txid</div><div class="line">-rw-rw-r-- 1 yunyu yunyu  202 Sep 10 18:07 VERSION</div><div class="line"></div><div class="line"># 启动hdfs服务</div><div class="line">$ ./sbin/start-dfs.sh</div><div class="line">Starting namenodes on [hadoop1]</div><div class="line">hadoop1: starting namenode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-namenode-ubuntu.out</div><div class="line">hadoop2: starting datanode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-datanode-ubuntu.out</div><div class="line">hadoop3: starting datanode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-datanode-ubuntu.out</div><div class="line">Starting secondary namenodes [hadoop1]</div><div class="line">hadoop1: starting secondarynamenode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-secondarynamenode-ubuntu.out</div><div class="line"></div><div class="line"># 使用jps检查启动的服务，可以看到NameNode和SecondaryNameNode已经启动</div><div class="line">$ jps</div><div class="line">20379 SecondaryNameNode</div><div class="line">20570 Jps</div><div class="line">20106 NameNode</div><div class="line"></div><div class="line"># 这时候在Hadoop2和Hadoop3节点上使用jps查看，DataNode已经启动</div><div class="line">$ jps</div><div class="line">16392 Jps</div><div class="line">16024 DataNode</div><div class="line"></div><div class="line"># 在Hadoop2和Hadoop3节点上，也会自动建好对应hdfs-site.xml的datanode配置的文件路径</div><div class="line">$ ll /data/hdfs/data/current/</div><div class="line">total 16</div><div class="line">drwxrwxr-x 3 yunyu yunyu 4096 Sep 10 18:10 ./</div><div class="line">drwx------ 3 yunyu yunyu 4096 Sep 10 18:10 ../</div><div class="line">drwx------ 4 yunyu yunyu 4096 Sep 10 18:10 BP-1965589257-127.0.1.1-1473502067891/</div><div class="line">-rw-rw-r-- 1 yunyu yunyu  229 Sep 10 18:10 VERSION</div><div class="line"></div><div class="line"># 启动yarn服务</div><div class="line">$ ./sbin/start-yarn.sh</div><div class="line">starting yarn daemons</div><div class="line">starting resourcemanager, logging to /data/hadoop-2.7.1/logs/yarn-yunyu-resourcemanager-ubuntu.out</div><div class="line">hadoop3: starting nodemanager, logging to /data/hadoop-2.7.1/logs/yarn-yunyu-nodemanager-ubuntu.out</div><div class="line">hadoop2: starting nodemanager, logging to /data/hadoop-2.7.1/logs/yarn-yunyu-nodemanager-ubuntu.out</div><div class="line"></div><div class="line"># 使用jps检查启动的服务，可以看到ResourceManager已经启动</div><div class="line">$ jps</div><div class="line">21653 Jps</div><div class="line">20379 SecondaryNameNode</div><div class="line">20106 NameNode</div><div class="line">21310 ResourceManager</div><div class="line"></div><div class="line"># 这时候在Hadoop2和Hadoop3节点上使用jps查看，NodeManager已经启动</div><div class="line">$ jps</div><div class="line">16946 NodeManager</div><div class="line">17235 Jps</div><div class="line">16024 DataNode</div><div class="line"></div><div class="line"># 启动jobhistory服务，默认jobhistory在使用start-all.sh是不启动的，所以即使使用start-all.sh也要手动启动jobhistory服务</div><div class="line">$ ./sbin/mr-jobhistory-daemon.sh start historyserver</div><div class="line">starting historyserver, logging to /data/hadoop-2.7.1/logs/mapred-yunyu-historyserver-ubuntu.out</div><div class="line"></div><div class="line"># 使用jps检查启动的服务，可以看到JobHistoryServer已经启动</div><div class="line">$ jps</div><div class="line">21937 Jps</div><div class="line">20379 SecondaryNameNode</div><div class="line">20106 NameNode</div><div class="line">21863 JobHistoryServer</div><div class="line">21310 ResourceManager</div></pre></td></tr></table></figure>
<p>注意：使用start-all.sh启动已经不再被推荐使用，所以这里使用的是Hadoop推荐的分开启动，分别启动start-dfs.sh和start-yarn.sh，所以看一些比较就的Hadoop版本安装的文章可能会用start-all.sh启动</p>
<h5 id="停止服务"><a href="#停止服务" class="headerlink" title="停止服务"></a>停止服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 停止hdfs服务</div><div class="line">$ ./sbin/stop-dfs.sh</div><div class="line"></div><div class="line"># 停止yarn服务</div><div class="line">$ ./sbin/stop-yarn.sh</div><div class="line"></div><div class="line"># 停止jobhistory服务</div><div class="line">$ ./sbin/mr-jobhistory-daemon.sh stop historyserver</div></pre></td></tr></table></figure>
<h5 id="验证Hadoop集群的Web服务"><a href="#验证Hadoop集群的Web服务" class="headerlink" title="验证Hadoop集群的Web服务"></a>验证Hadoop集群的Web服务</h5><ul>
<li><p>验证NameNode的Web服务能访问，浏览器访问<a href="http://192.168.1.119:50070" target="_blank" rel="external">http://192.168.1.119:50070</a><br><img src="http://img.blog.csdn.net/20160910184234348?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
</li>
<li><p>验证ResourceManager的Web服务能访问，浏览器访问<a href="http://192.168.1.119:8088" target="_blank" rel="external">http://192.168.1.119:8088</a><br><img src="http://img.blog.csdn.net/20160910184156566?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
</li>
<li><p>验证MapReduce JobHistory Server的Web服务能访问，浏览器访问<a href="http://192.168.1.119:19888" target="_blank" rel="external">http://192.168.1.119:19888</a><br><img src="http://img.blog.csdn.net/20160910184251708?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
</li>
</ul>
<h5 id="验证HDFS文件系统"><a href="#验证HDFS文件系统" class="headerlink" title="验证HDFS文件系统"></a>验证HDFS文件系统</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># 查看根目录下的文件</div><div class="line">$ hdfs dfs -ls /</div><div class="line">Found 1 items</div><div class="line">drwxrwx---   - yunyu supergroup          0 2016-09-10 03:15 /data</div><div class="line"></div><div class="line"># 创建temp目录</div><div class="line">$ hdfs dfs -mkdir /temp</div><div class="line"></div><div class="line"># 再次查看根目录下的文件，可以看到temp目录</div><div class="line">$ hdfs dfs -ls /</div><div class="line">Found 2 items</div><div class="line">drwxrwx---   - yunyu supergroup          0 2016-09-10 03:15 /data</div><div class="line">drwxr-xr-x   - yunyu supergroup          0 2016-09-10 03:45 /temp</div><div class="line"></div><div class="line"># 可以查看之前mapred-site.xml中配置的mapreduce作业执行中的目录和作业已完成的目录</div><div class="line">$ hdfs dfs -ls /data/history/</div><div class="line">Found 2 items</div><div class="line">drwxrwx---   - yunyu supergroup          0 2016-09-10 03:15 /data/history/done</div><div class="line">drwxrwxrwt   - yunyu supergroup          0 2016-09-10 03:15 /data/history/done_intermediate</div></pre></td></tr></table></figure>
<h3 id="需要注意的地方"><a href="#需要注意的地方" class="headerlink" title="需要注意的地方"></a>需要注意的地方</h3><p>网上一些Hadoop集群安装相关文章中，有一部分还是Hadoop老版本的配置，所以有些迷惑，像JobTracker，TaskTracker这些概念是Hadoop老版本才有的，新版本中使用ResourceManager和NodeManager替代了他们。后续的章节会详细的介绍Hadoop的相关原理以及新老版本的区别。</p>
<p>最近好久没有用Hadoop了，突然要做日志持久化，居然本地的Hadoop集群环境起不来了，后来发现是自己启动方式错了，三个Hadoop节点只需要在NameNode执行start-dfs.sh和start-yarn.sh脚本，而我却分别在三个Hadoop节点都去做了启动操作，发现下面的提示信息才反应过来，真是太尴尬了。。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ start-dfs.sh</div><div class="line">Starting namenodes on [hadoop1]</div><div class="line">yunyu@hadoop1&apos;s password: </div><div class="line">hadoop1: namenode running as process 9117. Stop it first.</div></pre></td></tr></table></figure>
<h3 id="使用HDFS默认端口号8020配置"><a href="#使用HDFS默认端口号8020配置" class="headerlink" title="使用HDFS默认端口号8020配置"></a>使用HDFS默认端口号8020配置</h3><p>修改core-site.xml配置文件如下（即把端口号去掉）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">    &lt;value&gt;hdfs://hadoop1&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p>启动HDFS服务之后，分别在Hadoop1，2，3三台服务器上查看8020端口，发现HDFS默认使用的是8020端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"># 启动HDFS服务</div><div class="line">$ ./sbin/start-dfs.sh</div><div class="line"></div><div class="line"># Hadoop1中查看8020端口</div><div class="line">$ lsof -i:8020</div><div class="line">COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</div><div class="line">java    5112 yunyu  197u  IPv4  26041      0t0  TCP hadoop1:8020 (LISTEN)</div><div class="line">java    5112 yunyu  207u  IPv4  27568      0t0  TCP hadoop1:8020-&gt;hadoop2:34867 (ESTABLISHED)</div><div class="line">java    5112 yunyu  208u  IPv4  26096      0t0  TCP hadoop1:8020-&gt;hadoop3:59852 (ESTABLISHED)</div><div class="line">java    5112 yunyu  209u  IPv4  29792      0t0  TCP hadoop1:8020-&gt;hadoop1:45542 (ESTABLISHED)</div><div class="line">java    5383 yunyu  196u  IPv4  28826      0t0  TCP hadoop1:45542-&gt;hadoop1:8020 (ESTABLISHED)</div><div class="line"></div><div class="line"># Hadoop2中查看8020端口</div><div class="line">$ lsof -i:8020</div><div class="line">COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</div><div class="line">java    4609 yunyu  234u  IPv4  24013      0t0  TCP hadoop2:34867-&gt;hadoop1:8020 (ESTABLISHED)</div><div class="line"></div><div class="line"># Hadoop3中查看8020端口</div><div class="line">$ lsof -i:8020</div><div class="line">COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</div><div class="line">java    4452 yunyu  234u  IPv4  23413      0t0  TCP hadoop3:59852-&gt;hadoop1:8020 (ESTABLISHED)</div></pre></td></tr></table></figure>
<p>访问HDFS集群的方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 访问本机的HDFS集群</div><div class="line">hdfs dfs -ls /</div><div class="line"></div><div class="line"># 可以指定host和port访问远程的HDFS集群（这里使用hostname和port访问本地集群）</div><div class="line">hdfs dfs -ls hdfs://Hadoop1:8020/</div><div class="line"></div><div class="line"># 如果使用的默认端口号8020，也可以不指定端口号访问</div><div class="line">hdfs dfs -ls hdfs://Hadoop1/</div></pre></td></tr></table></figure>
<h3 id="解决Unable-to-load-native-hadoop-library-for-your-platform"><a href="#解决Unable-to-load-native-hadoop-library-for-your-platform" class="headerlink" title="解决Unable to load native-hadoop library for your platform"></a>解决Unable to load native-hadoop library for your platform</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"># 线上环境安装Hadoop的时候遇到下面的错误</div><div class="line">$ start-dfs.sh</div><div class="line">17/02/07 16:00:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">Starting namenodes on [yy-logs-hdfs01]</div><div class="line">yy-logs-hdfs01: starting namenode, logging to /usr/local/hadoop-2.7.1/logs/hadoop-hadoop-namenode-yy-logs-hdfs01.out</div><div class="line">localhost: starting datanode, logging to /usr/local/hadoop-2.7.1/logs/hadoop-hadoop-datanode-yy-logs-hdfs01.out</div><div class="line">Starting secondary namenodes [yy-logs-hdfs01]</div><div class="line">yy-logs-hdfs01: starting secondarynamenode, logging to /usr/local/hadoop-2.7.1/logs/hadoop-hadoop-secondarynamenode-yy-logs-hdfs01.out</div><div class="line">17/02/07 16:00:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line"></div><div class="line"># 于是百度Google查找原因，网上有人说是因为Apache提供的hadoop本地库是32位的，而在64位的服务器上就会有问题，因此需要自己编译64位的版本。</div><div class="line"># 检查native库，发现果然是这个原因</div><div class="line">$ hadoop checknative -a</div><div class="line">17/02/07 16:06:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">Native library checking:</div><div class="line">hadoop:  false</div><div class="line">zlib:    false</div><div class="line">snappy:  false</div><div class="line">lz4:     false</div><div class="line">bzip2:   false</div><div class="line">openssl: false</div><div class="line">17/02/07 16:06:34 INFO util.ExitUtil: Exiting with status 1</div><div class="line"></div><div class="line"># 从下面的地址下载Hadoop对应版本已经编译好的Native库，我这里下载的是hadoop-2.7.x版本的</div><div class="line"># http://dl.bintray.com/sequenceiq/sequenceiq-bin/</div><div class="line"></div><div class="line"># 将下载的Native库解压到$HADOOP_HOME下的lib和lib/native目录下</div><div class="line">$ tar -xvf hadoop-native-64-2.7.0.tar -C /usr/local/hadoop/lib/</div><div class="line">$ tar -xvf hadoop-native-64-2.7.0.tar -C /usr/local/hadoop/lib/native/</div><div class="line"></div><div class="line"># 重新检查native库</div><div class="line">$ hadoop checknative -a</div><div class="line">17/02/07 16:26:56 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version</div><div class="line">17/02/07 16:26:56 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</div><div class="line">Native library checking:</div><div class="line">hadoop:  true /usr/local/hadoop-2.7.1/lib/libhadoop.so.1.0.0</div><div class="line">zlib:    true /lib64/libz.so.1</div><div class="line">snappy:  false</div><div class="line">lz4:     true revision:99</div><div class="line">bzip2:   false</div><div class="line">openssl: true /usr/lib64/libcrypto.so</div><div class="line">17/02/07 16:26:56 INFO util.ExitUtil: Exiting with status 1</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/ClusterSetup.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/ClusterSetup.html</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/core-default.xml" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/core-default.xml</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#secondarynamenode" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#secondarynamenode</a></li>
<li><a href="http://www.cnblogs.com/luogankun/p/4019303.html" target="_blank" rel="external">http://www.cnblogs.com/luogankun/p/4019303.html</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/#_3.1_hadoop_0.23.0" target="_blank" rel="external">http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/#_3.1_hadoop_0.23.0</a></li>
<li><a href="http://blog.chinaunix.net/uid-25266990-id-3900239.html" target="_blank" rel="external">http://blog.chinaunix.net/uid-25266990-id-3900239.html</a></li>
<li><a href="http://blog.csdn.net/jxnu_xiaobing/article/details/46931693" target="_blank" rel="external">http://blog.csdn.net/jxnu_xiaobing/article/details/46931693</a></li>
<li><a href="http://www.cnblogs.com/liuling/archive/2013/06/16/2013-6-16-01.html" target="_blank" rel="external">http://www.cnblogs.com/liuling/archive/2013/06/16/2013-6-16-01.html</a></li>
<li><a href="http://www.cnblogs.com/luogankun/p/4019303.html" target="_blank" rel="external">http://www.cnblogs.com/luogankun/p/4019303.html</a></li>
<li><a href="http://jacoxu.com/?p=961" target="_blank" rel="external">http://jacoxu.com/?p=961</a></li>
<li><a href="http://blog.csdn.net/jack85986370/article/details/51902871" target="_blank" rel="external">http://blog.csdn.net/jack85986370/article/details/51902871</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/NativeLibraries.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/NativeLibraries.html</a></li>
<li><a href="http://blog.csdn.net/liuxinghao/article/details/40121843" target="_blank" rel="external">http://blog.csdn.net/liuxinghao/article/details/40121843</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop原理架构体系/">Hadoop原理架构体系</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hadoop/Hadoop学习（三）MapReduce的WordCount实例" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/10/Hadoop/Hadoop学习（三）MapReduce的WordCount实例/" class="article-date">
  	<time datetime="2016-09-10T02:08:16.000Z" itemprop="datePublished">2016-09-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/10/Hadoop/Hadoop学习（三）MapReduce的WordCount实例/">Hadoop学习（三）MapReduce的WordCount实例</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>前面Hadoop的集群环境我们已经搭建好了，而且也分析了MapReduce和YARN之前的关系，以及在Hadoop中的作用，接下来我们将在Hadoop集群环境中跑一个我们自己创建的MapReduce离线任务实例WordCount。</p>
<p>我这里有一个Hadoop例子的项目，是我自己写的一些大数据相关的实例，后续会持续更新的。</p>
<ul>
<li><a href="http://github.com/birdben/birdHadoop">http://github.com/birdben/birdHadoop</a></li>
</ul>
<p>WordCount的实例很简单，就是要统计一下某一个文件中每次单词出现的次数，下面就是我们要统计的文件内容</p>
<h5 id="input-WordCount"><a href="#input-WordCount" class="headerlink" title="input_WordCount"></a>input_WordCount</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Hadoop Hive HBaseSpark Hive HadoopKafka HBase ES Logstash StormFlume Kafka Hadoop</div></pre></td></tr></table></figure>
<h5 id="output-WordCount"><a href="#output-WordCount" class="headerlink" title="output_WordCount"></a>output_WordCount</h5><pre><code>ES    1
Flume    1
HBase    2
Hadoop    3
Hive    2
Kafka    2
Logstash    1
Spark    1
Storm    1
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">下面引用了其他博客的图，因为这些图十分形象的描述了MapReduce的执行过程，博客原文链接如下：</div><div class="line"></div><div class="line">- http://www.cnblogs.com/xia520pi/archive/2012/05/16/2504205.html</div><div class="line"></div><div class="line">![WordCount1](http://img.blog.csdn.net/20161030182309567?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)</div><div class="line"></div><div class="line">- 将文件拆分成splits，由于测试用的文件较小，所以每个文件为一个split，并将文件按行分割形成&lt;key,value&gt;对，如上图所示。这一步由MapReduce框架自动完成，其中偏移量（即key值）包括了回车所占的字符数（Windows和Linux环境会不同）。</div><div class="line"></div><div class="line">![WordCount2](http://img.blog.csdn.net/20161030182342598?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)</div><div class="line"></div><div class="line">- 将分割好的&lt;key,value&gt;对交给用户定义的map方法进行处理，生成新的&lt;key,value&gt;对，如上图所示。</div><div class="line"></div><div class="line">![WordCount3](http://img.blog.csdn.net/20161030182404755?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)</div><div class="line"></div><div class="line">- 得到map方法输出的&lt;key,value&gt;对后，Mapper会将它们按照key值进行排序，并执行Combine过程，将key至相同value值累加，得到Mapper的最终输出结果。如上图所示。</div><div class="line"></div><div class="line">![WordCount4](http://img.blog.csdn.net/20161030182418630?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)</div><div class="line"></div><div class="line">- Reducer先对从Mapper接收的数据进行排序，再交由用户自定义的reduce方法进行处理，得到新的&lt;key,value&gt;对，并作为WordCount的输出结果，如上图所示。</div><div class="line"></div><div class="line">### WordCount实例程序</div><div class="line"></div><div class="line">实例程序请参考GitHub上的源代码</div><div class="line"></div><div class="line">- http://github.com/birdben/birdHadoop</div><div class="line"></div><div class="line">这里我们使用Maven来打包构建项目，pom文件中需要添加Hadoop相关jar的引用</div></pre></td></tr></table></figure>

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
    &lt;version&gt;2.4.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;
    &lt;version&gt;2.4.0&lt;/version&gt;
&lt;/dependency&gt;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">这里Maven构建将依赖的jar包也打包到birdHadoop.jar中，并且直接在pom文件中指定调用的入口类，我这里指定了入口类是com.birdben.mapreduce.demo.WordCount，然后运行java -jar birdHadoop.jar inputfile outputfile即可。pom文件中的配置如下</div></pre></td></tr></table></figure>

&lt;build&gt;
    &lt;finalName&gt;birdHadoop&lt;/finalName&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
            &lt;configuration&gt;
                &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;
                &lt;descriptorRefs&gt;
                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                &lt;/descriptorRefs&gt;
                &lt;archive&gt;
                    &lt;manifest&gt;
                        &lt;mainClass&gt;com.birdben.mapreduce.demo.WordCountMain&lt;/mainClass&gt;
                    &lt;/manifest&gt;
                &lt;/archive&gt;
            &lt;/configuration&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;make-assembly&lt;/id&gt;
                    &lt;phase&gt;package&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;assembly&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
            &lt;version&gt;3.3&lt;/version&gt;
            &lt;configuration&gt;
                &lt;source&gt;1.7&lt;/source&gt;
                &lt;target&gt;1.7&lt;/target&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div></pre></td></tr></table></figure>

# 进入项目根目录下
$ cd /Users/yunyu/workspace_git/birdHadoop
# 编译打包
$ mvn clean package
# 执行我们的Shell脚本，这里将HDFS的相关操作写成了Shell脚本
$ sh scripts/mapreduce/runWordCount.sh
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#### runWordCount.sh脚本文件</div></pre></td></tr></table></figure>

#!/bin/bash
local_path=~/Downloads/birdHadoop
hdfs_input_path=/birdben/input
hdfs_output_path=/birdben/output
# 在HDFS上创建需要分析的文件存储目录，如果已经存在就先删除再重新创建，保证脚本的正常执行
echo &quot;删除HDFS上的input目录$hdfs_input_path&quot;
hdfs dfs -rm -r $hdfs_input_path
echo &quot;创建HDFS上的input目录$hdfs_input_path&quot;
hdfs dfs -mkdir -p $hdfs_input_path
# 需要将我们要分析的track.log日志文件上传到HDFS文件目录下
echo &quot;将$local_path/inputfile/WordCount/input_WordCount文件复制到HDFS的目录$hdfs_input_path&quot;
hdfs dfs -put $local_path/inputfile/WordCount/input_WordCount $hdfs_input_path
# 需要先删除HDFS上已存在的目录，否则hadoop执行jar的时候会报错
echo &quot;删除HDFS的output目录$hdfs_output_path&quot;
hdfs dfs -rm -r -f $hdfs_output_path
# 需要在Maven的pom.xml文件中指定jar的入口类
echo &quot;开始执行birdHadoop.jar...&quot;
hadoop jar $local_path/target/birdHadoop.jar $hdfs_input_path $hdfs_output_path
echo &quot;结束执行birdHadoop.jar...&quot;

if [ ! -d $local_path/outputfile/WordCount ]; then
    # 如果本地文件目录不存在，就自动创建
    echo &quot;自动创建$local_path/outputfile/WordCount目录&quot;
    mkdir -p $local_path/outputfile/WordCount
else
    # 如果本地文件已经存在，就删除
    echo &quot;删除$local_path/outputfile/WordCount/*目录下的所有文件&quot;
    rm -rf $local_path/outputfile/WordCount/*
fi
# 从HDFS目录中导出mapreduce的结果文件到本地文件系统
echo &quot;导出HDFS目录$hdfs_output_path目录下的文件到本地$local_path/outputfile/WordCount/&quot;
hdfs dfs -get $hdfs_output_path/* $local_path/outputfile/WordCount/
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">下面是执行过程中的输出</div></pre></td></tr></table></figure>

$ sh scripts/mapreduce/runWordCount.sh
删除HDFS上的input目录/birdben/input
16/11/02 05:12:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /birdben/input
创建HDFS上的input目录/birdben/input
将/home/yunyu/Downloads/birdHadoop/inputfile/WordCount/input_WordCount文件复制到HDFS的目录/birdben/input
删除HDFS的output目录/birdben/output
16/11/02 05:13:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /birdben/output
开始执行birdHadoop.jar...
birdben out WordCount start
16/11/02 05:13:07 INFO demo.WordCount: birdben logger WordCount start
16/11/02 05:13:08 INFO client.RMProxy: Connecting to ResourceManager at hadoop1/10.10.1.49:8032
16/11/02 05:13:09 INFO input.FileInputFormat: Total input paths to process : 1
16/11/02 05:13:09 INFO mapreduce.JobSubmitter: number of splits:1
16/11/02 05:13:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1478088725123_0001
16/11/02 05:13:10 INFO impl.YarnClientImpl: Submitted application application_1478088725123_0001
16/11/02 05:13:10 INFO mapreduce.Job: The url to track the job: http://hadoop1:8088/proxy/application_1478088725123_0001/
16/11/02 05:13:10 INFO mapreduce.Job: Running job: job_1478088725123_0001
16/11/02 05:13:17 INFO mapreduce.Job: Job job_1478088725123_0001 running in uber mode : false
16/11/02 05:13:17 INFO mapreduce.Job:  map 0% reduce 0%
16/11/02 05:13:24 INFO mapreduce.Job:  map 100% reduce 0%
16/11/02 05:13:32 INFO mapreduce.Job:  map 100% reduce 100%
16/11/02 05:13:32 INFO mapreduce.Job: Job job_1478088725123_0001 completed successfully
16/11/02 05:13:32 INFO mapreduce.Job: Counters: 49
    File System Counters
        FILE: Number of bytes read=114
        FILE: Number of bytes written=230785
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=194
        HDFS: Number of bytes written=72
        HDFS: Number of read operations=6
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=2
    Job Counters 
        Launched map tasks=1
        Launched reduce tasks=1
        Data-local map tasks=1
        Total time spent by all maps in occupied slots (ms)=4836
        Total time spent by all reduces in occupied slots (ms)=3355
        Total time spent by all map tasks (ms)=4836
        Total time spent by all reduce tasks (ms)=3355
        Total vcore-seconds taken by all map tasks=4836
        Total vcore-seconds taken by all reduce tasks=3355
        Total megabyte-seconds taken by all map tasks=4952064
        Total megabyte-seconds taken by all reduce tasks=3435520
    Map-Reduce Framework
        Map input records=4
        Map output records=14
        Map output bytes=141
        Map output materialized bytes=114
        Input split bytes=109
        Combine input records=14
        Combine output records=9
        Reduce input groups=9
        Reduce shuffle bytes=114
        Reduce input records=9
        Reduce output records=9
        Spilled Records=18
        Shuffled Maps =1
        Failed Shuffles=0
        Merged Map outputs=1
        GC time elapsed (ms)=54
        CPU time spent (ms)=1330
        Physical memory (bytes) snapshot=455933952
        Virtual memory (bytes) snapshot=1415868416
        Total committed heap usage (bytes)=276299776
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters 
        Bytes Read=85
    File Output Format Counters 
        Bytes Written=72
结束执行birdHadoop.jar...
删除/home/yunyu/Downloads/birdHadoop/outputfile/WordCount/*目录下的所有文件
导出HDFS目录/birdben/output目录下的文件到本地/home/yunyu/Downloads/birdHadoop/outputfile/WordCount/
16/11/02 05:13:34 WARN hdfs.DFSClient: DFSInputStream has been closed already
16/11/02 05:13:35 WARN hdfs.DFSClient: DFSInputStream has been closed already
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">Shell脚本的最后我们将HDFS文件导出到本地系统文件，查看一下这个目录下的文件。</div></pre></td></tr></table></figure>

$ ll outputfile/WordCount/
total 12
drwxrwxr-x 2 yunyu yunyu 4096 Nov  2 20:13 ./
drwxrwxr-x 4 yunyu yunyu 4096 Oct 26 19:46 ../
-rw-r--r-- 1 yunyu yunyu   72 Nov  2 20:13 part-r-00000
-rw-r--r-- 1 yunyu yunyu    0 Nov  2 20:13 _SUCCESS
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">查看一下我们所期望的结果文件part-r-00000的内容</div></pre></td></tr></table></figure>

$ cat outputfile/WordCount/part-r-00000 
ES    1
Flume    1
HBase    2
Hadoop    3
Hive    2
Kafka    2
Logstash    1
Spark    1
Storm    1
</code></pre><p>参考文章：</p>
<ul>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="external">http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html</a></li>
<li><a href="http://www.cnblogs.com/xwdreamer/archive/2011/01/04/2297049.html" target="_blank" rel="external">http://www.cnblogs.com/xwdreamer/archive/2011/01/04/2297049.html</a></li>
<li><a href="http://blog.csdn.net/lisonglisonglisong/article/details/47125319" target="_blank" rel="external">http://blog.csdn.net/lisonglisonglisong/article/details/47125319</a></li>
<li><a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2504205.html" target="_blank" rel="external">http://www.cnblogs.com/xia520pi/archive/2012/05/16/2504205.html</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop原理架构体系/">Hadoop原理架构体系</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/">MapReduce</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hadoop/Hadoop学习（四）MapReduce清洗数据实例" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/10/Hadoop/Hadoop学习（四）MapReduce清洗数据实例/" class="article-date">
  	<time datetime="2016-09-10T02:08:16.000Z" itemprop="datePublished">2016-09-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/10/Hadoop/Hadoop学习（四）MapReduce清洗数据实例/">Hadoop学习（四）MapReduce清洗数据实例</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>通过前两篇的文章内容我们已经介绍了MapReduce的运行原理，以及WordCount实例的执行过程，接下来我们将根据我们的实际应用改写出一个清洗Log数据的MapReduce。</p>
<p>具体源代码请关注下面的GitHub项目</p>
<ul>
<li><a href="http://github.com/birdben/birdHadoop">http://github.com/birdben/birdHadoop</a></li>
</ul>
<h3 id="数据清洗的目标"><a href="#数据清洗的目标" class="headerlink" title="数据清洗的目标"></a>数据清洗的目标</h3><p>这里我们期望将下面的track.log日志文件内容转化一下，将logs外层结构去掉，提起出来logs的内层数据，并且将原来的logs下的数组转换成多条新的日志记录。</p>
<h5 id="track-log日志文件"><a href="#track-log日志文件" class="headerlink" title="track.log日志文件"></a>track.log日志文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475114816071&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829286&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.286Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475114827206&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840425&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.425Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915077351&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915090579&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:50.579Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914816133&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829332&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.332Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914827284&quot;,&quot;rpid&quot;:&quot;65351516503932936&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840498&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.499Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915077585&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915090789&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:50.789Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475913832349&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475913845544&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:04:05.544Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915080561&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915093792&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:53.792Z&quot;&#125;</div></pre></td></tr></table></figure>
<h5 id="期望清洗之后的文件内容如下"><a href="#期望清洗之后的文件内容如下" class="headerlink" title="期望清洗之后的文件内容如下"></a>期望清洗之后的文件内容如下</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475915093792&quot;,&quot;timestamp&quot;:1475915080561,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475913845544&quot;,&quot;timestamp&quot;:1475913832349,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475912715001&quot;,&quot;timestamp&quot;:1475912701768,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475915090789&quot;,&quot;timestamp&quot;:1475915077585,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932936&quot;,&quot;server_timestamp&quot;:&quot;1475914840498&quot;,&quot;timestamp&quot;:1475914827284,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475914829332&quot;,&quot;timestamp&quot;:1475914816133,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475915090579&quot;,&quot;timestamp&quot;:1475915077351,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475914840425&quot;,&quot;timestamp&quot;:1475114827206,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475914829286&quot;,&quot;timestamp&quot;:1475114816071,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;</div></pre></td></tr></table></figure>
<h3 id="AdLog实例程序"><a href="#AdLog实例程序" class="headerlink" title="AdLog实例程序"></a>AdLog实例程序</h3><p>实例程序请参考GitHub上的源代码</p>
<ul>
<li><a href="http://github.com/birdben/birdHadoop">http://github.com/birdben/birdHadoop</a></li>
</ul>
<p>这里我们使用Maven来打包构建项目，同之前的WordCount实例是一个项目。我们也是将依赖的jar包也打包到birdHadoop.jar中，并且直接在pom文件中指定调用的入口类，注意这里我们修改了入口类是com.birdben.mapreduce.adlog.AdLogMain，需要在pom文件中配置如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">&lt;build&gt;</div><div class="line">    &lt;finalName&gt;birdHadoop&lt;/finalName&gt;</div><div class="line">    &lt;plugins&gt;</div><div class="line">        &lt;plugin&gt;</div><div class="line">            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</div><div class="line">            &lt;configuration&gt;</div><div class="line">                &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;</div><div class="line">                &lt;descriptorRefs&gt;</div><div class="line">                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</div><div class="line">                &lt;/descriptorRefs&gt;</div><div class="line">                &lt;archive&gt;</div><div class="line">                    &lt;manifest&gt;</div><div class="line">                        &lt;mainClass&gt;com.birdben.mapreduce.adlog.AdLogMain&lt;/mainClass&gt;</div><div class="line">                    &lt;/manifest&gt;</div><div class="line">                &lt;/archive&gt;</div><div class="line">            &lt;/configuration&gt;</div><div class="line">            &lt;executions&gt;</div><div class="line">                &lt;execution&gt;</div><div class="line">                    &lt;id&gt;make-assembly&lt;/id&gt;</div><div class="line">                    &lt;phase&gt;package&lt;/phase&gt;</div><div class="line">                    &lt;goals&gt;</div><div class="line">                        &lt;goal&gt;assembly&lt;/goal&gt;</div><div class="line">                    &lt;/goals&gt;</div><div class="line">                &lt;/execution&gt;</div><div class="line">            &lt;/executions&gt;</div><div class="line">        &lt;/plugin&gt;</div><div class="line">        &lt;plugin&gt;</div><div class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;3.3&lt;/version&gt;</div><div class="line">            &lt;configuration&gt;</div><div class="line">                &lt;source&gt;1.7&lt;/source&gt;</div><div class="line">                &lt;target&gt;1.7&lt;/target&gt;</div><div class="line">            &lt;/configuration&gt;</div><div class="line">        &lt;/plugin&gt;</div><div class="line">    &lt;/plugins&gt;</div><div class="line">&lt;/build&gt;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 进入项目根目录下</div><div class="line">$ cd /Users/yunyu/workspace_git/birdHadoop</div><div class="line"># 编译打包</div><div class="line">$ mvn clean package</div><div class="line"># 执行我们的Shell脚本</div><div class="line">$ sh scripts/mapreduce/runAdLog.sh</div></pre></td></tr></table></figure>
<h4 id="runAdLog-sh脚本文件"><a href="#runAdLog-sh脚本文件" class="headerlink" title="runAdLog.sh脚本文件"></a>runAdLog.sh脚本文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line">local_path=~/Downloads/birdHadoop</div><div class="line">hdfs_input_path=/birdben/input</div><div class="line">hdfs_output_path=/birdben/output</div><div class="line"># 在HDFS上创建需要分析的文件存储目录，如果已经存在就先删除再重新创建，保证脚本的正常执行</div><div class="line">echo &quot;删除HDFS上的input目录$hdfs_input_path&quot;</div><div class="line">hdfs dfs -rm -r $hdfs_input_path</div><div class="line">echo &quot;创建HDFS上的input目录$hdfs_input_path&quot;</div><div class="line">hdfs dfs -mkdir -p $hdfs_input_path</div><div class="line"># 需要将我们要分析的track.log日志文件上传到HDFS文件目录下</div><div class="line">echo &quot;将$local_path/inputfile/AdLog/track.log文件复制到HDFS的目录$hdfs_input_path&quot;</div><div class="line">hdfs dfs -put $local_path/inputfile/AdLog/track.log $hdfs_input_path</div><div class="line"># 需要先删除HDFS上已存在的目录，否则hadoop执行jar的时候会报错</div><div class="line">echo &quot;删除HDFS的output目录$hdfs_output_path&quot;</div><div class="line">hdfs dfs -rm -r -f $hdfs_output_path</div><div class="line"># 需要在Maven的pom.xml文件中指定jar的入口类</div><div class="line">echo &quot;开始执行birdHadoop.jar...&quot;</div><div class="line">hadoop jar $local_path/target/birdHadoop.jar $hdfs_input_path $hdfs_output_path</div><div class="line">echo &quot;结束执行birdHadoop.jar...&quot;</div><div class="line"></div><div class="line">if [ ! -d $local_path/outputfile/AdLog ]; then</div><div class="line">	# 如果本地文件目录不存在，就自动创建</div><div class="line">	echo &quot;自动创建$local_path/outputfile/AdLog目录&quot;</div><div class="line">	mkdir -p $local_path/outputfile/AdLog</div><div class="line">else</div><div class="line">	# 如果本地文件已经存在，就删除</div><div class="line">	echo &quot;删除$local_path/outputfile/AdLog/*目录下的所有文件&quot;</div><div class="line">	rm -rf $local_path/outputfile/AdLog/*</div><div class="line">fi</div><div class="line"># 从HDFS目录中导出mapreduce的结果文件到本地文件系统</div><div class="line">echo &quot;导出HDFS目录$hdfs_output_path目录下的文件到本地$local_path/outputfile/AdLog/&quot;</div><div class="line">hdfs dfs -get $hdfs_output_path/* $local_path/outputfile/AdLog/</div></pre></td></tr></table></figure>
<p>下面是执行过程中的输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sh scripts/mapreduce/runAdLog.sh 删除HDFS上的input目录/birdben/input16/11/02 20:03:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.Deleted /birdben/input创建HDFS上的input目录/birdben/input将/home/yunyu/Downloads/birdHadoop/inputfile/AdLog/track.log文件复制到HDFS的目录/birdben/input删除HDFS的output目录/birdben/output16/11/02 20:03:28 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.Deleted /birdben/output开始执行birdHadoop.jar...birdben out AdLog start16/11/02 20:03:30 INFO adlog.AdLogMain: birdben logger AdLog start16/11/02 20:03:31 INFO client.RMProxy: Connecting to ResourceManager at hadoop1/10.10.1.49:803216/11/02 20:03:33 INFO input.FileInputFormat: Total input paths to process : 116/11/02 20:03:33 INFO mapreduce.JobSubmitter: number of splits:116/11/02 20:03:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1478138258749_000116/11/02 20:03:33 INFO impl.YarnClientImpl: Submitted application application_1478138258749_000116/11/02 20:03:33 INFO mapreduce.Job: The url to track the job: http://hadoop1:8088/proxy/application_1478138258749_0001/16/11/02 20:03:33 INFO mapreduce.Job: Running job: job_1478138258749_000116/11/02 20:03:41 INFO mapreduce.Job: Job job_1478138258749_0001 running in uber mode : false16/11/02 20:03:41 INFO mapreduce.Job:  map 0% reduce 0%16/11/02 20:03:48 INFO mapreduce.Job:  map 100% reduce 0%16/11/02 20:03:54 INFO mapreduce.Job:  map 100% reduce 100%16/11/02 20:03:54 INFO mapreduce.Job: Job job_1478138258749_0001 completed successfully16/11/02 20:03:54 INFO mapreduce.Job: Counters: 49	File System Counters		FILE: Number of bytes read=1545		FILE: Number of bytes written=233699		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=2509		HDFS: Number of bytes written=1503		HDFS: Number of read operations=6		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Launched reduce tasks=1		Data-local map tasks=1		Total time spent by all maps in occupied slots (ms)=4100		Total time spent by all reduces in occupied slots (ms)=3026		Total time spent by all map tasks (ms)=4100		Total time spent by all reduce tasks (ms)=3026		Total vcore-seconds taken by all map tasks=4100		Total vcore-seconds taken by all reduce tasks=3026		Total megabyte-seconds taken by all map tasks=4198400		Total megabyte-seconds taken by all reduce tasks=3098624	Map-Reduce Framework		Map input records=9		Map output records=9		Map output bytes=1512		Map output materialized bytes=1545		Input split bytes=103		Combine input records=9		Combine output records=9		Reduce input groups=1		Reduce shuffle bytes=1545		Reduce input records=9		Reduce output records=9		Spilled Records=18		Shuffled Maps =1		Failed Shuffles=0		Merged Map outputs=1		GC time elapsed (ms)=169		CPU time spent (ms)=1450		Physical memory (bytes) snapshot=336318464		Virtual memory (bytes) snapshot=1343729664		Total committed heap usage (bytes)=136450048	Shuffle Errors		BAD_ID=0		CONNECTION=0		IO_ERROR=0		WRONG_LENGTH=0		WRONG_MAP=0		WRONG_REDUCE=0	File Input Format Counters 		Bytes Read=2406	File Output Format Counters 		Bytes Written=1503结束执行birdHadoop.jar...删除/home/yunyu/Downloads/birdHadoop/outputfile/AdLog/*目录下的所有文件导出HDFS目录/birdben/output目录下的文件到本地/home/yunyu/Downloads/birdHadoop/outputfile/AdLog/16/11/02 20:03:57 WARN hdfs.DFSClient: DFSInputStream has been closed already16/11/02 20:03:57 WARN hdfs.DFSClient: DFSInputStream has been closed already</div></pre></td></tr></table></figure>
<p>Shell脚本的最后我们将HDFS文件导出到本地系统文件，查看一下这个目录下的文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ll outputfile/AdLog/total 12drwxrwxr-x 2 yunyu yunyu 4096 Nov  3 11:03 ./drwxrwxr-x 4 yunyu yunyu 4096 Oct 26 19:46 ../-rw-r--r-- 1 yunyu yunyu 1503 Nov  3 11:03 part-r-00000-rw-r--r-- 1 yunyu yunyu    0 Nov  3 11:03 _SUCCESS</div></pre></td></tr></table></figure>
<p>查看一下我们所期望的结果文件part-r-00000的内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cat outputfile/AdLog/part-r-00000 &#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475915093792&quot;,&quot;timestamp&quot;:1475915080561,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475913845544&quot;,&quot;timestamp&quot;:1475913832349,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475912715001&quot;,&quot;timestamp&quot;:1475912701768,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475915090789&quot;,&quot;timestamp&quot;:1475915077585,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932936&quot;,&quot;server_timestamp&quot;:&quot;1475914840498&quot;,&quot;timestamp&quot;:1475914827284,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475914829332&quot;,&quot;timestamp&quot;:1475914816133,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475915090579&quot;,&quot;timestamp&quot;:1475915077351,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475914840425&quot;,&quot;timestamp&quot;:1475114827206,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475914829286&quot;,&quot;timestamp&quot;:1475114816071,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;</div></pre></td></tr></table></figure>
<p>可以看到最终的结果是我们之前所期望的，大功告成 ^_^</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop原理架构体系/">Hadoop原理架构体系</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/">MapReduce</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/6/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/8/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 birdben
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>



<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82900755-1', 'auto');
  ga('send', 'pageview');

</script>
<!-- End Google Analytics -->




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>