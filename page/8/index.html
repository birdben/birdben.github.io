<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>birdben</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="birdben">
<meta property="og:url" content="https://github.com/birdben/page/8/index.html">
<meta property="og:site_name" content="birdben">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="birdben">
  
    <link rel="alternative" href="/atom.xml" title="birdben" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
<script type="text/javascript">
var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1260188951'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s4.cnzz.com/z_stat.php%3Fid%3D1260188951' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/images/logo.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">birdben</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						<li>Links</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/AWK/" style="font-size: 10.83px;">AWK</a> <a href="/tags/Akka/" style="font-size: 10.83px;">Akka</a> <a href="/tags/Dockerfile/" style="font-size: 20px;">Dockerfile</a> <a href="/tags/Docker命令/" style="font-size: 19.17px;">Docker命令</a> <a href="/tags/Docker环境/" style="font-size: 15px;">Docker环境</a> <a href="/tags/ELK/" style="font-size: 16.67px;">ELK</a> <a href="/tags/ElasticSearch/" style="font-size: 10.83px;">ElasticSearch</a> <a href="/tags/Elasticsearch/" style="font-size: 12.5px;">Elasticsearch</a> <a href="/tags/Flume/" style="font-size: 17.5px;">Flume</a> <a href="/tags/Git命令/" style="font-size: 13.33px;">Git命令</a> <a href="/tags/Go/" style="font-size: 14.17px;">Go</a> <a href="/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/tags/HDFS/" style="font-size: 18.33px;">HDFS</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Hadoop原理架构体系/" style="font-size: 13.33px;">Hadoop原理架构体系</a> <a href="/tags/Hive/" style="font-size: 16.67px;">Hive</a> <a href="/tags/JVM/" style="font-size: 11.67px;">JVM</a> <a href="/tags/Java-Web，Socket，Python/" style="font-size: 10px;">Java Web，Socket，Python</a> <a href="/tags/Jenkins环境/" style="font-size: 10px;">Jenkins环境</a> <a href="/tags/Kafka/" style="font-size: 15.83px;">Kafka</a> <a href="/tags/Kibana/" style="font-size: 14.17px;">Kibana</a> <a href="/tags/Linux命令/" style="font-size: 12.5px;">Linux命令</a> <a href="/tags/Logstash/" style="font-size: 15.83px;">Logstash</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/MapReduce/" style="font-size: 11.67px;">MapReduce</a> <a href="/tags/Maven配置/" style="font-size: 11.67px;">Maven配置</a> <a href="/tags/MongoDB/" style="font-size: 11.67px;">MongoDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Nginx/" style="font-size: 10px;">Nginx</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/Shell/" style="font-size: 16.67px;">Shell</a> <a href="/tags/Spring/" style="font-size: 10.83px;">Spring</a> <a href="/tags/Storm/" style="font-size: 12.5px;">Storm</a> <a href="/tags/Zookeeper/" style="font-size: 12.5px;">Zookeeper</a> <a href="/tags/其他/" style="font-size: 10px;">其他</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://blog.csdn.net/birdben">我的CSDN的博客</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">birdben</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="/images/logo.png" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">birdben</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-Hadoop/Hadoop学习（四）MapReduce清洗数据实例" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/10/Hadoop/Hadoop学习（四）MapReduce清洗数据实例/" class="article-date">
  	<time datetime="2016-09-10T02:08:16.000Z" itemprop="datePublished">2016-09-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/10/Hadoop/Hadoop学习（四）MapReduce清洗数据实例/">Hadoop学习（四）MapReduce清洗数据实例</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>通过前两篇的文章内容我们已经介绍了MapReduce的运行原理，以及WordCount实例的执行过程，接下来我们将根据我们的实际应用改写出一个清洗Log数据的MapReduce。</p>
<p>具体源代码请关注下面的GitHub项目</p>
<ul>
<li><a href="http://github.com/birdben/birdHadoop">http://github.com/birdben/birdHadoop</a></li>
</ul>
<h3 id="数据清洗的目标"><a href="#数据清洗的目标" class="headerlink" title="数据清洗的目标"></a>数据清洗的目标</h3><p>这里我们期望将下面的track.log日志文件内容转化一下，将logs外层结构去掉，提起出来logs的内层数据，并且将原来的logs下的数组转换成多条新的日志记录。</p>
<h5 id="track-log日志文件"><a href="#track-log日志文件" class="headerlink" title="track.log日志文件"></a>track.log日志文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475114816071&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829286&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.286Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475114827206&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840425&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.425Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915077351&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915090579&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:50.579Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914816133&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829332&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.332Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914827284&quot;,&quot;rpid&quot;:&quot;65351516503932936&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840498&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.499Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915077585&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915090789&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:50.789Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475913832349&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475913845544&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:04:05.544Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915080561&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915093792&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:53.792Z&quot;&#125;</div></pre></td></tr></table></figure>
<h5 id="期望清洗之后的文件内容如下"><a href="#期望清洗之后的文件内容如下" class="headerlink" title="期望清洗之后的文件内容如下"></a>期望清洗之后的文件内容如下</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475915093792&quot;,&quot;timestamp&quot;:1475915080561,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475913845544&quot;,&quot;timestamp&quot;:1475913832349,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475912715001&quot;,&quot;timestamp&quot;:1475912701768,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475915090789&quot;,&quot;timestamp&quot;:1475915077585,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932936&quot;,&quot;server_timestamp&quot;:&quot;1475914840498&quot;,&quot;timestamp&quot;:1475914827284,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475914829332&quot;,&quot;timestamp&quot;:1475914816133,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475915090579&quot;,&quot;timestamp&quot;:1475915077351,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475914840425&quot;,&quot;timestamp&quot;:1475114827206,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475914829286&quot;,&quot;timestamp&quot;:1475114816071,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;</div></pre></td></tr></table></figure>
<h3 id="AdLog实例程序"><a href="#AdLog实例程序" class="headerlink" title="AdLog实例程序"></a>AdLog实例程序</h3><p>实例程序请参考GitHub上的源代码</p>
<ul>
<li><a href="http://github.com/birdben/birdHadoop">http://github.com/birdben/birdHadoop</a></li>
</ul>
<p>这里我们使用Maven来打包构建项目，同之前的WordCount实例是一个项目。我们也是将依赖的jar包也打包到birdHadoop.jar中，并且直接在pom文件中指定调用的入口类，注意这里我们修改了入口类是com.birdben.mapreduce.adlog.AdLogMain，需要在pom文件中配置如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">&lt;build&gt;</div><div class="line">    &lt;finalName&gt;birdHadoop&lt;/finalName&gt;</div><div class="line">    &lt;plugins&gt;</div><div class="line">        &lt;plugin&gt;</div><div class="line">            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</div><div class="line">            &lt;configuration&gt;</div><div class="line">                &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;</div><div class="line">                &lt;descriptorRefs&gt;</div><div class="line">                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</div><div class="line">                &lt;/descriptorRefs&gt;</div><div class="line">                &lt;archive&gt;</div><div class="line">                    &lt;manifest&gt;</div><div class="line">                        &lt;mainClass&gt;com.birdben.mapreduce.adlog.AdLogMain&lt;/mainClass&gt;</div><div class="line">                    &lt;/manifest&gt;</div><div class="line">                &lt;/archive&gt;</div><div class="line">            &lt;/configuration&gt;</div><div class="line">            &lt;executions&gt;</div><div class="line">                &lt;execution&gt;</div><div class="line">                    &lt;id&gt;make-assembly&lt;/id&gt;</div><div class="line">                    &lt;phase&gt;package&lt;/phase&gt;</div><div class="line">                    &lt;goals&gt;</div><div class="line">                        &lt;goal&gt;assembly&lt;/goal&gt;</div><div class="line">                    &lt;/goals&gt;</div><div class="line">                &lt;/execution&gt;</div><div class="line">            &lt;/executions&gt;</div><div class="line">        &lt;/plugin&gt;</div><div class="line">        &lt;plugin&gt;</div><div class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;3.3&lt;/version&gt;</div><div class="line">            &lt;configuration&gt;</div><div class="line">                &lt;source&gt;1.7&lt;/source&gt;</div><div class="line">                &lt;target&gt;1.7&lt;/target&gt;</div><div class="line">            &lt;/configuration&gt;</div><div class="line">        &lt;/plugin&gt;</div><div class="line">    &lt;/plugins&gt;</div><div class="line">&lt;/build&gt;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 进入项目根目录下</div><div class="line">$ cd /Users/yunyu/workspace_git/birdHadoop</div><div class="line"># 编译打包</div><div class="line">$ mvn clean package</div><div class="line"># 执行我们的Shell脚本</div><div class="line">$ sh scripts/mapreduce/runAdLog.sh</div></pre></td></tr></table></figure>
<h4 id="runAdLog-sh脚本文件"><a href="#runAdLog-sh脚本文件" class="headerlink" title="runAdLog.sh脚本文件"></a>runAdLog.sh脚本文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line">local_path=~/Downloads/birdHadoop</div><div class="line">hdfs_input_path=/birdben/input</div><div class="line">hdfs_output_path=/birdben/output</div><div class="line"># 在HDFS上创建需要分析的文件存储目录，如果已经存在就先删除再重新创建，保证脚本的正常执行</div><div class="line">echo &quot;删除HDFS上的input目录$hdfs_input_path&quot;</div><div class="line">hdfs dfs -rm -r $hdfs_input_path</div><div class="line">echo &quot;创建HDFS上的input目录$hdfs_input_path&quot;</div><div class="line">hdfs dfs -mkdir -p $hdfs_input_path</div><div class="line"># 需要将我们要分析的track.log日志文件上传到HDFS文件目录下</div><div class="line">echo &quot;将$local_path/inputfile/AdLog/track.log文件复制到HDFS的目录$hdfs_input_path&quot;</div><div class="line">hdfs dfs -put $local_path/inputfile/AdLog/track.log $hdfs_input_path</div><div class="line"># 需要先删除HDFS上已存在的目录，否则hadoop执行jar的时候会报错</div><div class="line">echo &quot;删除HDFS的output目录$hdfs_output_path&quot;</div><div class="line">hdfs dfs -rm -r -f $hdfs_output_path</div><div class="line"># 需要在Maven的pom.xml文件中指定jar的入口类</div><div class="line">echo &quot;开始执行birdHadoop.jar...&quot;</div><div class="line">hadoop jar $local_path/target/birdHadoop.jar $hdfs_input_path $hdfs_output_path</div><div class="line">echo &quot;结束执行birdHadoop.jar...&quot;</div><div class="line"></div><div class="line">if [ ! -d $local_path/outputfile/AdLog ]; then</div><div class="line">	# 如果本地文件目录不存在，就自动创建</div><div class="line">	echo &quot;自动创建$local_path/outputfile/AdLog目录&quot;</div><div class="line">	mkdir -p $local_path/outputfile/AdLog</div><div class="line">else</div><div class="line">	# 如果本地文件已经存在，就删除</div><div class="line">	echo &quot;删除$local_path/outputfile/AdLog/*目录下的所有文件&quot;</div><div class="line">	rm -rf $local_path/outputfile/AdLog/*</div><div class="line">fi</div><div class="line"># 从HDFS目录中导出mapreduce的结果文件到本地文件系统</div><div class="line">echo &quot;导出HDFS目录$hdfs_output_path目录下的文件到本地$local_path/outputfile/AdLog/&quot;</div><div class="line">hdfs dfs -get $hdfs_output_path/* $local_path/outputfile/AdLog/</div></pre></td></tr></table></figure>
<p>下面是执行过程中的输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sh scripts/mapreduce/runAdLog.sh 删除HDFS上的input目录/birdben/input16/11/02 20:03:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.Deleted /birdben/input创建HDFS上的input目录/birdben/input将/home/yunyu/Downloads/birdHadoop/inputfile/AdLog/track.log文件复制到HDFS的目录/birdben/input删除HDFS的output目录/birdben/output16/11/02 20:03:28 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.Deleted /birdben/output开始执行birdHadoop.jar...birdben out AdLog start16/11/02 20:03:30 INFO adlog.AdLogMain: birdben logger AdLog start16/11/02 20:03:31 INFO client.RMProxy: Connecting to ResourceManager at hadoop1/10.10.1.49:803216/11/02 20:03:33 INFO input.FileInputFormat: Total input paths to process : 116/11/02 20:03:33 INFO mapreduce.JobSubmitter: number of splits:116/11/02 20:03:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1478138258749_000116/11/02 20:03:33 INFO impl.YarnClientImpl: Submitted application application_1478138258749_000116/11/02 20:03:33 INFO mapreduce.Job: The url to track the job: http://hadoop1:8088/proxy/application_1478138258749_0001/16/11/02 20:03:33 INFO mapreduce.Job: Running job: job_1478138258749_000116/11/02 20:03:41 INFO mapreduce.Job: Job job_1478138258749_0001 running in uber mode : false16/11/02 20:03:41 INFO mapreduce.Job:  map 0% reduce 0%16/11/02 20:03:48 INFO mapreduce.Job:  map 100% reduce 0%16/11/02 20:03:54 INFO mapreduce.Job:  map 100% reduce 100%16/11/02 20:03:54 INFO mapreduce.Job: Job job_1478138258749_0001 completed successfully16/11/02 20:03:54 INFO mapreduce.Job: Counters: 49	File System Counters		FILE: Number of bytes read=1545		FILE: Number of bytes written=233699		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=2509		HDFS: Number of bytes written=1503		HDFS: Number of read operations=6		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Launched reduce tasks=1		Data-local map tasks=1		Total time spent by all maps in occupied slots (ms)=4100		Total time spent by all reduces in occupied slots (ms)=3026		Total time spent by all map tasks (ms)=4100		Total time spent by all reduce tasks (ms)=3026		Total vcore-seconds taken by all map tasks=4100		Total vcore-seconds taken by all reduce tasks=3026		Total megabyte-seconds taken by all map tasks=4198400		Total megabyte-seconds taken by all reduce tasks=3098624	Map-Reduce Framework		Map input records=9		Map output records=9		Map output bytes=1512		Map output materialized bytes=1545		Input split bytes=103		Combine input records=9		Combine output records=9		Reduce input groups=1		Reduce shuffle bytes=1545		Reduce input records=9		Reduce output records=9		Spilled Records=18		Shuffled Maps =1		Failed Shuffles=0		Merged Map outputs=1		GC time elapsed (ms)=169		CPU time spent (ms)=1450		Physical memory (bytes) snapshot=336318464		Virtual memory (bytes) snapshot=1343729664		Total committed heap usage (bytes)=136450048	Shuffle Errors		BAD_ID=0		CONNECTION=0		IO_ERROR=0		WRONG_LENGTH=0		WRONG_MAP=0		WRONG_REDUCE=0	File Input Format Counters 		Bytes Read=2406	File Output Format Counters 		Bytes Written=1503结束执行birdHadoop.jar...删除/home/yunyu/Downloads/birdHadoop/outputfile/AdLog/*目录下的所有文件导出HDFS目录/birdben/output目录下的文件到本地/home/yunyu/Downloads/birdHadoop/outputfile/AdLog/16/11/02 20:03:57 WARN hdfs.DFSClient: DFSInputStream has been closed already16/11/02 20:03:57 WARN hdfs.DFSClient: DFSInputStream has been closed already</div></pre></td></tr></table></figure>
<p>Shell脚本的最后我们将HDFS文件导出到本地系统文件，查看一下这个目录下的文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ll outputfile/AdLog/total 12drwxrwxr-x 2 yunyu yunyu 4096 Nov  3 11:03 ./drwxrwxr-x 4 yunyu yunyu 4096 Oct 26 19:46 ../-rw-r--r-- 1 yunyu yunyu 1503 Nov  3 11:03 part-r-00000-rw-r--r-- 1 yunyu yunyu    0 Nov  3 11:03 _SUCCESS</div></pre></td></tr></table></figure>
<p>查看一下我们所期望的结果文件part-r-00000的内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cat outputfile/AdLog/part-r-00000 &#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475915093792&quot;,&quot;timestamp&quot;:1475915080561,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475913845544&quot;,&quot;timestamp&quot;:1475913832349,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475912715001&quot;,&quot;timestamp&quot;:1475912701768,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475915090789&quot;,&quot;timestamp&quot;:1475915077585,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932936&quot;,&quot;server_timestamp&quot;:&quot;1475914840498&quot;,&quot;timestamp&quot;:1475914827284,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475914829332&quot;,&quot;timestamp&quot;:1475914816133,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475915090579&quot;,&quot;timestamp&quot;:1475915077351,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475914840425&quot;,&quot;timestamp&quot;:1475114827206,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475914829286&quot;,&quot;timestamp&quot;:1475114816071,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;</div></pre></td></tr></table></figure>
<p>可以看到最终的结果是我们之前所期望的，大功告成 ^_^</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop原理架构体系/">Hadoop原理架构体系</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/">MapReduce</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hadoop/Hadoop学习（五）Hadoop日志总结" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/10/Hadoop/Hadoop学习（五）Hadoop日志总结/" class="article-date">
  	<time datetime="2016-09-10T02:08:16.000Z" itemprop="datePublished">2016-09-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/10/Hadoop/Hadoop学习（五）Hadoop日志总结/">Hadoop学习（五）Hadoop日志总结</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>通过之前的WordCount和AdLog实例，我们已经能够编写出简单的MapReduce实例了，但是第一次编写还是难免会遇到一些问题，比如AdLog解析json日志结构的时候出错怎么办，如何查看MapReduce的运行日志呢，这就是我们本篇要重点介绍的Hadoop日志</p>
<h2 id="理解Hadoop的日志"><a href="#理解Hadoop的日志" class="headerlink" title="理解Hadoop的日志"></a>理解Hadoop的日志</h2><p>Hadoop的日志一般会分为下面两种</p>
<ul>
<li>Hadoop系统服务输出的日志</li>
<li>Mapreduce程序输出来的日志</li>
</ul>
<h3 id="Hadoop系统服务输出的日志"><a href="#Hadoop系统服务输出的日志" class="headerlink" title="Hadoop系统服务输出的日志"></a>Hadoop系统服务输出的日志</h3><p>也就是我们启动NameNode, DataNode, NodeManager ResourceManager, HistoryServer等等系统自带的服务输出来的日志，默认是存放在${HADOOP_HOME}/logs目录下。</p>
<p>可以在mapred-site.xml配置文件中指定Hadoop日志的输出路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">	&lt;!-- hadoop的日志输出指定目录--&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;mapred.local.dir&lt;/name&gt;</div><div class="line">		&lt;value&gt;/home/yunyu/birdben_logs&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">&lt;configuration&gt;</div></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>服务名</th>
<th>服务类型</th>
<th>日志文件名</th>
</tr>
</thead>
<tbody>
<tr>
<td>resourcemanager</td>
<td>YARN</td>
<td>yarn-${USER}-resourcemanager-${hostname}.log</td>
</tr>
<tr>
<td>nodemanager</td>
<td>YARN</td>
<td>yarn-${USER}-nodemanager-${hostname}.log</td>
</tr>
<tr>
<td>historyserver</td>
<td>HDFS</td>
<td>mapred-${USER}-historyserver-${hostname}.log</td>
</tr>
<tr>
<td>secondarynamenode</td>
<td>HDFS</td>
<td>hadoop-${USER}-secondarynamenode-${hostname}.log</td>
</tr>
<tr>
<td>namenode</td>
<td>HDFS</td>
<td>hadoop-${USER}-namenode-${hostname}.log</td>
</tr>
<tr>
<td>datanode</td>
<td>HDFS</td>
<td>hadoop-${USER}-datanode-${hostname}.log</td>
</tr>
</tbody>
</table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">用resourcemanager的输出日志举例 : yarn-$&#123;USER&#125;-resourcemanager-$&#123;hostname&#125;.log</div><div class="line"></div><div class="line">$&#123;USER&#125;是指启动resourcemanager进程的用户</div><div class="line">$&#123;hostname&#125;是resourcemanager进程所在机器的hostname；当日志到达一定的大小（可以在$&#123;HADOOP_HOME&#125;/etc/Hadoop/log4j.properties文件中配置）将会被切割出一个新的文件，切割出来的日志文件名类似yarn-$&#123;USER&#125;-resourcemanager-$&#123;hostname&#125;.log.数字的，后面的数字越大，代表日志越旧。在默认情况下，只保存前20个日志文件</div><div class="line"></div><div class="line">-rw-rw-r--  1 yunyu yunyu  8987088 Oct 27 11:24 yarn-yunyu-nodemanager-hadoop1.log-rw-rw-r--  1 yunyu yunyu      700 Oct 27 10:19 yarn-yunyu-nodemanager-hadoop1.out-rw-rw-r--  1 yunyu yunyu     2062 Oct 26 18:25 yarn-yunyu-nodemanager-hadoop1.out.1-rw-rw-r--  1 yunyu yunyu     2062 Oct 26 17:51 yarn-yunyu-nodemanager-hadoop1.out.2-rw-rw-r--  1 yunyu yunyu      700 Oct 25 16:18 yarn-yunyu-nodemanager-hadoop1.out.3-rw-rw-r--  1 yunyu yunyu     2062 Oct 23 17:54 yarn-yunyu-nodemanager-hadoop1.out.4</div></pre></td></tr></table></figure>
<h3 id="Mapreduce程序输出来的日志"><a href="#Mapreduce程序输出来的日志" class="headerlink" title="Mapreduce程序输出来的日志"></a>Mapreduce程序输出来的日志</h3><p>MapReduce程序输出来的日志又细分为下面两种</p>
<ul>
<li>作业运行日志（历史作业日志）</li>
<li>任务运行日志（Container日志）</li>
</ul>
<h4 id="作业运行日志（历史作业日志）"><a href="#作业运行日志（历史作业日志）" class="headerlink" title="作业运行日志（历史作业日志）"></a>作业运行日志（历史作业日志）</h4><p>作业运行由MRAppMaster（MapReduce作业的ApplicationMaster）产生，详细记录了作业启动时间、运行时间，每个任务启动时间、运行时间、Counter值等信息。这些信息对分析作业是很有帮助的，我们可以通过这些历史作业记录得到每天有多少个作业运行成功、有多少个作业运行失败、每个队列作业运行了多少个作业等很有用的信息。</p>
<p>MapReduce作业的ApplicationMaster也运行在Container中，且是编号为000001的Container，比如container_1385051297072_0001_01_000001，它自身可认为是一个特殊的task，因此，也有自己的运行日志（Container日志），该日志与Map Task和Reduce Task类似，但并不是这里介绍的“作业运行日志”。</p>
<p>作业运行日志和其他的日志文件不一样，是因为这些历史作业记录文件是存储在HDFS上的，而不是存储在本地系统文件中的，可以修改mapred-site.xml配置文件指定对应HDFS的存储路径，而且可以指定正在运行的MapReduce作业和已经完成的MapReduce作业信息在HDFS的存储路径。</p>
<h5 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h5><pre><code>&lt;!-- MapReduce已完成作业信息在HDFS的存储路径 --&gt;
&lt;property&gt;
    &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;
    &lt;value&gt;${yarn.app.mapreduce.am.staging-dir}/history/done&lt;/value&gt;
&lt;/property&gt;
&lt;!-- MapReduce正在运行作业信息在HDFS的存储路径 --&gt;
&lt;property&gt;
    &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;
    &lt;value&gt;${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate&lt;/value&gt;
&lt;/property&gt;
&lt;!-- MapReduce作业信息在HDFS默认的存储路径 --&gt;
&lt;property&gt;
    &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;
    &lt;value&gt;/tmp/hadoop-yarn/staging&lt;/value&gt;
&lt;/property&gt;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">作业运行日志产生过程如下：</div><div class="line"></div><div class="line">- 步骤1：ResourceManager启动作业的ApplicationMaster，ApplicationMaster运行过程中，将日志写到$&#123;yarn.app.mapreduce.am.staging-dir&#125;/yarn/.staging/job_XXXXX_XXX/下，其中参数yarn.app.mapreduce.am.staging-dir 的默认值是/tmp/hadoop-yarn/staging，该目录下将存在3个文件，分别是以&quot;.jhist&quot;、&quot;.summary&quot;和&quot;.xml&quot;结尾的文件，分别表示作业运行日志、作业概要信息和作业配置属性</div><div class="line"></div><div class="line">- 步骤2：所有任务运行完成后，意味着，该作业运行完成，此时ApplicationMaster将三个文件拷贝到$&#123;mapreduce.jobhistory.intermediate-done-dir&#125;/$&#123;username&#125;目录下，拷贝后的文件名后面添加&quot;_tmp&quot;,其中mapreduce.jobhistory.intermediate-done-dir默认值是$&#123;yarn.app.mapreduce.am.staging-dir&#125;/history/done_intermediate</div><div class="line"></div><div class="line">- 步骤3：ApplicationMaster将拷贝完成的三个文件重新命名成&quot;.jhist&quot;、&quot;.summary&quot;和&quot;.xml&quot;结尾的文件（去掉&quot;_tmp&quot;）</div><div class="line"></div><div class="line">- 步骤4：周期性扫描线程定期将done_intermediate的日志文件转移到done目录（通过参数mapreduce.jobhistory.done-dir配置，默认值为$&#123;yarn.app.mapreduce.am.staging-dir&#125;/history/done）下，同时删除&quot;.summary&quot;文件（该文件中的信息，.jhist文件中都有）。</div><div class="line"></div><div class="line">- 步骤5：ApplicationMaster移除</div><div class="line">$&#123;yarn.app.mapreduce.am.staging-dir&#125;/yarn/.staging/job_XXXXX_XXX/目录</div><div class="line">默认情况下，任务运行日志产只会存放在各NodeManager的本地磁盘上，你可以打开日志聚集功能，以便让任务将运行日志推送到HDFS上，以便集中管理和分析。</div><div class="line"></div><div class="line">### 特别需要注意下</div><div class="line"></div><div class="line">默认情况下，NodeManager将日志保存到yarn.nodemanager.log-dirs下，，该属性缺省值为$&#123;yarn.log.dir&#125;/userlogs，也就是Hadoop安装目录下的logs/userlogs目录中，通常为了分摊磁盘负载，我们会为该参数设置多个路径，此外，需要注意的是，ApplicationMaster的自身的日志也存放在该路目下，因为它也运行在Container之中，是一个特殊的task。举例如下，其中，最后一个是某个作业的ApplicationMaster日志（编号是000001）。</div><div class="line"></div><div class="line">即ApplicationMaster日志目录名称为container_XXX_000001，普通task日志目录名称则为container_XXX_000002，container_XXX_000003，...</div><div class="line"></div><div class="line">container_XXX_00000X每个目录下包含三个日志文件：stdout、stderr和syslog</div><div class="line"></div><div class="line">- stderr : 错误文件输出</div><div class="line">- stdout : System.out.println控制台输出，我们自己写的MapReduce程序的System.out.println输出都将写入到此文件中</div><div class="line">- syslog : logger系统日志输出，我们自己的MapReduce程序的logger.info日志记录都将写入到此文件中</div><div class="line"></div><div class="line">#### HistoryServer</div><div class="line"></div><div class="line">Hadoop自带了一个HistoryServer用于查看Mapreduce作业记录，比如用了多少个Map、用了多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。</div><div class="line"></div><div class="line">需要在mapred-site.xml配置文件中配置HistoryServer的通信地址，并且需要我们手动启动HistoryServer服务</div><div class="line"></div><div class="line">##### 启动HistoryServer</div></pre></td></tr></table></figure>

$ mr-jobhistory-daemon.sh   start historyserver
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">##### mapred-site.xml</div></pre></td></tr></table></figure>

&lt;!-- MapReduce JobHistory Server的IPC通信地址，默认端口号是10020 --&gt;
&lt;property&gt;
    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
    &lt;value&gt;hadoop1:10020&lt;/value&gt;
&lt;/property&gt;
&lt;!-- MapReduce JobHistory Server的Web服务器访问地址，默认端口号是19888 --&gt;
&lt;property&gt;
    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
    &lt;value&gt;hadoop1:19888&lt;/value&gt;
&lt;/property&gt;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">我们可以在HDFS上查看一下MapReduce作业信息相关内容</div></pre></td></tr></table></figure>

$ hdfs dfs -ls /data/history/done/2016/10/26/000000
Found 34 items
-rwxrwx---   2 yunyu supergroup      33487 2016-10-26 01:46 /data/history/done/2016/10/26/000000/job_1477390779880_0003-1477471590330-yunyu-wordcount-1477471605995-1-1-SUCCEEDED-default-1477471593992.jhist
-rwxrwx---   2 yunyu supergroup     113716 2016-10-26 01:46 /data/history/done/2016/10/26/000000/job_1477390779880_0003_conf.xml
-rwxrwx---   2 yunyu supergroup      33478 2016-10-26 01:48 /data/history/done/2016/10/26/000000/job_1477390779880_0004-1477471680580-yunyu-wordcount-1477471695328-1-1-SUCCEEDED-default-1477471684537.jhist
-rwxrwx---   2 yunyu supergroup     113716 2016-10-26 01:48 /data/history/done/2016/10/26/000000/job_1477390779880_0004_conf.xml
-rwxrwx---   2 yunyu supergroup      33472 2016-10-26 01:49 /data/history/done/2016/10/26/000000/job_1477390779880_0005-1477471725086-yunyu-wordcount-1477471740048-1-1-SUCCEEDED-default-1477471729179.jhist
-rwxrwx---   2 yunyu supergroup     113716 2016-10-26 01:49 /data/history/done/2016/10/26/000000/job_1477390779880_0005_conf.xml
-rwxrwx---   2 yunyu supergroup      33483 2016-10-26 01:53 /data/history/done/2016/10/26/000000/job_1477390779880_0006-1477471983695-yunyu-wordcount-1477471999289-1-1-SUCCEEDED-default-1477471987996.jhist
-rwxrwx---   2 yunyu supergroup     113716 2016-10-26 01:53 /data/history/done/2016/10/26/000000/job_1477390779880_0006_conf.xml
-rwxrwx---   2 yunyu supergroup      33479 2016-10-26 01:55 /data/history/done/2016/10/26/000000/job_1477390779880_0007-1477472135663-yunyu-wordcount-1477472150775-1-1-SUCCEEDED-default-1477472139406.jhist
-rwxrwx---   2 yunyu supergroup     113716 2016-10-26 01:55 /data/history/done/2016/10/26/000000/job_1477390779880_0007_conf.xml
-rwxrwx---   2 yunyu supergroup      33477 2016-10-26 02:42 /data/history/done/2016/10/26/000000/job_1477390779880_0008-1477474917310-yunyu-wordcount-1477474934028-1-1-SUCCEEDED-default-1477474921803.jhist
-rwxrwx---   2 yunyu supergroup     113716 2016-10-26 02:42 /data/history/done/2016/10/26/000000/job_1477390779880_0008_conf.xml
-rwxrwx---   2 yunyu supergroup      33487 2016-10-26 03:25 /data/history/done/2016/10/26/000000/job_1477477415810_0001-1477477521120-yunyu-wordcount-1477477540308-1-1-SUCCEEDED-default-1477477527364.jhist
-rwxrwx---   2 yunyu supergroup     113712 2016-10-26 03:25 /data/history/done/2016/10/26/000000/job_1477477415810_0001_conf.xml
-rwxrwx---   2 yunyu supergroup      33483 2016-10-26 03:51 /data/history/done/2016/10/26/000000/job_1477477415810_0002-1477479057999-yunyu-wordcount-1477479076500-1-1-SUCCEEDED-default-1477479062997.jhist
-rwxrwx---   2 yunyu supergroup     113712 2016-10-26 03:51 /data/history/done/2016/10/26/000000/job_1477477415810_0002_conf.xml
-rwxrwx---   2 yunyu supergroup      33476 2016-10-26 04:01 /data/history/done/2016/10/26/000000/job_1477477415810_0003-1477479645080-yunyu-wordcount-1477479661516-1-1-SUCCEEDED-default-1477479650248.jhist
-rwxrwx---   2 yunyu supergroup     113712 2016-10-26 04:01 /data/history/done/2016/10/26/000000/job_1477477415810_0003_conf.xml
-rwxrwx---   2 yunyu supergroup      33470 2016-10-26 04:36 /data/history/done/2016/10/26/000000/job_1477477415810_0004-1477481804256-yunyu-wordcount-1477481820774-1-1-SUCCEEDED-default-1477481809917.jhist
-rwxrwx---   2 yunyu supergroup     113638 2016-10-26 04:36 /data/history/done/2016/10/26/000000/job_1477477415810_0004_conf.xml
-rwxrwx---   2 yunyu supergroup      33519 2016-10-26 04:46 /data/history/done/2016/10/26/000000/job_1477477415810_0005-1477482378129-yunyu-wordcount-1477482394056-1-1-SUCCEEDED-default-1477482382630.jhist
-rwxrwx---   2 yunyu supergroup     113682 2016-10-26 04:46 /data/history/done/2016/10/26/000000/job_1477477415810_0005_conf.xml
-rwxrwx---   2 yunyu supergroup      33518 2016-10-26 04:56 /data/history/done/2016/10/26/000000/job_1477477415810_0006-1477482967602-yunyu-wordcount-1477482983257-1-1-SUCCEEDED-default-1477482971751.jhist
-rwxrwx---   2 yunyu supergroup     113682 2016-10-26 04:56 /data/history/done/2016/10/26/000000/job_1477477415810_0006_conf.xml
-rwxrwx---   2 yunyu supergroup      33524 2016-10-26 05:05 /data/history/done/2016/10/26/000000/job_1477477415810_0007-1477483516841-yunyu-wordcount-1477483533885-1-1-SUCCEEDED-default-1477483521388.jhist
-rwxrwx---   2 yunyu supergroup     113682 2016-10-26 05:05 /data/history/done/2016/10/26/000000/job_1477477415810_0007_conf.xml
-rwxrwx---   2 yunyu supergroup      33519 2016-10-26 05:27 /data/history/done/2016/10/26/000000/job_1477477415810_0008-1477484838977-yunyu-wordcount-1477484854521-1-1-SUCCEEDED-default-1477484843086.jhist
-rwxrwx---   2 yunyu supergroup     113682 2016-10-26 05:27 /data/history/done/2016/10/26/000000/job_1477477415810_0008_conf.xml
-rwxrwx---   2 yunyu supergroup      33520 2016-10-26 19:21 /data/history/done/2016/10/26/000000/job_1477534790849_0001-1477534850971-yunyu-wordcount-1477534870748-1-1-SUCCEEDED-default-1477534857063.jhist
-rwxrwx---   2 yunyu supergroup     113699 2016-10-26 19:21 /data/history/done/2016/10/26/000000/job_1477534790849_0001_conf.xml
-rwxrwx---   2 yunyu supergroup      33521 2016-10-26 20:23 /data/history/done/2016/10/26/000000/job_1477534790849_0002-1477538573459-yunyu-wordcount-1477538590195-1-1-SUCCEEDED-default-1477538577756.jhist
-rwxrwx---   2 yunyu supergroup     113699 2016-10-26 20:23 /data/history/done/2016/10/26/000000/job_1477534790849_0002_conf.xml
-rwxrwx---   2 yunyu supergroup      33519 2016-10-26 20:24 /data/history/done/2016/10/26/000000/job_1477534790849_0003-1477538645546-yunyu-wordcount-1477538662701-1-1-SUCCEEDED-default-1477538650360.jhist
-rwxrwx---   2 yunyu supergroup     113699 2016-10-26 20:24 /data/history/done/2016/10/26/000000/job_1477534790849_0003_conf.xml
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">通过上面的结果我们可以得到一下几点：</div><div class="line"></div><div class="line">- （1）历史作业记录是存放在HDFS目录中；</div><div class="line">- （2）由于历史作业记录可能非常多，所以历史作业记录是按照年/月/日的形式分别存放在相应的目录中，这样便于管理和查找；</div><div class="line">- （3）对于每一个Hadoop历史作业记录相关信息都用两个文件存放，后缀名分别为*.jhist，*.xml。*.jhist文件里存放的是具体Hadoop作业的详细信息，如下：</div><div class="line">- （4）每一个作业的历史记录都存放在一个单独的文件中。</div></pre></td></tr></table></figure>

hdfs dfs -cat /data/history/done/2016/10/26/000000/job_1477534790849_0003-1477538645546-yunyu-wordcount-1477538662701-1-1-SUCCEEDED-default-1477538650360.jhist
Avro-Json
{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;Event&quot;,&quot;namespace&quot;:&quot;org.apache.hadoop.mapreduce.jobhistory&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;type&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;enum&quot;,&quot;name&quot;:&quot;EventType&quot;,&quot;symbols&quot;:[&quot;JOB_SUBMITTED&quot;,&quot;JOB_INITED&quot;,&quot;JOB_FINISHED&quot;,&quot;JOB_PRIORITY_CHANGED&quot;,&quot;JOB_STATUS_CHANGED&quot;,&quot;JOB_QUEUE_CHANGED&quot;,&quot;JOB_FAILED&quot;,&quot;JOB_KILLED&quot;,&quot;JOB_ERROR&quot;,&quot;JOB_INFO_CHANGED&quot;,&quot;TASK_STARTED&quot;,&quot;TASK_FINISHED&quot;,&quot;TASK_FAILED&quot;,&quot;TASK_UPDATED&quot;,&quot;NORMALIZED_RESOURCE&quot;,&quot;MAP_ATTEMPT_STARTED&quot;,&quot;MAP_ATTEMPT_FINISHED&quot;,&quot;MAP_ATTEMPT_FAILED&quot;,&quot;MAP_ATTEMPT_KILLED&quot;,&quot;REDUCE_ATTEMPT_STARTED&quot;,&quot;REDUCE_ATTEMPT_FINISHED&quot;,&quot;REDUCE_ATTEMPT_FAILED&quot;,&quot;REDUCE_ATTEMPT_KILLED&quot;,&quot;SETUP_ATTEMPT_STARTED&quot;,&quot;SETUP_ATTEMPT_FINISHED&quot;,&quot;SETUP_ATTEMPT_FAILED&quot;,&quot;SETUP_ATTEMPT_KILLED&quot;,&quot;CLEANUP_ATTEMPT_STARTED&quot;,&quot;CLEANUP_ATTEMPT_FINISHED&quot;,&quot;CLEANUP_ATTEMPT_FAILED&quot;,&quot;CLEANUP_ATTEMPT_KILLED&quot;,&quot;AM_STARTED&quot;]}},{&quot;name&quot;:&quot;event&quot;,&quot;type&quot;:[{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JobFinished&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;jobid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;finishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;finishedMaps&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;finishedReduces&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;failedMaps&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;failedReduces&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;totalCounters&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JhCounters&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;name&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;groups&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JhCounterGroup&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;name&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;displayName&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;counts&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JhCounter&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;name&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;displayName&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;value&quot;,&quot;type&quot;:&quot;long&quot;}]}}}]}}}]}},{&quot;name&quot;:&quot;mapCounters&quot;,&quot;type&quot;:&quot;JhCounters&quot;},{&quot;name&quot;:&quot;reduceCounters&quot;,&quot;type&quot;:&quot;JhCounters&quot;}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JobInfoChange&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;jobid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;submitTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;launchTime&quot;,&quot;type&quot;:&quot;long&quot;}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JobInited&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;jobid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;launchTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;totalMaps&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;totalReduces&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;jobStatus&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;uberized&quot;,&quot;type&quot;:&quot;boolean&quot;}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;AMStarted&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;applicationAttemptId&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;startTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;containerId&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;nodeManagerHost&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;nodeManagerPort&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;nodeManagerHttpPort&quot;,&quot;type&quot;:&quot;int&quot;}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JobPriorityChange&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;jobid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;priority&quot;,&quot;type&quot;:&quot;string&quot;}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JobQueueChange&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;jobid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;jobQueueName&quot;,&quot;type&quot;:&quot;string&quot;}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JobStatusChanged&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;jobid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;jobStatus&quot;,&quot;type&quot;:&quot;string&quot;}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JobSubmitted&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;jobid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;jobName&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;userName&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;submitTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;jobConfPath&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;acls&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;map&quot;,&quot;values&quot;:&quot;string&quot;}},{&quot;name&quot;:&quot;jobQueueName&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;workflowId&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null},{&quot;name&quot;:&quot;workflowName&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null},{&quot;name&quot;:&quot;workflowNodeName&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null},{&quot;name&quot;:&quot;workflowAdjacencies&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null},{&quot;name&quot;:&quot;workflowTags&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;JobUnsuccessfulCompletion&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;jobid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;finishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;finishedMaps&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;finishedReduces&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;jobStatus&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;diagnostics&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;MapAttemptFinished&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;taskid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;attemptId&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskType&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskStatus&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;mapFinishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;finishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;hostname&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;port&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;rackname&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;state&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;counters&quot;,&quot;type&quot;:&quot;JhCounters&quot;},{&quot;name&quot;:&quot;clockSplits&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}},{&quot;name&quot;:&quot;cpuUsages&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}},{&quot;name&quot;:&quot;vMemKbytes&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}},{&quot;name&quot;:&quot;physMemKbytes&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;ReduceAttemptFinished&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;taskid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;attemptId&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskType&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskStatus&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;shuffleFinishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;sortFinishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;finishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;hostname&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;port&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;rackname&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;state&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;counters&quot;,&quot;type&quot;:&quot;JhCounters&quot;},{&quot;name&quot;:&quot;clockSplits&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}},{&quot;name&quot;:&quot;cpuUsages&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}},{&quot;name&quot;:&quot;vMemKbytes&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}},{&quot;name&quot;:&quot;physMemKbytes&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;TaskAttemptFinished&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;taskid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;attemptId&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskType&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskStatus&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;finishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;rackname&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;hostname&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;state&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;counters&quot;,&quot;type&quot;:&quot;JhCounters&quot;}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;TaskAttemptStarted&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;taskid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskType&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;attemptId&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;startTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;trackerName&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;httpPort&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;shufflePort&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;containerId&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;locality&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null},{&quot;name&quot;:&quot;avataar&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;TaskAttemptUnsuccessfulCompletion&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;taskid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskType&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;attemptId&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;finishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;hostname&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;port&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;rackname&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;status&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;error&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;counters&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;JhCounters&quot;],&quot;default&quot;:null},{&quot;name&quot;:&quot;clockSplits&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}},{&quot;name&quot;:&quot;cpuUsages&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}},{&quot;name&quot;:&quot;vMemKbytes&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}},{&quot;name&quot;:&quot;physMemKbytes&quot;,&quot;type&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:&quot;int&quot;}}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;TaskFailed&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;taskid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskType&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;finishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;error&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;failedDueToAttempt&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;]},{&quot;name&quot;:&quot;status&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;counters&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;JhCounters&quot;],&quot;default&quot;:null}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;TaskFinished&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;taskid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskType&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;finishTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;status&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;counters&quot;,&quot;type&quot;:&quot;JhCounters&quot;},{&quot;name&quot;:&quot;successfulAttemptId&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;TaskStarted&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;taskid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;taskType&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;startTime&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;splitLocations&quot;,&quot;type&quot;:&quot;string&quot;}]},{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;TaskUpdated&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;taskid&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;finishTime&quot;,&quot;type&quot;:&quot;long&quot;}]}]}]}
{&quot;type&quot;:&quot;AM_STARTED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.AMStarted&quot;:{&quot;applicationAttemptId&quot;:&quot;appattempt_1477534790849_0003_000001&quot;,&quot;startTime&quot;:1477538647394,&quot;containerId&quot;:&quot;container_1477534790849_0003_01_000001&quot;,&quot;nodeManagerHost&quot;:&quot;hadoop1&quot;,&quot;nodeManagerPort&quot;:47596,&quot;nodeManagerHttpPort&quot;:8042}}}

{&quot;type&quot;:&quot;JOB_SUBMITTED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.JobSubmitted&quot;:{&quot;jobid&quot;:&quot;job_1477534790849_0003&quot;,&quot;jobName&quot;:&quot;wordcount&quot;,&quot;userName&quot;:&quot;yunyu&quot;,&quot;submitTime&quot;:1477538645546,&quot;jobConfPath&quot;:&quot;hdfs://hadoop1/tmp/hadoop-yarn/staging/yunyu/.staging/job_1477534790849_0003/job.xml&quot;,&quot;acls&quot;:{},&quot;jobQueueName&quot;:&quot;default&quot;,&quot;workflowId&quot;:{&quot;string&quot;:&quot;&quot;},&quot;workflowName&quot;:{&quot;string&quot;:&quot;&quot;},&quot;workflowNodeName&quot;:{&quot;string&quot;:&quot;&quot;},&quot;workflowAdjacencies&quot;:{&quot;string&quot;:&quot;&quot;},&quot;workflowTags&quot;:{&quot;string&quot;:&quot;&quot;}}}}

{&quot;type&quot;:&quot;JOB_QUEUE_CHANGED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.JobQueueChange&quot;:{&quot;jobid&quot;:&quot;job_1477534790849_0003&quot;,&quot;jobQueueName&quot;:&quot;default&quot;}}}

{&quot;type&quot;:&quot;JOB_INITED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.JobInited&quot;:{&quot;jobid&quot;:&quot;job_1477534790849_0003&quot;,&quot;launchTime&quot;:1477538650360,&quot;totalMaps&quot;:1,&quot;totalReduces&quot;:1,&quot;jobStatus&quot;:&quot;INITED&quot;,&quot;uberized&quot;:false}}}

{&quot;type&quot;:&quot;JOB_INFO_CHANGED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.JobInfoChange&quot;:{&quot;jobid&quot;:&quot;job_1477534790849_0003&quot;,&quot;submitTime&quot;:1477538645546,&quot;launchTime&quot;:1477538650360}}}

{&quot;type&quot;:&quot;TASK_STARTED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.TaskStarted&quot;:{&quot;taskid&quot;:&quot;task_1477534790849_0003_m_000000&quot;,&quot;taskType&quot;:&quot;MAP&quot;,&quot;startTime&quot;:1477538650763,&quot;splitLocations&quot;:&quot;hadoop1,hadoop2&quot;}}}

{&quot;type&quot;:&quot;TASK_STARTED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.TaskStarted&quot;:{&quot;taskid&quot;:&quot;task_1477534790849_0003_r_000000&quot;,&quot;taskType&quot;:&quot;REDUCE&quot;,&quot;startTime&quot;:1477538650767,&quot;splitLocations&quot;:&quot;&quot;}}}

{&quot;type&quot;:&quot;MAP_ATTEMPT_STARTED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.TaskAttemptStarted&quot;:{&quot;taskid&quot;:&quot;task_1477534790849_0003_m_000000&quot;,&quot;taskType&quot;:&quot;MAP&quot;,&quot;attemptId&quot;:&quot;attempt_1477534790849_0003_m_000000_0&quot;,&quot;startTime&quot;:1477538652833,&quot;trackerName&quot;:&quot;hadoop2&quot;,&quot;httpPort&quot;:8042,&quot;shufflePort&quot;:13562,&quot;containerId&quot;:&quot;container_1477534790849_0003_01_000002&quot;,&quot;locality&quot;:{&quot;string&quot;:&quot;NODE_LOCAL&quot;},&quot;avataar&quot;:{&quot;string&quot;:&quot;VIRGIN&quot;}}}}

{&quot;type&quot;:&quot;MAP_ATTEMPT_FINISHED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.MapAttemptFinished&quot;:{&quot;taskid&quot;:&quot;task_1477534790849_0003_m_000000&quot;,&quot;attemptId&quot;:&quot;attempt_1477534790849_0003_m_000000_0&quot;,&quot;taskType&quot;:&quot;MAP&quot;,&quot;taskStatus&quot;:&quot;SUCCEEDED&quot;,&quot;mapFinishTime&quot;:1477538656711,&quot;finishTime&quot;:1477538656933,&quot;hostname&quot;:&quot;hadoop2&quot;,&quot;port&quot;:34358,&quot;rackname&quot;:&quot;/default-rack&quot;,&quot;state&quot;:&quot;map&quot;,&quot;counters&quot;:{&quot;name&quot;:&quot;COUNTERS&quot;,&quot;groups&quot;:[{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.FileSystemCounter&quot;,&quot;displayName&quot;:&quot;File System Counters&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;FILE_BYTES_READ&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes read&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes written&quot;,&quot;value&quot;:115420},{&quot;name&quot;:&quot;FILE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of write operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_BYTES_READ&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes read&quot;,&quot;value&quot;:194},{&quot;name&quot;:&quot;HDFS_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes written&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of read operations&quot;,&quot;value&quot;:3},{&quot;name&quot;:&quot;HDFS_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of write operations&quot;,&quot;value&quot;:0}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.TaskCounter&quot;,&quot;displayName&quot;:&quot;Map-Reduce Framework&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;MAP_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Map input records&quot;,&quot;value&quot;:4},{&quot;name&quot;:&quot;MAP_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Map output records&quot;,&quot;value&quot;:14},{&quot;name&quot;:&quot;MAP_OUTPUT_BYTES&quot;,&quot;displayName&quot;:&quot;Map output bytes&quot;,&quot;value&quot;:141},{&quot;name&quot;:&quot;MAP_OUTPUT_MATERIALIZED_BYTES&quot;,&quot;displayName&quot;:&quot;Map output materialized bytes&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;SPLIT_RAW_BYTES&quot;,&quot;displayName&quot;:&quot;Input split bytes&quot;,&quot;value&quot;:109},{&quot;name&quot;:&quot;COMBINE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine input records&quot;,&quot;value&quot;:14},{&quot;name&quot;:&quot;COMBINE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine output records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;SPILLED_RECORDS&quot;,&quot;displayName&quot;:&quot;Spilled Records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;FAILED_SHUFFLE&quot;,&quot;displayName&quot;:&quot;Failed Shuffles&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;MERGED_MAP_OUTPUTS&quot;,&quot;displayName&quot;:&quot;Merged Map outputs&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;GC_TIME_MILLIS&quot;,&quot;displayName&quot;:&quot;GC time elapsed (ms)&quot;,&quot;value&quot;:83},{&quot;name&quot;:&quot;CPU_MILLISECONDS&quot;,&quot;displayName&quot;:&quot;CPU time spent (ms)&quot;,&quot;value&quot;:490},{&quot;name&quot;:&quot;PHYSICAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Physical memory (bytes) snapshot&quot;,&quot;value&quot;:216502272},{&quot;name&quot;:&quot;VIRTUAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Virtual memory (bytes) snapshot&quot;,&quot;value&quot;:666292224},{&quot;name&quot;:&quot;COMMITTED_HEAP_BYTES&quot;,&quot;displayName&quot;:&quot;Total committed heap usage (bytes)&quot;,&quot;value&quot;:120721408}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter&quot;,&quot;displayName&quot;:&quot;File Input Format Counters &quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BYTES_READ&quot;,&quot;displayName&quot;:&quot;Bytes Read&quot;,&quot;value&quot;:85}]}]},&quot;clockSplits&quot;:[3941,11,12,11,11,12,11,11,12,11,11,12],&quot;cpuUsages&quot;:[40,41,41,41,41,41,40,41,41,41,41,41],&quot;vMemKbytes&quot;:[27111,81334,135557,189780,244003,298226,352449,406672,460895,515118,569341,623564],&quot;physMemKbytes&quot;:[8809,26428,44047,61666,79285,96904,114523,132142,149761,167380,184999,202618]}}}

{&quot;type&quot;:&quot;TASK_FINISHED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.TaskFinished&quot;:{&quot;taskid&quot;:&quot;task_1477534790849_0003_m_000000&quot;,&quot;taskType&quot;:&quot;MAP&quot;,&quot;finishTime&quot;:1477538656933,&quot;status&quot;:&quot;SUCCEEDED&quot;,&quot;counters&quot;:{&quot;name&quot;:&quot;COUNTERS&quot;,&quot;groups&quot;:[{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.FileSystemCounter&quot;,&quot;displayName&quot;:&quot;File System Counters&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;FILE_BYTES_READ&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes read&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes written&quot;,&quot;value&quot;:115420},{&quot;name&quot;:&quot;FILE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of write operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_BYTES_READ&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes read&quot;,&quot;value&quot;:194},{&quot;name&quot;:&quot;HDFS_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes written&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of read operations&quot;,&quot;value&quot;:3},{&quot;name&quot;:&quot;HDFS_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of write operations&quot;,&quot;value&quot;:0}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.TaskCounter&quot;,&quot;displayName&quot;:&quot;Map-Reduce Framework&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;MAP_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Map input records&quot;,&quot;value&quot;:4},{&quot;name&quot;:&quot;MAP_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Map output records&quot;,&quot;value&quot;:14},{&quot;name&quot;:&quot;MAP_OUTPUT_BYTES&quot;,&quot;displayName&quot;:&quot;Map output bytes&quot;,&quot;value&quot;:141},{&quot;name&quot;:&quot;MAP_OUTPUT_MATERIALIZED_BYTES&quot;,&quot;displayName&quot;:&quot;Map output materialized bytes&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;SPLIT_RAW_BYTES&quot;,&quot;displayName&quot;:&quot;Input split bytes&quot;,&quot;value&quot;:109},{&quot;name&quot;:&quot;COMBINE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine input records&quot;,&quot;value&quot;:14},{&quot;name&quot;:&quot;COMBINE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine output records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;SPILLED_RECORDS&quot;,&quot;displayName&quot;:&quot;Spilled Records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;FAILED_SHUFFLE&quot;,&quot;displayName&quot;:&quot;Failed Shuffles&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;MERGED_MAP_OUTPUTS&quot;,&quot;displayName&quot;:&quot;Merged Map outputs&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;GC_TIME_MILLIS&quot;,&quot;displayName&quot;:&quot;GC time elapsed (ms)&quot;,&quot;value&quot;:83},{&quot;name&quot;:&quot;CPU_MILLISECONDS&quot;,&quot;displayName&quot;:&quot;CPU time spent (ms)&quot;,&quot;value&quot;:490},{&quot;name&quot;:&quot;PHYSICAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Physical memory (bytes) snapshot&quot;,&quot;value&quot;:216502272},{&quot;name&quot;:&quot;VIRTUAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Virtual memory (bytes) snapshot&quot;,&quot;value&quot;:666292224},{&quot;name&quot;:&quot;COMMITTED_HEAP_BYTES&quot;,&quot;displayName&quot;:&quot;Total committed heap usage (bytes)&quot;,&quot;value&quot;:120721408}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter&quot;,&quot;displayName&quot;:&quot;File Input Format Counters &quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BYTES_READ&quot;,&quot;displayName&quot;:&quot;Bytes Read&quot;,&quot;value&quot;:85}]}]},&quot;successfulAttemptId&quot;:{&quot;string&quot;:&quot;attempt_1477534790849_0003_m_000000_0&quot;}}}}

{&quot;type&quot;:&quot;REDUCE_ATTEMPT_STARTED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.TaskAttemptStarted&quot;:{&quot;taskid&quot;:&quot;task_1477534790849_0003_r_000000&quot;,&quot;taskType&quot;:&quot;REDUCE&quot;,&quot;attemptId&quot;:&quot;attempt_1477534790849_0003_r_000000_0&quot;,&quot;startTime&quot;:1477538659565,&quot;trackerName&quot;:&quot;hadoop2&quot;,&quot;httpPort&quot;:8042,&quot;shufflePort&quot;:13562,&quot;containerId&quot;:&quot;container_1477534790849_0003_01_000003&quot;,&quot;locality&quot;:{&quot;string&quot;:&quot;OFF_SWITCH&quot;},&quot;avataar&quot;:{&quot;string&quot;:&quot;VIRGIN&quot;}}}}

{&quot;type&quot;:&quot;REDUCE_ATTEMPT_FINISHED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.ReduceAttemptFinished&quot;:{&quot;taskid&quot;:&quot;task_1477534790849_0003_r_000000&quot;,&quot;attemptId&quot;:&quot;attempt_1477534790849_0003_r_000000_0&quot;,&quot;taskType&quot;:&quot;REDUCE&quot;,&quot;taskStatus&quot;:&quot;SUCCEEDED&quot;,&quot;shuffleFinishTime&quot;:1477538662152,&quot;sortFinishTime&quot;:1477538662173,&quot;finishTime&quot;:1477538662652,&quot;hostname&quot;:&quot;hadoop2&quot;,&quot;port&quot;:34358,&quot;rackname&quot;:&quot;/default-rack&quot;,&quot;state&quot;:&quot;reduce &gt; reduce&quot;,&quot;counters&quot;:{&quot;name&quot;:&quot;COUNTERS&quot;,&quot;groups&quot;:[{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.FileSystemCounter&quot;,&quot;displayName&quot;:&quot;File System Counters&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;FILE_BYTES_READ&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes read&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;FILE_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes written&quot;,&quot;value&quot;:115365},{&quot;name&quot;:&quot;FILE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of write operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_BYTES_READ&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes read&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes written&quot;,&quot;value&quot;:72},{&quot;name&quot;:&quot;HDFS_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of read operations&quot;,&quot;value&quot;:3},{&quot;name&quot;:&quot;HDFS_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of write operations&quot;,&quot;value&quot;:2}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.TaskCounter&quot;,&quot;displayName&quot;:&quot;Map-Reduce Framework&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;COMBINE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine input records&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;COMBINE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine output records&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;REDUCE_INPUT_GROUPS&quot;,&quot;displayName&quot;:&quot;Reduce input groups&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;REDUCE_SHUFFLE_BYTES&quot;,&quot;displayName&quot;:&quot;Reduce shuffle bytes&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;REDUCE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Reduce input records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;REDUCE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Reduce output records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;SPILLED_RECORDS&quot;,&quot;displayName&quot;:&quot;Spilled Records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;SHUFFLED_MAPS&quot;,&quot;displayName&quot;:&quot;Shuffled Maps &quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;FAILED_SHUFFLE&quot;,&quot;displayName&quot;:&quot;Failed Shuffles&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;MERGED_MAP_OUTPUTS&quot;,&quot;displayName&quot;:&quot;Merged Map outputs&quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;GC_TIME_MILLIS&quot;,&quot;displayName&quot;:&quot;GC time elapsed (ms)&quot;,&quot;value&quot;:60},{&quot;name&quot;:&quot;CPU_MILLISECONDS&quot;,&quot;displayName&quot;:&quot;CPU time spent (ms)&quot;,&quot;value&quot;:820},{&quot;name&quot;:&quot;PHYSICAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Physical memory (bytes) snapshot&quot;,&quot;value&quot;:120524800},{&quot;name&quot;:&quot;VIRTUAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Virtual memory (bytes) snapshot&quot;,&quot;value&quot;:672628736},{&quot;name&quot;:&quot;COMMITTED_HEAP_BYTES&quot;,&quot;displayName&quot;:&quot;Total committed heap usage (bytes)&quot;,&quot;value&quot;:15728640}]},{&quot;name&quot;:&quot;Shuffle Errors&quot;,&quot;displayName&quot;:&quot;Shuffle Errors&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BAD_ID&quot;,&quot;displayName&quot;:&quot;BAD_ID&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;CONNECTION&quot;,&quot;displayName&quot;:&quot;CONNECTION&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;IO_ERROR&quot;,&quot;displayName&quot;:&quot;IO_ERROR&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_LENGTH&quot;,&quot;displayName&quot;:&quot;WRONG_LENGTH&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_MAP&quot;,&quot;displayName&quot;:&quot;WRONG_MAP&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_REDUCE&quot;,&quot;displayName&quot;:&quot;WRONG_REDUCE&quot;,&quot;value&quot;:0}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter&quot;,&quot;displayName&quot;:&quot;File Output Format Counters &quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;Bytes Written&quot;,&quot;value&quot;:72}]}]},&quot;clockSplits&quot;:[2689,35,35,35,35,35,35,35,35,35,35,36],&quot;cpuUsages&quot;:[68,68,69,68,68,69,68,68,69,68,68,69],&quot;vMemKbytes&quot;:[27369,82107,136846,191584,246323,301062,355801,410539,465278,520017,574755,629494],&quot;physMemKbytes&quot;:[4904,14712,24520,34328,44137,53945,63754,73561,83370,93179,102986,112795]}}}

{&quot;type&quot;:&quot;TASK_FINISHED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.TaskFinished&quot;:{&quot;taskid&quot;:&quot;task_1477534790849_0003_r_000000&quot;,&quot;taskType&quot;:&quot;REDUCE&quot;,&quot;finishTime&quot;:1477538662652,&quot;status&quot;:&quot;SUCCEEDED&quot;,&quot;counters&quot;:{&quot;name&quot;:&quot;COUNTERS&quot;,&quot;groups&quot;:[{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.FileSystemCounter&quot;,&quot;displayName&quot;:&quot;File System Counters&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;FILE_BYTES_READ&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes read&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;FILE_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes written&quot;,&quot;value&quot;:115365},{&quot;name&quot;:&quot;FILE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of write operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_BYTES_READ&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes read&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes written&quot;,&quot;value&quot;:72},{&quot;name&quot;:&quot;HDFS_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of read operations&quot;,&quot;value&quot;:3},{&quot;name&quot;:&quot;HDFS_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of write operations&quot;,&quot;value&quot;:2}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.TaskCounter&quot;,&quot;displayName&quot;:&quot;Map-Reduce Framework&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;COMBINE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine input records&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;COMBINE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine output records&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;REDUCE_INPUT_GROUPS&quot;,&quot;displayName&quot;:&quot;Reduce input groups&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;REDUCE_SHUFFLE_BYTES&quot;,&quot;displayName&quot;:&quot;Reduce shuffle bytes&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;REDUCE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Reduce input records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;REDUCE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Reduce output records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;SPILLED_RECORDS&quot;,&quot;displayName&quot;:&quot;Spilled Records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;SHUFFLED_MAPS&quot;,&quot;displayName&quot;:&quot;Shuffled Maps &quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;FAILED_SHUFFLE&quot;,&quot;displayName&quot;:&quot;Failed Shuffles&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;MERGED_MAP_OUTPUTS&quot;,&quot;displayName&quot;:&quot;Merged Map outputs&quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;GC_TIME_MILLIS&quot;,&quot;displayName&quot;:&quot;GC time elapsed (ms)&quot;,&quot;value&quot;:60},{&quot;name&quot;:&quot;CPU_MILLISECONDS&quot;,&quot;displayName&quot;:&quot;CPU time spent (ms)&quot;,&quot;value&quot;:820},{&quot;name&quot;:&quot;PHYSICAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Physical memory (bytes) snapshot&quot;,&quot;value&quot;:120524800},{&quot;name&quot;:&quot;VIRTUAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Virtual memory (bytes) snapshot&quot;,&quot;value&quot;:672628736},{&quot;name&quot;:&quot;COMMITTED_HEAP_BYTES&quot;,&quot;displayName&quot;:&quot;Total committed heap usage (bytes)&quot;,&quot;value&quot;:15728640}]},{&quot;name&quot;:&quot;Shuffle Errors&quot;,&quot;displayName&quot;:&quot;Shuffle Errors&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BAD_ID&quot;,&quot;displayName&quot;:&quot;BAD_ID&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;CONNECTION&quot;,&quot;displayName&quot;:&quot;CONNECTION&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;IO_ERROR&quot;,&quot;displayName&quot;:&quot;IO_ERROR&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_LENGTH&quot;,&quot;displayName&quot;:&quot;WRONG_LENGTH&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_MAP&quot;,&quot;displayName&quot;:&quot;WRONG_MAP&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_REDUCE&quot;,&quot;displayName&quot;:&quot;WRONG_REDUCE&quot;,&quot;value&quot;:0}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter&quot;,&quot;displayName&quot;:&quot;File Output Format Counters &quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;Bytes Written&quot;,&quot;value&quot;:72}]}]},&quot;successfulAttemptId&quot;:{&quot;string&quot;:&quot;attempt_1477534790849_0003_r_000000_0&quot;}}}}

{&quot;type&quot;:&quot;JOB_FINISHED&quot;,&quot;event&quot;:{&quot;org.apache.hadoop.mapreduce.jobhistory.JobFinished&quot;:{&quot;jobid&quot;:&quot;job_1477534790849_0003&quot;,&quot;finishTime&quot;:1477538662701,&quot;finishedMaps&quot;:1,&quot;finishedReduces&quot;:1,&quot;failedMaps&quot;:0,&quot;failedReduces&quot;:0,&quot;totalCounters&quot;:{&quot;name&quot;:&quot;TOTAL_COUNTERS&quot;,&quot;groups&quot;:[{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.FileSystemCounter&quot;,&quot;displayName&quot;:&quot;File System Counters&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;FILE_BYTES_READ&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes read&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;FILE_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes written&quot;,&quot;value&quot;:230785},{&quot;name&quot;:&quot;FILE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of write operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_BYTES_READ&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes read&quot;,&quot;value&quot;:194},{&quot;name&quot;:&quot;HDFS_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes written&quot;,&quot;value&quot;:72},{&quot;name&quot;:&quot;HDFS_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of read operations&quot;,&quot;value&quot;:6},{&quot;name&quot;:&quot;HDFS_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of write operations&quot;,&quot;value&quot;:2}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.JobCounter&quot;,&quot;displayName&quot;:&quot;Job Counters &quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;TOTAL_LAUNCHED_MAPS&quot;,&quot;displayName&quot;:&quot;Launched map tasks&quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;TOTAL_LAUNCHED_REDUCES&quot;,&quot;displayName&quot;:&quot;Launched reduce tasks&quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;DATA_LOCAL_MAPS&quot;,&quot;displayName&quot;:&quot;Data-local map tasks&quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;SLOTS_MILLIS_MAPS&quot;,&quot;displayName&quot;:&quot;Total time spent by all maps in occupied slots (ms)&quot;,&quot;value&quot;:4100},{&quot;name&quot;:&quot;SLOTS_MILLIS_REDUCES&quot;,&quot;displayName&quot;:&quot;Total time spent by all reduces in occupied slots (ms)&quot;,&quot;value&quot;:3087},{&quot;name&quot;:&quot;MILLIS_MAPS&quot;,&quot;displayName&quot;:&quot;Total time spent by all map tasks (ms)&quot;,&quot;value&quot;:4100},{&quot;name&quot;:&quot;MILLIS_REDUCES&quot;,&quot;displayName&quot;:&quot;Total time spent by all reduce tasks (ms)&quot;,&quot;value&quot;:3087},{&quot;name&quot;:&quot;VCORES_MILLIS_MAPS&quot;,&quot;displayName&quot;:&quot;Total vcore-seconds taken by all map tasks&quot;,&quot;value&quot;:4100},{&quot;name&quot;:&quot;VCORES_MILLIS_REDUCES&quot;,&quot;displayName&quot;:&quot;Total vcore-seconds taken by all reduce tasks&quot;,&quot;value&quot;:3087},{&quot;name&quot;:&quot;MB_MILLIS_MAPS&quot;,&quot;displayName&quot;:&quot;Total megabyte-seconds taken by all map tasks&quot;,&quot;value&quot;:4198400},{&quot;name&quot;:&quot;MB_MILLIS_REDUCES&quot;,&quot;displayName&quot;:&quot;Total megabyte-seconds taken by all reduce tasks&quot;,&quot;value&quot;:3161088}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.TaskCounter&quot;,&quot;displayName&quot;:&quot;Map-Reduce Framework&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;MAP_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Map input records&quot;,&quot;value&quot;:4},{&quot;name&quot;:&quot;MAP_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Map output records&quot;,&quot;value&quot;:14},{&quot;name&quot;:&quot;MAP_OUTPUT_BYTES&quot;,&quot;displayName&quot;:&quot;Map output bytes&quot;,&quot;value&quot;:141},{&quot;name&quot;:&quot;MAP_OUTPUT_MATERIALIZED_BYTES&quot;,&quot;displayName&quot;:&quot;Map output materialized bytes&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;SPLIT_RAW_BYTES&quot;,&quot;displayName&quot;:&quot;Input split bytes&quot;,&quot;value&quot;:109},{&quot;name&quot;:&quot;COMBINE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine input records&quot;,&quot;value&quot;:14},{&quot;name&quot;:&quot;COMBINE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine output records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;REDUCE_INPUT_GROUPS&quot;,&quot;displayName&quot;:&quot;Reduce input groups&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;REDUCE_SHUFFLE_BYTES&quot;,&quot;displayName&quot;:&quot;Reduce shuffle bytes&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;REDUCE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Reduce input records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;REDUCE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Reduce output records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;SPILLED_RECORDS&quot;,&quot;displayName&quot;:&quot;Spilled Records&quot;,&quot;value&quot;:18},{&quot;name&quot;:&quot;SHUFFLED_MAPS&quot;,&quot;displayName&quot;:&quot;Shuffled Maps &quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;FAILED_SHUFFLE&quot;,&quot;displayName&quot;:&quot;Failed Shuffles&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;MERGED_MAP_OUTPUTS&quot;,&quot;displayName&quot;:&quot;Merged Map outputs&quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;GC_TIME_MILLIS&quot;,&quot;displayName&quot;:&quot;GC time elapsed (ms)&quot;,&quot;value&quot;:143},{&quot;name&quot;:&quot;CPU_MILLISECONDS&quot;,&quot;displayName&quot;:&quot;CPU time spent (ms)&quot;,&quot;value&quot;:1310},{&quot;name&quot;:&quot;PHYSICAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Physical memory (bytes) snapshot&quot;,&quot;value&quot;:337027072},{&quot;name&quot;:&quot;VIRTUAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Virtual memory (bytes) snapshot&quot;,&quot;value&quot;:1338920960},{&quot;name&quot;:&quot;COMMITTED_HEAP_BYTES&quot;,&quot;displayName&quot;:&quot;Total committed heap usage (bytes)&quot;,&quot;value&quot;:136450048}]},{&quot;name&quot;:&quot;Shuffle Errors&quot;,&quot;displayName&quot;:&quot;Shuffle Errors&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BAD_ID&quot;,&quot;displayName&quot;:&quot;BAD_ID&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;CONNECTION&quot;,&quot;displayName&quot;:&quot;CONNECTION&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;IO_ERROR&quot;,&quot;displayName&quot;:&quot;IO_ERROR&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_LENGTH&quot;,&quot;displayName&quot;:&quot;WRONG_LENGTH&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_MAP&quot;,&quot;displayName&quot;:&quot;WRONG_MAP&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_REDUCE&quot;,&quot;displayName&quot;:&quot;WRONG_REDUCE&quot;,&quot;value&quot;:0}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter&quot;,&quot;displayName&quot;:&quot;File Input Format Counters &quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BYTES_READ&quot;,&quot;displayName&quot;:&quot;Bytes Read&quot;,&quot;value&quot;:85}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter&quot;,&quot;displayName&quot;:&quot;File Output Format Counters &quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;Bytes Written&quot;,&quot;value&quot;:72}]}]},&quot;mapCounters&quot;:{&quot;name&quot;:&quot;MAP_COUNTERS&quot;,&quot;groups&quot;:[{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.FileSystemCounter&quot;,&quot;displayName&quot;:&quot;File System Counters&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;FILE_BYTES_READ&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes read&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes written&quot;,&quot;value&quot;:115420},{&quot;name&quot;:&quot;FILE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of write operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_BYTES_READ&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes read&quot;,&quot;value&quot;:194},{&quot;name&quot;:&quot;HDFS_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes written&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of read operations&quot;,&quot;value&quot;:3},{&quot;name&quot;:&quot;HDFS_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of write operations&quot;,&quot;value&quot;:0}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.TaskCounter&quot;,&quot;displayName&quot;:&quot;Map-Reduce Framework&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;MAP_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Map input records&quot;,&quot;value&quot;:4},{&quot;name&quot;:&quot;MAP_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Map output records&quot;,&quot;value&quot;:14},{&quot;name&quot;:&quot;MAP_OUTPUT_BYTES&quot;,&quot;displayName&quot;:&quot;Map output bytes&quot;,&quot;value&quot;:141},{&quot;name&quot;:&quot;MAP_OUTPUT_MATERIALIZED_BYTES&quot;,&quot;displayName&quot;:&quot;Map output materialized bytes&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;SPLIT_RAW_BYTES&quot;,&quot;displayName&quot;:&quot;Input split bytes&quot;,&quot;value&quot;:109},{&quot;name&quot;:&quot;COMBINE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine input records&quot;,&quot;value&quot;:14},{&quot;name&quot;:&quot;COMBINE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine output records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;SPILLED_RECORDS&quot;,&quot;displayName&quot;:&quot;Spilled Records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;FAILED_SHUFFLE&quot;,&quot;displayName&quot;:&quot;Failed Shuffles&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;MERGED_MAP_OUTPUTS&quot;,&quot;displayName&quot;:&quot;Merged Map outputs&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;GC_TIME_MILLIS&quot;,&quot;displayName&quot;:&quot;GC time elapsed (ms)&quot;,&quot;value&quot;:83},{&quot;name&quot;:&quot;CPU_MILLISECONDS&quot;,&quot;displayName&quot;:&quot;CPU time spent (ms)&quot;,&quot;value&quot;:490},{&quot;name&quot;:&quot;PHYSICAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Physical memory (bytes) snapshot&quot;,&quot;value&quot;:216502272},{&quot;name&quot;:&quot;VIRTUAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Virtual memory (bytes) snapshot&quot;,&quot;value&quot;:666292224},{&quot;name&quot;:&quot;COMMITTED_HEAP_BYTES&quot;,&quot;displayName&quot;:&quot;Total committed heap usage (bytes)&quot;,&quot;value&quot;:120721408}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter&quot;,&quot;displayName&quot;:&quot;File Input Format Counters &quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BYTES_READ&quot;,&quot;displayName&quot;:&quot;Bytes Read&quot;,&quot;value&quot;:85}]}]},&quot;reduceCounters&quot;:{&quot;name&quot;:&quot;REDUCE_COUNTERS&quot;,&quot;groups&quot;:[{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.FileSystemCounter&quot;,&quot;displayName&quot;:&quot;File System Counters&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;FILE_BYTES_READ&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes read&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;FILE_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;FILE: Number of bytes written&quot;,&quot;value&quot;:115365},{&quot;name&quot;:&quot;FILE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;FILE_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;FILE: Number of write operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_BYTES_READ&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes read&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;HDFS: Number of bytes written&quot;,&quot;value&quot;:72},{&quot;name&quot;:&quot;HDFS_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of read operations&quot;,&quot;value&quot;:3},{&quot;name&quot;:&quot;HDFS_LARGE_READ_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of large read operations&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;HDFS_WRITE_OPS&quot;,&quot;displayName&quot;:&quot;HDFS: Number of write operations&quot;,&quot;value&quot;:2}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.TaskCounter&quot;,&quot;displayName&quot;:&quot;Map-Reduce Framework&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;COMBINE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine input records&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;COMBINE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Combine output records&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;REDUCE_INPUT_GROUPS&quot;,&quot;displayName&quot;:&quot;Reduce input groups&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;REDUCE_SHUFFLE_BYTES&quot;,&quot;displayName&quot;:&quot;Reduce shuffle bytes&quot;,&quot;value&quot;:114},{&quot;name&quot;:&quot;REDUCE_INPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Reduce input records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;REDUCE_OUTPUT_RECORDS&quot;,&quot;displayName&quot;:&quot;Reduce output records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;SPILLED_RECORDS&quot;,&quot;displayName&quot;:&quot;Spilled Records&quot;,&quot;value&quot;:9},{&quot;name&quot;:&quot;SHUFFLED_MAPS&quot;,&quot;displayName&quot;:&quot;Shuffled Maps &quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;FAILED_SHUFFLE&quot;,&quot;displayName&quot;:&quot;Failed Shuffles&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;MERGED_MAP_OUTPUTS&quot;,&quot;displayName&quot;:&quot;Merged Map outputs&quot;,&quot;value&quot;:1},{&quot;name&quot;:&quot;GC_TIME_MILLIS&quot;,&quot;displayName&quot;:&quot;GC time elapsed (ms)&quot;,&quot;value&quot;:60},{&quot;name&quot;:&quot;CPU_MILLISECONDS&quot;,&quot;displayName&quot;:&quot;CPU time spent (ms)&quot;,&quot;value&quot;:820},{&quot;name&quot;:&quot;PHYSICAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Physical memory (bytes) snapshot&quot;,&quot;value&quot;:120524800},{&quot;name&quot;:&quot;VIRTUAL_MEMORY_BYTES&quot;,&quot;displayName&quot;:&quot;Virtual memory (bytes) snapshot&quot;,&quot;value&quot;:672628736},{&quot;name&quot;:&quot;COMMITTED_HEAP_BYTES&quot;,&quot;displayName&quot;:&quot;Total committed heap usage (bytes)&quot;,&quot;value&quot;:15728640}]},{&quot;name&quot;:&quot;Shuffle Errors&quot;,&quot;displayName&quot;:&quot;Shuffle Errors&quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BAD_ID&quot;,&quot;displayName&quot;:&quot;BAD_ID&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;CONNECTION&quot;,&quot;displayName&quot;:&quot;CONNECTION&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;IO_ERROR&quot;,&quot;displayName&quot;:&quot;IO_ERROR&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_LENGTH&quot;,&quot;displayName&quot;:&quot;WRONG_LENGTH&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_MAP&quot;,&quot;displayName&quot;:&quot;WRONG_MAP&quot;,&quot;value&quot;:0},{&quot;name&quot;:&quot;WRONG_REDUCE&quot;,&quot;displayName&quot;:&quot;WRONG_REDUCE&quot;,&quot;value&quot;:0}]},{&quot;name&quot;:&quot;org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter&quot;,&quot;displayName&quot;:&quot;File Output Format Counters &quot;,&quot;counts&quot;:[{&quot;name&quot;:&quot;BYTES_WRITTEN&quot;,&quot;displayName&quot;:&quot;Bytes Written&quot;,&quot;value&quot;:72}]}]}}}}
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#### 任务运行日志（Container日志）</div><div class="line"></div><div class="line">任务运行日志（即：Container日志）包含ApplicationMaster日志和普通Task日志等信息。默认情况下，这些日志信息是存放在$&#123;HADOOP_HOME&#125;/logs/userlogs目录下</div><div class="line"></div><div class="line">普通Task就是我们的MapReduce实例程序，这类日志就是我们在MapReduce程序中输出的log日志和System.out打印到控制台的输出，例如：logger.info(&quot;test&quot;);和System.out.println(&quot;test&quot;);。这部分日志默认是存储在$&#123;HADOOP_HOME&#125;/logs/userlogs</div><div class="line"></div><div class="line">##### yarn-site.xml</div></pre></td></tr></table></figure>

&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;
    &lt;value&gt;${yarn.log.dir}/userlogs&lt;/value&gt;
&lt;/property&gt;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">对于Container日志，在Hadoop 2.x版本里面，Task是按照application-&gt;Container的层次来管理的，所以在NameNode机器上运行MapReduce程序的时候，在Console里面看到的log都可以通过在相应的DataNode/NodeManager里面的$&#123;HADOOP_HOME&#125;/logs/userlogs下面找到。</div></pre></td></tr></table></figure>

# 查看默认日志存储路径下的文件（一个MapReduce程序对应一个Application，这里的Application会在Console提示出哪一个Application执行我们的MapReduce程序）
$ ls ${HADOOP_HOME}/logs/userlogs
drwx--x---  5 yunyu yunyu  4096 Nov  2 20:13 application_1478088725123_0001/
drwx--x---  5 yunyu yunyu  4096 Nov  2 20:31 application_1478088725123_0002/
drwx--x---  5 yunyu yunyu  4096 Nov  2 21:05 application_1478088725123_0003/
drwx--x---  3 yunyu yunyu  4096 Nov  2 21:08 application_1478088725123_0004/
drwx--x---  3 yunyu yunyu  4096 Nov  2 21:34 application_1478088725123_0006/
drwx--x---  4 yunyu yunyu  4096 Nov  2 23:49 application_1478101603149_0001/
drwx--x---  3 yunyu yunyu  4096 Nov  3 00:04 application_1478101603149_0002/
drwx--x---  3 yunyu yunyu  4096 Nov  3 11:03 application_1478138258749_0001/

# 查看某个Application下的Container日志（如果是Hadoop分布式集群，Container日志可能会分布在多个DataNode机器中）
$ ll application_1478088725123_0003/
total 20
drwx--x---  5 yunyu yunyu 4096 Nov  2 21:05 ./
drwxr-xr-x 33 yunyu yunyu 4096 Nov  3 14:57 ../
drwx--x---  2 yunyu yunyu 4096 Nov  2 21:04 container_1478088725123_0003_01_000001/
drwx--x---  2 yunyu yunyu 4096 Nov  2 21:05 container_1478088725123_0003_01_000002/
drwx--x---  2 yunyu yunyu 4096 Nov  2 21:05 container_1478088725123_0003_01_000003/

# 查看container下的日志文件，都有三个日志文件
# stderr : 错误文件输出
# stdout : System.out.println控制台输出，我们自己写的MapReduce程序的System.out.println输出都将写入到此文件中
# syslog : logger系统日志输出，我们自己的MapReduce程序的logger.info日志记录都将写入到此文件中
$ ll application_1478088725123_0003/container_1478088725123_0003_01_000001/*
-rw-rw-r-- 1 yunyu yunyu   760 Nov  2 21:05 application_1478088725123_0003/container_1478088725123_0003_01_000001/stderr
-rw-rw-r-- 1 yunyu yunyu     0 Nov  2 21:04 application_1478088725123_0003/container_1478088725123_0003_01_000001/stdout
-rw-rw-r-- 1 yunyu yunyu 34718 Nov  2 21:05 application_1478088725123_0003/container_1478088725123_0003_01_000001/syslog
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">如果是Hadoop分布式集群，Container日志文件有可能会被分配到多个机器中</div></pre></td></tr></table></figure>

# Hadoop1机器中运行了一部分的Application分配的任务，任务日志在container_1478101603149_0002_01_000001中
$ ll application_1478101603149_0002/
total 12
drwx--x---  3 yunyu yunyu 4096 Nov  3 00:04 ./
drwxr-xr-x 33 yunyu yunyu 4096 Nov  3 15:03 ../
drwx--x---  2 yunyu yunyu 4096 Nov  3 00:04 container_1478101603149_0002_01_000001/

# Hadoop2机器中运行了一部分的Application分配的任务，任务日志在container_1478101603149_0002_01_000002和container_1478101603149_0002_01_000003中
$ ll application_1478101603149_0002/
total 16
drwx--x---  4 yunyu yunyu 4096 Nov  3 00:04 ./
drwxr-xr-x 36 yunyu yunyu 4096 Nov  3 15:03 ../
drwx--x---  2 yunyu yunyu 4096 Nov  3 00:04 container_1478101603149_0002_01_000002/
drwx--x---  2 yunyu yunyu 4096 Nov  3 00:04 container_1478101603149_0002_01_000003/
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 日志聚合</div><div class="line"></div><div class="line">将作业和任务日志存放在各个节点上不便于统一管理和分析，为此，我们可以启用日志聚集功能。打开该功能后，各个任务运行完成后，会将生成的日志推送到HDFS的一个目录下（之前的并不会立即删除，在HDFS上，每个任务产生的三个文件，即syslog、stderr和stdout将合并一个文件，并通过索引记录各自位置）。这样我们可以使用HistoryServer统一查看Hadoop相关日志。</div><div class="line"></div><div class="line">这里需要在yarn-site.xml配置文件中添加如下配置</div></pre></td></tr></table></figure>

&lt;property&gt;
    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">如果不添加此配置我们在HistoryServer中直接查看logs会报错如下</div></pre></td></tr></table></figure>

Aggregation is not enabled. Try the nodemanager at hadoop1:39175
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#### 日志聚集相关配置参数</div><div class="line"></div><div class="line">日志聚集是YARN提供的日志中央化管理功能，它能将运行完成的Container/任务日志上传到HDFS上，从而减轻NodeManager负载，且提供一个中央化存储和分析机制。默认情况下，Container/任务日志存在在各个NodeManager上，如果启用日志聚集功能需要额外的配置。</div></pre></td></tr></table></figure>

（1） yarn.log-aggregation-enable
参数解释：是否启用日志聚集功能。
默认值：false

（2） yarn.log-aggregation.retain-seconds
参数解释：在HDFS上聚集的日志最多保存多长时间。
默认值：-1

（3） yarn.log-aggregation.retain-check-interval-seconds
参数解释：多长时间检查一次日志，并将满足条件的删除，如果是0或者负数，则为上一个值的1/10。
默认值：-1

（4） yarn.nodemanager.remote-app-log-dir
参数解释：当应用程序运行结束后，日志被转移到的HDFS目录（启用日志聚集功能时有效）。
默认值：/tmp/logs

（5） yarn.log-aggregation.retain-seconds
参数解释：远程日志目录子目录名称（启用日志聚集功能时有效）。
默认值：日志将被转移到目录
${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam}下
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">我这里犯了一个比较低级的错误，导致我的日志结果一直和我预期的不同，这也让我困惑了许久。问题的现象是我只在Hadoop1的机器中启用了日志聚集功能（yarn.log-aggregation-enable为true），但是我在跑自己的MapReduce任务时还是看不到自己代码中的System.out.println和logger.info日志输出到Container日志文件中，而且发现Container日志文件会少，正常情况下会有container_XXX_00001，container_XXX_00002，container_XXX_00003三个容器运行日志，但是有的时候我发现只有container_XXX_00001和container_XXX_00003没有container_XXX_00002。后来我多次重新跑了MapReduce程序，发现container日志会被正常生成出来，但是MapReduce运行完很快就被删掉了。这样我困惑了很久，后来发现原来是我自己粗心导致了，日志聚集功能只在Hadoop1机器中配置了，而其他两台Hadoop机器没有配置，所以导致只有Hadoop1的Container日志文件被聚集到HDFS上，而且Hadoop1的Container日志聚集到HDFS之后，会将Hadoop1系统中的log文件删除，所以就会少了container_XXX_00002的日志文件。</div></pre></td></tr></table></figure>

# 可以在HDFS中查看到stderr, stdout, syslog聚集的日志文件
$ hdfs dfs -ls /tmp/logs/yunyu/logs/application_1478159146498_0001
Found 1 items
-rw-r-----   2 yunyu supergroup      52122 2016-11-03 00:47 /tmp/logs/yunyu/logs/application_1478159146498_0001/hadoop1_35650
</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这里我简单说一下我研究的结论</p>
<ul>
<li>Q: 在Hadoop中如何看到MapReduce中的System.out.println输出</li>
<li><p>A: 在Container日志目录下的stdout日志文件中，默认是在${HADOOP_HOME}/logs/userlogs/application_XXXX/container_XXX_0000X/stdout</p>
</li>
<li><p>Q: 在Hadoop中如何看到Log4j或者其他日志组件的日志输出</p>
</li>
<li><p>A: 在Container日志目录下的stdout日志文件中，默认是在${HADOOP_HOME}/logs/userlogs/application_XXXX/container_XXX_0000X/syslog</p>
</li>
<li><p>Q: 日志文件保存在哪里</p>
</li>
<li><p>A: 默认保存在${HADOOP_HOME}/logs/userlogs/目录下</p>
</li>
<li><p>Q: 如何通过HistoryServer查看Hadoop日志</p>
</li>
<li>A: 需要在yarn-site.xml配置文件中配置yarn.log-aggregation-enable属性为true</li>
</ul>
<p>参考文章：</p>
<ul>
<li><a href="https://www.iteblog.com/archives/896" target="_blank" rel="external">https://www.iteblog.com/archives/896</a></li>
<li><a href="https://www.iteblog.com/archives/936" target="_blank" rel="external">https://www.iteblog.com/archives/936</a></li>
<li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-2-0-jobhistory-log/" target="_blank" rel="external">http://dongxicheng.org/mapreduce-nextgen/hadoop-2-0-jobhistory-log/</a></li>
<li><a href="http://blog.csdn.net/infovisthinker/article/details/45370089" target="_blank" rel="external">http://blog.csdn.net/infovisthinker/article/details/45370089</a></li>
<li><a href="http://blog.caiyongfu.cn/?p=614" target="_blank" rel="external">http://blog.caiyongfu.cn/?p=614</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop原理架构体系/">Hadoop原理架构体系</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/">MapReduce</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Docker/Docker实战（二十）Docker镜像的导入导出" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/04/Docker/Docker实战（二十）Docker镜像的导入导出/" class="article-date">
  	<time datetime="2016-09-04T03:30:19.000Z" itemprop="datePublished">2016-09-04</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/04/Docker/Docker实战（二十）Docker镜像的导入导出/">Docker实战（二十）Docker镜像的导入导出</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Docker镜像导入导出方式"><a href="#Docker镜像导入导出方式" class="headerlink" title="Docker镜像导入导出方式"></a>Docker镜像导入导出方式</h2><p>最近公司需要做Docker私有化部署，需要将本地安装好的Docker容器部署到客户的环境，这里遇到了一些问题客户的服务器不能连接外网，无法在线做Docker镜像的构建，所以需要只能通过导入导出镜像的方式来做。下面是我总结的Docker镜像导入导出方式。</p>
<p>Docker提供了两种方式的导入导出：</p>
<ul>
<li>load/save方式导入导出镜像<ul>
<li>docker save：来导出本地镜像库中指定的镜像存储成文件</li>
<li>docker load：来导入镜像存储文件到本地镜像库</li>
</ul>
</li>
<li>import/export方式导入导出容器<ul>
<li>docker export：来导出一个容器快照到本地文件</li>
<li>docker import：来导入一个容器快照文件到本地镜像库</li>
</ul>
</li>
<li>区别：容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。我个人比较推荐用load/save方式，这样所有之前的镜像都会存在，只是比较占用空间。</li>
</ul>
<h3 id="Docker镜像save-load方式"><a href="#Docker镜像save-load方式" class="headerlink" title="Docker镜像save/load方式"></a>Docker镜像save/load方式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">$ sudo docker imagesREPOSITORY                  TAG                 IMAGE ID            CREATED             VIRTUAL SIZEbirdben/zookeeper           v1                  20e4011b9286        2 minutes ago       1.658 GBubuntu                      latest              37b164bb431e        7 days ago          126.6 MBcentos                      7                   d83a55af4e75        5 weeks ago         196.7 MBcentos                      latest              d83a55af4e75        5 weeks ago         196.7 MBbirdben/jdk7                v1                  25c2f0e69206        8 months ago        583.4 MB</div><div class="line"></div><div class="line"># 导出birdben/zookeeper:v1镜像到zookeeper_image.tar文件</div><div class="line">$ sudo docker save birdben/zookeeper:v1 &gt; zookeeper_image.tar</div><div class="line"></div><div class="line"># 删除之前的birdben/zookeeper:v1镜像</div><div class="line">$ sudo docker rmi &quot;birdben/zookeeper:v1&quot;</div><div class="line"></div><div class="line"># 导入zookeeper_image.tar镜像文件</div><div class="line">$ sudo docker load &lt; zookeeper_image.tar</div><div class="line"></div><div class="line"># 再次查看所有的镜像，可以看到birdben/zookeeper:v1又回来了</div><div class="line"># 注意：这里import回来的ImageID也和原来是一样的</div><div class="line">$ sudo docker imagesREPOSITORY                  TAG                 IMAGE ID            CREATED             VIRTUAL SIZEbirdben/zookeeper           v1                  20e4011b9286        6 minutes ago       1.658 GBubuntu                      latest              37b164bb431e        7 days ago          126.6 MBcentos                      7                   d83a55af4e75        5 weeks ago         196.7 MBcentos                      latest              d83a55af4e75        5 weeks ago         196.7 MBbirdben/jdk7                v1                  25c2f0e69206        8 months ago        583.4 MB</div><div class="line"></div><div class="line"># 这时候在查看birdben/zookeeper:v1镜像的tree结构，发现之前所有的镜像历史都在</div><div class="line">$ sudo docker images --tree</div><div class="line">├─3690474eb5b4 Virtual Size: 0 B│ └─b200b2d33d98 Virtual Size: 196.7 MB│   └─3fbd5972aaac Virtual Size: 196.7 MB│     └─d83a55af4e75 Virtual Size: 196.7 MB Tags: centos:7, centos:latest│       └─1df8e9ff4de7 Virtual Size: 196.7 MB│         └─b37af9ce019a Virtual Size: 196.7 MB│           └─7858b8d134c6 Virtual Size: 403.3 MB│             └─c872974343d2 Virtual Size: 403.3 MB│               └─d4c0e59dc712 Virtual Size: 403.3 MB│                 └─30c3076be68f Virtual Size: 556.8 MB│                   └─0e66c066e1de Virtual Size: 571 MB│                     └─69f8ec0b7932 Virtual Size: 889.1 MB│                       └─7cfcd6d4c6e7 Virtual Size: 911.4 MB│                         └─c2bc26e11781 Virtual Size: 911.4 MB│                           └─31d728531f9a Virtual Size: 911.4 MB│                             └─6434457046ec Virtual Size: 911.4 MB│                               └─651290e3ddef Virtual Size: 911.4 MB│                                 └─d99d028fca92 Virtual Size: 911.6 MB│                                   └─5d4d89731a7d Virtual Size: 911.6 MB│                                     └─a530df3b220c Virtual Size: 925.6 MB│                                       └─39381e27bf53 Virtual Size: 1.232 GB│                                         └─cda80cbe8e7f Virtual Size: 1.276 GB│                                           └─287a8cf1090c Virtual Size: 1.289 GB│                                             └─d5672fcec9a4 Virtual Size: 1.289 GB│                                               └─e63cb61422e1 Virtual Size: 1.289 GB│                                                 └─aa8f6ecc78ca Virtual Size: 1.303 GB│                                                   └─b44f1969877f Virtual Size: 1.303 GB│                                                     └─d17184db904f Virtual Size: 1.303 GB│                                                       └─7df628e7fd36 Virtual Size: 1.303 GB│                                                         └─dfe01b409095 Virtual Size: 1.303 GB│                                                           └─238718f45aa6 Virtual Size: 1.303 GB│                                                             └─a678149d4c34 Virtual Size: 1.303 GB│                                                               └─d3f7fb8e3bc2 Virtual Size: 1.658 GB│                                                                 └─ff152402a43c Virtual Size: 1.658 GB│                                                                   └─fdf82aa49b89 Virtual Size: 1.658 GB│                                                                     └─0dbfccd66315 Virtual Size: 1.658 GB│                                                                       └─4cae49ef2ecb Virtual Size: 1.658 GB Tags: birdben/zookeeper:v1</div></pre></td></tr></table></figure>
<h3 id="Docker镜像import-export方式"><a href="#Docker镜像import-export方式" class="headerlink" title="Docker镜像import/export方式"></a>Docker镜像import/export方式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">$ sudo docker images</div><div class="line">REPOSITORY                  TAG                 IMAGE ID            CREATED             VIRTUAL SIZEbirdben/zookeeper           v1       	        20e4011b9286        11 seconds ago      1.658 GBubuntu                      latest              37b164bb431e        7 days ago          126.6 MBcentos                      7                   d83a55af4e75        5 weeks ago         196.7 MBcentos                      latest              d83a55af4e75        5 weeks ago         196.7 MBbirdben/jdk7                v1                  25c2f0e69206        8 months ago        583.4 MB</div><div class="line"></div><div class="line">$ sudo docker ps -aCONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS                                            NAMESf99771de17b0        20e4011b9286:latest   &quot;/bin/bash&quot;         6 seconds ago       Up 5 seconds        0.0.0.0:3306-&gt;3306/tcp, 0.0.0.0:8080-&gt;8080/tcp   birdben/zookeeper:v1</div><div class="line"></div><div class="line"># 导出容器ID为f99771de17b0的Docker容器</div><div class="line">$ sudo docker export f99771de17b0 &gt; container.tar.gz</div><div class="line"></div><div class="line"># 删除之前的镜像birdben/zookeeper:v1</div><div class="line">$ sudo docker rmi &quot;birdben/zookeeper:v1&quot;</div><div class="line"></div><div class="line"># 导入容器文件container.tar.gz</div><div class="line">$ cat container.tar.gz | sudo docker import - birdben/zookeeper:v1</div><div class="line"></div><div class="line"># 注意：这里import回来的ImageID也和原来不一样了</div><div class="line">$ sudo docker imagesREPOSITORY                  TAG                 IMAGE ID            CREATED             VIRTUAL SIZEbirdben/zookeeper           v1                  e80c1046dc12        9 minutes ago       1.119 GBubuntu                      latest              37b164bb431e        7 days ago          126.6 MBcentos                      7                   d83a55af4e75        5 weeks ago         196.7 MBcentos                      latest              d83a55af4e75        5 weeks ago         196.7 MBbirdben/jdk7                v1                  25c2f0e69206        8 months ago        583.4 MB</div><div class="line"></div><div class="line"># 这时候在查看birdben/zookeeper:v1镜像的tree结构，发现只有最有的镜像，没有以前的历史镜像</div><div class="line">$ sudo docker images --treeWarning: &apos;--tree&apos; is deprecated, it will be removed soon. See usage.├─e80c1046dc12 Virtual Size: 1.119 GB Tags: birdben/zookeeper:v1├─f1b49dd0c243 Virtual Size: 126.6 MB│ └─008ecf8686ec Virtual Size: 126.6 MB│   └─fd74137ff5ae Virtual Size: 126.6 MB│     └─35371c8124e2 Virtual Size: 126.6 MB│       └─99dc4d8f603d Virtual Size: 126.6 MB│         └─37b164bb431e Virtual Size: 126.6 MB Tags: ubuntu:latest</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="https://segmentfault.com/a/1190000000586840" target="_blank" rel="external">https://segmentfault.com/a/1190000000586840</a></li>
<li><a href="http://www.sxt.cn/u/756/blog/5339" target="_blank" rel="external">http://www.sxt.cn/u/756/blog/5339</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dockerfile/">Dockerfile</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker命令/">Docker命令</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Docker/">Docker</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Flume/Flume学习（八）Flume解析自定义日志" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/03/Flume/Flume学习（八）Flume解析自定义日志/" class="article-date">
  	<time datetime="2016-09-03T08:11:01.000Z" itemprop="datePublished">2016-09-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/03/Flume/Flume学习（八）Flume解析自定义日志/">Flume学习（八）Flume解析自定义日志</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="环境简介"><a href="#环境简介" class="headerlink" title="环境简介"></a>环境简介</h3><ul>
<li>JDK1.7.0_79</li>
<li>Flume1.6.0</li>
<li>Elasticsearch2.0.0</li>
</ul>
<p>这里是基于上一篇《Flume学习（七）Flume整合Elasticsearch2.x》解析自定义的日志格式</p>
<h3 id="解析的日志格式"><a href="#解析的日志格式" class="headerlink" title="解析的日志格式"></a>解析的日志格式</h3><p>这里由于篇幅原因，我简单列举了两条典型的日志格式</p>
<h4 id="日志文件格式"><a href="#日志文件格式" class="headerlink" title="日志文件格式"></a>日志文件格式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[&#123;&quot;name&quot;:&quot;rp.api.call&quot;,&quot;request&quot;:&quot;GET /api/test/settings&quot;,&quot;status&quot;:&quot;succeeded&quot;,&quot;uid&quot;:889,&quot;did&quot;:13,&quot;duid&quot;:&quot;app001&quot;,&quot;ua&quot;:&quot;Dalvik/2.1.0 (Linux; U; Android 6.0.1; MI NOTE LTE MIUI/6.5.12)&quot;,&quot;device_id&quot;:&quot;65768768252343&quot;,&quot;ip&quot;:&quot;10.190.1.67&quot;,&quot;server_timestamp&quot;:1463713488740&#125;]</div><div class="line">[&#123;&quot;name&quot;:&quot;rp.api.call&quot;,&quot;request&quot;:&quot;GET /api/test/search&quot;,&quot;errorStatus&quot;:200,&quot;errorCode&quot;:&quot;0000&quot;,&quot;errorMsg&quot;:&quot;操作成功&quot;,&quot;status&quot;:&quot;failed&quot;,&quot;uid&quot;:889,&quot;did&quot;:13,&quot;duid&quot;:&quot;app002&quot;,&quot;ua&quot;:&quot;Dalvik/2.1.0 (Linux; U; Android 6.0.1; MI NOTE LTE MIUI/6.5.12)&quot;,&quot;device_id&quot;:&quot;4543657687878989&quot;,&quot;ip&quot;:&quot;10.190.1.66&quot;,&quot;server_timestamp&quot;:1463650301701&#125;]</div></pre></td></tr></table></figure>
<p>上一篇已经讲过了Flume解析日志格式主要使用interceptors，interceptors本身又支持多种type，这里我们主要介绍regex_extractor，即正则表达式匹配方式。下面的正则表达式可以匹配上面的两种格式的日志，上面两种日志格式的主要区别就是errorStatus，errorCode，errorMsg这三个字段有可能不存在，当没有报错的时候，这三个字段是不需要的。</p>
<h4 id="日志解析正则表达式"><a href="#日志解析正则表达式" class="headerlink" title="日志解析正则表达式"></a>日志解析正则表达式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&quot;name&quot;:(.*),&quot;request&quot;:(.*),(&quot;errorStatus&quot;:(.*),)?(&quot;errorCode&quot;:(.*),)?(&quot;errorMsg&quot;:(.*),)?&quot;status&quot;:(.*),&quot;uid&quot;:(.*),&quot;did&quot;:(.*),&quot;duid&quot;:(.*),&quot;ua&quot;:(.*),&quot;device_id&quot;:(.*),&quot;ip&quot;:(.*),&quot;server_timestamp&quot;:([0-9]*)</div></pre></td></tr></table></figure>
<p>但是在Flume的interceptors的regex表达式中配置上面的正则表达式会报错，我自己分析的原因是Flume的interceptor.serializers需要指定正则表达式拆分后的对应的字段和值，没有办法做到根据正则表达式动态处理。（这里我的分析可能不一定对，如果有疑问我们可以私下交流）</p>
<p>下面是我的解决办法，我根据上面两种日志格式写了两个interceptor，分别是es_interceptor和es_error_interceptor，每个interceptor对应不同的正则表达式，分别用来处理上面两种不同的日志格式。这样interceptor.serializers就能根据对应的正则表达式格式解析出来日志中对应的字段和值，再插入到ES索引中。</p>
<h4 id="flume-conf配置文件"><a href="#flume-conf配置文件" class="headerlink" title="flume.conf配置文件"></a>flume.conf配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"># 原始的正则表达式：&quot;name&quot;:(.*),&quot;request&quot;:(.*),(&quot;errorStatus&quot;:(.*),)?(&quot;errorCode&quot;:(.*),)?(&quot;errorMsg&quot;:(.*),)?&quot;status&quot;:(.*),&quot;uid&quot;:(.*),&quot;did&quot;:(.*),&quot;duid&quot;:(.*),&quot;ua&quot;:(.*),&quot;device_id&quot;:(.*),&quot;ip&quot;:(.*),&quot;server_timestamp&quot;:([0-9]*)</div><div class="line">agentX.sources.flume-avro-sink.interceptors = es_interceptor es_error_interceptor</div><div class="line"></div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.type = regex_extractor</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.regex = &quot;name&quot;:(.*),&quot;request&quot;:(.*),&quot;status&quot;:(.*),&quot;uid&quot;:(.*),&quot;did&quot;:(.*),&quot;duid&quot;:(.*),&quot;ua&quot;:(.*),&quot;device_id&quot;:(.*),&quot;ip&quot;:(.*),&quot;server_timestamp&quot;:([0-9]*)</div><div class="line"></div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers = s1 s2 s3 s4 s5 s6 s7 s8 s9 s10</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers.s1.name = name</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers.s2.name = request</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers.s3.name = status</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers.s4.name = uid</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers.s5.name = did</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers.s6.name = duid</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers.s7.name = ua</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers.s8.name = device_id</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers.s9.name = ip</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_interceptor.serializers.s10.name = server_timestamp</div><div class="line"></div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.type = regex_extractor</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.regex = &quot;name&quot;:(.*),&quot;request&quot;:(.*),&quot;errorStatus&quot;:(.*),&quot;errorCode&quot;:(.*),&quot;errorMsg&quot;:(.*),&quot;status&quot;:(.*),&quot;uid&quot;:(.*),&quot;did&quot;:(.*),&quot;duid&quot;:(.*),&quot;ua&quot;:(.*),&quot;device_id&quot;:(.*),&quot;ip&quot;:(.*),&quot;server_timestamp&quot;:([0-9]*)</div><div class="line"></div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers = s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s1.name = name</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s2.name = request</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s3.name = errorStatus</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s4.name = errorCode</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s5.name = errorMsg</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s6.name = status</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s7.name = uid</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s8.name = did</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s9.name = duid</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s10.name = ua</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s11.name = device_id</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s12.name = ip</div><div class="line">agentX.sources.flume-avro-sink.interceptors.es_error_interceptor.serializers.s13.name = server_timestamp</div></pre></td></tr></table></figure>
<h4 id="ES的mapping如下"><a href="#ES的mapping如下" class="headerlink" title="ES的mapping如下"></a>ES的mapping如下</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;mappings&quot;: &#123;</div><div class="line">    &quot;hb&quot;: &#123;</div><div class="line">      &quot;properties&quot;: &#123;</div><div class="line">        &quot;@fields&quot;: &#123;</div><div class="line">          &quot;properties&quot;: &#123;</div><div class="line">            &quot;uid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;duid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;status&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;request&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;name&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;errorCode&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;ua&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;did&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;errorMsg&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;device_id&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;server_timestamp&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;ip&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;errorStatus&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;,</div><div class="line">        &quot;@message&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="ES索引中的日志信息"><a href="#ES索引中的日志信息" class="headerlink" title="ES索引中的日志信息"></a>ES索引中的日志信息</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">	&quot;_index&quot;: &quot;test_log_index-2016-09-03&quot;,</div><div class="line">	&quot;_type&quot;: &quot;test&quot;,</div><div class="line">	&quot;_id&quot;: &quot;AVbvrWIPe8IcP1cQoXS2&quot;,</div><div class="line">	&quot;_version&quot;: 1,</div><div class="line">	&quot;_score&quot;: 1,</div><div class="line">	&quot;_source&quot;: &#123;</div><div class="line">		&quot;@message&quot;: &quot;[</div><div class="line">			&#123;&quot;name&quot;:&quot;1&quot;,&quot;request&quot;:&quot;GET /api/test/settings&quot;,&quot;status&quot;:&quot;succeeded&quot;,&quot;uid&quot;:889,&quot;did&quot;:13,&quot;duid&quot;:&quot;app001&quot;,&quot;ua&quot;:&quot;Dalvik/2.1.0 (Linux; U; Android 6.0.1; MI NOTE LTE MIUI/6.5.12)&quot;,&quot;device_id&quot;:,&quot;ip&quot;:&quot;10.190.1.67&quot;,&quot;server_timestamp&quot;:1463713488740&#125;</div><div class="line">		]&quot;,</div><div class="line">		&quot;@fields&quot;: &#123;</div><div class="line">			&quot;uid&quot;: &quot;889&quot;,</div><div class="line">			&quot;duid&quot;: &quot;&quot;app001&quot;&quot;,</div><div class="line">			&quot;status&quot;: &quot;&quot;succeeded&quot;&quot;,</div><div class="line">			&quot;name&quot;: &quot;&quot;1&quot;&quot;,</div><div class="line">			&quot;request&quot;: &quot;&quot;GET /api/test/settings&quot;&quot;,</div><div class="line">			&quot;did&quot;: &quot;13&quot;,</div><div class="line">			&quot;ua&quot;: &quot;&quot;Dalvik/2.1.0 (Linux; U; Android 6.0.1; MI NOTE LTE MIUI/6.5.12)&quot;&quot;,</div><div class="line">			&quot;device_id&quot;: &quot;&quot;,</div><div class="line">			&quot;server_timestamp&quot;: &quot;1463713488740&quot;,</div><div class="line">			&quot;ip&quot;: &quot;&quot;10.190.1.67&quot;&quot;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">&#123;</div><div class="line">	&quot;_index&quot;: &quot;test_log_index-2016-09-03&quot;,</div><div class="line">	&quot;_type&quot;: &quot;test&quot;,</div><div class="line">	&quot;_id&quot;: &quot;AVbvrWIPe8IcP1cQoXS3&quot;,</div><div class="line">	&quot;_version&quot;: 1,</div><div class="line">	&quot;_score&quot;: 1,</div><div class="line">	&quot;_source&quot;: &#123;</div><div class="line">		&quot;@message&quot;: &quot;[</div><div class="line">			&#123;&quot;name&quot;:&quot;rp.api.call&quot;,&quot;request&quot;:&quot;GET /api/test/search&quot;,&quot;errorStatus&quot;:200,&quot;errorCode&quot;:&quot;0000&quot;,&quot;errorMsg&quot;:&quot;操作成功&quot;,&quot;status&quot;:&quot;failed&quot;,&quot;uid&quot;:889,&quot;did&quot;:13,&quot;duid&quot;:&quot;app001&quot;,&quot;ua&quot;:&quot;Dalvik/2.1.0 (Linux; U; Android 6.0.1; MI NOTE LTE MIUI/6.5.12)&quot;,&quot;device_id&quot;:&quot;4543657687878989&quot;,&quot;ip&quot;:&quot;10.190.1.66&quot;,&quot;server_timestamp&quot;:1463650301701&#125;</div><div class="line">		]&quot;,</div><div class="line">		&quot;@fields&quot;: &#123;</div><div class="line">			&quot;uid&quot;: &quot;889&quot;,</div><div class="line">			&quot;status&quot;: &quot;&quot;failed&quot;&quot;,</div><div class="line">			&quot;did&quot;: &quot;13&quot;,</div><div class="line">			&quot;device_id&quot;: &quot;&quot;4543657687878989&quot;&quot;,</div><div class="line">			&quot;errorMsg&quot;: &quot;&quot;操作成功&quot;&quot;,</div><div class="line">			&quot;errorStatus&quot;: &quot;200&quot;,</div><div class="line">			&quot;ip&quot;: &quot;&quot;10.190.1.66&quot;&quot;,</div><div class="line">			&quot;duid&quot;: &quot;&quot;app001&quot;&quot;,</div><div class="line">			&quot;request&quot;: &quot;&quot;GET /api/test/search&quot;&quot;,</div><div class="line">			&quot;name&quot;: &quot;&quot;rp.api.call&quot;&quot;,</div><div class="line">			&quot;errorCode&quot;: &quot;&quot;0000&quot;&quot;,</div><div class="line">			&quot;ua&quot;: &quot;&quot;Dalvik/2.1.0 (Linux; U; Android 6.0.1; MI NOTE LTE MIUI/6.5.12)&quot;&quot;,</div><div class="line">			&quot;server_timestamp&quot;: &quot;1463650301701&quot;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>个人觉得这样的做法并不理想，因为日志格式肯定会多种多样，如果每种日志格式都需要不同的正则表达式来处理，显得太过笨重和冗余，因为刚接触Flume暂时没有发现有更好的做法，日后发现有更好的处理方式会重新更新上来。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Docker/Docker实战（十九）Docker环境安装问题" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/03/Docker/Docker实战（十九）Docker环境安装问题/" class="article-date">
  	<time datetime="2016-09-03T02:50:49.000Z" itemprop="datePublished">2016-09-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/03/Docker/Docker实战（十九）Docker环境安装问题/">Docker实战（十九）Docker环境安装问题</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h2><h3 id="本地环境"><a href="#本地环境" class="headerlink" title="本地环境"></a>本地环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Ubuntu14.04</div><div class="line"></div><div class="line">Client version: 1.6.2Client API version: 1.18Go version (client): go1.2.1Git commit (client): 7c8fca2OS/Arch (client): linux/amd64</div><div class="line">Server version: 1.6.2Server API version: 1.18Go version (server): go1.2.1Git commit (server): 7c8fca2OS/Arch (server): linux/amd64</div></pre></td></tr></table></figure>
<h3 id="10-10-1-15测试环境"><a href="#10-10-1-15测试环境" class="headerlink" title="10.10.1.15测试环境"></a>10.10.1.15测试环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">Ubuntu15.04</div><div class="line"></div><div class="line">Client:</div><div class="line"> Version:      1.10.3</div><div class="line"> API version:  1.21</div><div class="line"> Go version:   go1.5.3</div><div class="line"> Git commit:   20f81dd</div><div class="line"> Built:        Thu Mar 10 21:49:11 2016</div><div class="line"> OS/Arch:      linux/amd64</div><div class="line"></div><div class="line">Server:</div><div class="line"> Version:      1.9.1</div><div class="line"> API version:  1.21</div><div class="line"> Go version:   go1.4.2</div><div class="line"> Git commit:   a34a1d5</div><div class="line"> Built:        Fri Nov 20 13:16:54 UTC 2015</div><div class="line"> OS/Arch:      linux/amd64</div></pre></td></tr></table></figure>
<h2 id="Docker的安装和使用"><a href="#Docker的安装和使用" class="headerlink" title="Docker的安装和使用"></a>Docker的安装和使用</h2><h3 id="本地环境安装"><a href="#本地环境安装" class="headerlink" title="本地环境安装"></a>本地环境安装</h3><p>直接使用apt方式安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ apt-get update</div><div class="line">$ apt-get install docker</div><div class="line">$ apt-get install docker.io</div></pre></td></tr></table></figure>
<h3 id="10-10-1-15测试环境-1"><a href="#10-10-1-15测试环境-1" class="headerlink" title="10.10.1.15测试环境"></a>10.10.1.15测试环境</h3><p>使用apt方式安装报错E: Sub-process /usr/bin/dpkg returned an error code (1)，之后尝试了一些解决方式，但都没有解决成功，最后决定将docker卸载掉重新按照官网的步骤安装成功</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"># 安装前先查看Linux内核版本，内核版本需要高于3.10</div><div class="line">$ uname -r</div><div class="line">3.19.0-68-generic</div><div class="line"></div><div class="line"># Update your apt sources</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install apt-transport-https ca-certificates</div><div class="line">$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D</div><div class="line"></div><div class="line"># 创建并保存docker.list更新源文件</div><div class="line">$ /etc/apt/sources.list.d/docker.list</div><div class="line"></div><div class="line"># 根据自己的系统版本选择不同的数据源</div><div class="line">- On Ubuntu Precise 12.04 (LTS)</div><div class="line">deb https://apt.dockerproject.org/repo ubuntu-precise main</div><div class="line">- On Ubuntu Trusty 14.04 (LTS)</div><div class="line">deb https://apt.dockerproject.org/repo ubuntu-trusty main</div><div class="line">- Ubuntu Wily 15.10</div><div class="line">deb https://apt.dockerproject.org/repo ubuntu-wily main</div><div class="line">- Ubuntu Xenial 16.04 (LTS)</div><div class="line">deb https://apt.dockerproject.org/repo ubuntu-xenial main</div><div class="line"></div><div class="line"># 再次更新源</div><div class="line">$ sudo apt-get update</div><div class="line"># 删除旧的资源文件</div><div class="line">$ sudo apt-get purge lxc-docker</div><div class="line"># 验证apt的更新源是从正确的仓库获取</div><div class="line">$ apt-cache policy docker-engine</div><div class="line"></div><div class="line"># 安装Ubuntu必备的安装包linux-image-extra-*</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual</div><div class="line"></div><div class="line"># 安装Docker</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install docker-engine</div><div class="line"># 启动Docker服务</div><div class="line">$ sudo service docker start</div><div class="line"># 检查Docker版本</div><div class="line">$ sudo docker version</div></pre></td></tr></table></figure>
<h3 id="遇到的问题和解决方法"><a href="#遇到的问题和解决方法" class="headerlink" title="遇到的问题和解决方法"></a>遇到的问题和解决方法</h3><h4 id="Depends-libdevmapper1-02-1-gt-2-1-02-99-but-2-1-02-90-2ubuntu1-is-to-be-installed"><a href="#Depends-libdevmapper1-02-1-gt-2-1-02-99-but-2-1-02-90-2ubuntu1-is-to-be-installed" class="headerlink" title="Depends: libdevmapper1.02.1 (&gt;= 2:1.02.99) but 2:1.02.90-2ubuntu1 is to be installed"></a>Depends: libdevmapper1.02.1 (&gt;= 2:1.02.99) but 2:1.02.90-2ubuntu1 is to be installed</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">$ sudo apt install docker-engine</div><div class="line">Reading package lists... Done</div><div class="line">Building dependency tree</div><div class="line">Reading state information... Done</div><div class="line">Some packages could not be installed. This may mean that you have</div><div class="line">requested an impossible situation or if you are using the unstable</div><div class="line">distribution that some required packages have not yet been created</div><div class="line">or been moved out of Incoming.</div><div class="line">The following information may help to resolve the situation:</div><div class="line"></div><div class="line">The following packages have unmet dependencies:</div><div class="line"> docker-engine : Depends: libdevmapper1.02.1 (&gt;= 2:1.02.99) but 2:1.02.90-2ubuntu1 is to be installed</div><div class="line">E: Unable to correct problems, you have held broken packages.</div><div class="line"></div><div class="line"># 遇到这个问题是因为更新源不正确的原因，因为我们用的Ubuntu15.04版本，所以上面官网提供的数据源中并不包含我们的版本</div><div class="line">$ sudo lsb_release -a</div><div class="line">No LSB modules are available.</div><div class="line">Distributor ID:	Ubuntu</div><div class="line">Description:	Ubuntu 15.04</div><div class="line">Release:	15.04</div><div class="line">Codename:	vivid</div><div class="line"></div><div class="line"># 这里在github上找到了解决方法，依次执行下面的命令可以修复正确的数据源</div><div class="line">$ sudo sed -i &apos;/wily/d&apos; /etc/apt/sources.list.d/docker.list</div><div class="line">$ sudo sed -i &apos;/trusty/d&apos; /etc/apt/sources.list.d/docker.list</div><div class="line">$ sudo sed -i &apos;/precise/d&apos; /etc/apt/sources.list.d/docker.list</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install docker-engine</div></pre></td></tr></table></figure>
<h4 id="Error-response-from-daemon-client-is-newer-than-server-client-API-version-1-22-server-API-version-1-21"><a href="#Error-response-from-daemon-client-is-newer-than-server-client-API-version-1-22-server-API-version-1-21" class="headerlink" title="Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.21)"></a>Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.21)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ docker version</div><div class="line">Client:</div><div class="line"> Version:      1.10.3</div><div class="line"> API version:  1.22</div><div class="line"> Go version:   go1.5.3</div><div class="line"> Git commit:   20f81dd</div><div class="line"> Built:        Thu Mar 10 21:49:11 2016</div><div class="line"> OS/Arch:      linux/amd64</div><div class="line">Error response from daemon: client is newer than server (client API version: 1.22, server API version: 1.21)</div><div class="line"></div><div class="line"># 遇到这个问题的原因是Docker API version的版本号不一致导致的，这个我们需要添加一个环境变量来指定Docker API version的版本号</div><div class="line"></div><div class="line"># 这里建议更改/etc/profile文件，而不是临时更改环境变量，修改/etc/profile之后需要source /etc/profile，如果要在sudo也生效，需要切换到root账号也source /etc/profile</div><div class="line">$ export DOCKER_API_VERSION=1.21</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="http://docs.docker.com.s3-website-us-east-1.amazonaws.com/engine/installation/linux/ubuntulinux/" target="_blank" rel="external">http://docs.docker.com.s3-website-us-east-1.amazonaws.com/engine/installation/linux/ubuntulinux/</a></li>
<li><a href="http://askubuntu.com/questions/686698/docker-installation-error-libdevmapper1-02-1-21-02-99" target="_blank" rel="external">http://askubuntu.com/questions/686698/docker-installation-error-libdevmapper1-02-1-21-02-99</a></li>
<li><a href="https://github.com/docker/machine/issues/2147">https://github.com/docker/machine/issues/2147</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dockerfile/">Dockerfile</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker命令/">Docker命令</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Docker/">Docker</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Zookeeper/Zookeeper学习（二）Zookeeper集群环境搭建" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/02/Zookeeper/Zookeeper学习（二）Zookeeper集群环境搭建/" class="article-date">
  	<time datetime="2016-09-02T15:54:31.000Z" itemprop="datePublished">2016-09-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/02/Zookeeper/Zookeeper学习（二）Zookeeper集群环境搭建/">Zookeeper学习（二）Zookeeper集群环境搭建</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="ZooKeeper-Cluster模式"><a href="#ZooKeeper-Cluster模式" class="headerlink" title="ZooKeeper Cluster模式"></a>ZooKeeper Cluster模式</h3><h4 id="etc-hosts文件配置"><a href="#etc-hosts文件配置" class="headerlink" title="/etc/hosts文件配置"></a>/etc/hosts文件配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">172.17.0.51     zoo1</div><div class="line">172.17.0.52     zoo2</div><div class="line">172.17.0.53     zoo3</div></pre></td></tr></table></figure>
<h4 id="var-zookeeper-myid文件配置"><a href="#var-zookeeper-myid文件配置" class="headerlink" title="/var/zookeeper/myid文件配置"></a>/var/zookeeper/myid文件配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># zoo1的myid配置文件</div><div class="line">1</div><div class="line"></div><div class="line"># zoo2的myid配置文件</div><div class="line">2</div><div class="line"></div><div class="line"># zoo3的myid配置文件</div><div class="line">3</div></pre></td></tr></table></figure>
<h4 id="Zookeeper配置文件"><a href="#Zookeeper配置文件" class="headerlink" title="Zookeeper配置文件"></a>Zookeeper配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># the basic time unit in milliseconds used by ZooKeeper. It is used to do heartbeats and the minimum session timeout will be twice the tickTime.</div><div class="line"># tickTime 这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳。</div><div class="line">tickTime=2000</div><div class="line"></div><div class="line"># the location to store the in-memory database snapshots and, unless specified otherwise, the transaction log of updates to the database.</div><div class="line"># dataDir 顾名思义就是Zookeeper保存数据的目录,默认情况下Zookeeper将写数据的日志文件也保存在这个目录里。</div><div class="line">dataDir=/var/lib/zookeeper</div><div class="line"></div><div class="line"># the port to listen for client connections</div><div class="line"># clientPort 这个端口就是客户端（应用程序）连接Zookeeper服务器的端口，Zookeeper会监听这个端口接受客户端的访问请求。</div><div class="line">clientPort=2181</div><div class="line"></div><div class="line"># initLimit 这个配置项是用来配置Zookeeper接受客户端（这里所说的客户端不是用户连接Zookeeper 服务器的客户端，而是Zookeeper服务器集群中连接到Leader的Follower服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过10个心跳的时间（也就是tickTime）长度后Zookeeper服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 10*2000=20 秒。</div><div class="line">initLimit=10</div><div class="line"></div><div class="line"># syncLimit 这个配置项标识Leader与Follower之间发送消息，请求和应答时间长度，最长不能超过多少个tickTime的时间长度，总的时间长度就是 5*2000=10 秒。</div><div class="line">syncLimit=5</div><div class="line"></div><div class="line"># 第一个port是从机器（follower）连接到主机器（leader）的端口号，第二个port是进行leadership选举的端口号。</div><div class="line"># 值得重点注意的一点是，所有三个机器都应该打开端口 2181、2888 和 3888。在本例中，端口 2181 由 ZooKeeper 客户端使用，用于连接到 ZooKeeper 服务器；端口 2888 由对等 ZooKeeper 服务器使用，用于互相通信；而端口 3888 用于领导者选举。您可以选择自己喜欢的任何端口。通常建议在所有 ZooKeeper 服务器上使用相同的端口。</div><div class="line">server.1=zoo1:2888:3888</div><div class="line">server.2=zoo2:2888:3888</div><div class="line">server.3=zoo3:2888:3888</div></pre></td></tr></table></figure>
<h4 id="启动Zookeeper服务端"><a href="#启动Zookeeper服务端" class="headerlink" title="启动Zookeeper服务端"></a>启动Zookeeper服务端</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 分别启动Hadoop1，Hadoop2，Hadoop3的Zookeeper服务</div><div class="line">$ ./bin/zkServer.sh start</div><div class="line"></div><div class="line">ZooKeeper JMX enabled by defaultUsing config: /software/zookeeper-3.4.8/bin/../conf/zoo.cfgStarting zookeeper ... STARTED</div><div class="line"></div><div class="line"># 检查Hadoop1的Zookeeper服务状态（这里Hadoop1节点的zk是leader，Hadoop2和Hadoop3节点的zk是follower）</div><div class="line">$ ./bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /data/zookeeper-3.4.8/bin/../conf/zoo.cfgMode: leader</div><div class="line"></div><div class="line"># 检查Hadoop2的Zookeeper服务状态</div><div class="line">$ ./bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /data/zookeeper-3.4.8/bin/../conf/zoo.cfgMode: follower</div><div class="line"></div><div class="line"># 检查Hadoop3的Zookeeper服务状态</div><div class="line">$ ./bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /data/zookeeper-3.4.8/bin/../conf/zoo.cfgMode: follower</div></pre></td></tr></table></figure>
<h4 id="需要注意的地方"><a href="#需要注意的地方" class="headerlink" title="需要注意的地方"></a>需要注意的地方</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">JMX enabled by default</div><div class="line">Using config: /root/zookeeper-3.4.6/bin/../conf/zoo.cfg</div><div class="line">Error contacting service. It is probably not running.</div></pre></td></tr></table></figure>
<p>确认下面两点，应该就能排查出问题，我就遇到过重启Docker容器IP<br>地址变化，导致/etc/hosts中的IP地址配置不正确</p>
<ul>
<li>确认/etc/hosts中是否有各个节点域名解析</li>
<li>是否/var/zookeeper/myid有重复值</li>
<li>集群模式只启动一台也会遇到该问题，最好等把其他集群的机器启动好在查看状态</li>
</ul>
<h4 id="启动Zookeeper-Client端"><a href="#启动Zookeeper-Client端" class="headerlink" title="启动Zookeeper Client端"></a>启动Zookeeper Client端</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"># -server：client端连接的IP和端口号</div><div class="line">$ ./bin/zkCli.sh -server 127.0.0.1:2181Connecting to 127.0.0.1:21812016-09-30 01:51:22,268 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT2016-09-30 01:51:22,271 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=hadoop12016-09-30 01:51:22,271 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_792016-09-30 01:51:22,272 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2016-09-30 01:51:22,272 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/data/jdk1.7.0_79/jre2016-09-30 01:51:22,272 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/data/zookeeper-3.4.8/bin/../build/classes:/data/zookeeper-3.4.8/bin/../build/lib/*.jar:/data/zookeeper-3.4.8/bin/../lib/slf4j-log4j12-1.6.1.jar:/data/zookeeper-3.4.8/bin/../lib/slf4j-api-1.6.1.jar:/data/zookeeper-3.4.8/bin/../lib/netty-3.7.0.Final.jar:/data/zookeeper-3.4.8/bin/../lib/log4j-1.2.16.jar:/data/zookeeper-3.4.8/bin/../lib/jline-0.9.94.jar:/data/zookeeper-3.4.8/bin/../zookeeper-3.4.8.jar:/data/zookeeper-3.4.8/bin/../src/java/lib/*.jar:/data/zookeeper-3.4.8/bin/../conf:2016-09-30 01:51:22,273 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2016-09-30 01:51:22,273 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp2016-09-30 01:51:22,273 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;2016-09-30 01:51:22,273 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux2016-09-30 01:51:22,273 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd642016-09-30 01:51:22,273 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=3.16.0-77-generic2016-09-30 01:51:22,273 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=yunyu2016-09-30 01:51:22,273 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/home/yunyu2016-09-30 01:51:22,273 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/data/zookeeper-3.4.82016-09-30 01:51:22,275 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@71adff7cWelcome to ZooKeeper!2016-09-30 01:51:22,299 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@1032] - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)2016-09-30 01:51:22,303 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@876] - Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating sessionJLine support is enabled2016-09-30 01:51:22,337 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@1299] - Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x1577a41e9b90000, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: 127.0.0.1:2181(CONNECTED) 0]</div><div class="line"></div><div class="line"></div><div class="line"># zkShell中输入help会提示出所有的命令参数[zk: 127.0.0.1:2181(CONNECTED) 0] help</div><div class="line">ZooKeeper host:port cmd args</div><div class="line">        get path [watch]</div><div class="line">        ls path [watch]</div><div class="line">        set path data [version]</div><div class="line">        delquota [-n|-b] path</div><div class="line">        quit</div><div class="line">        printwatches on|off</div><div class="line">        create path data acl</div><div class="line">        stat path [watch]</div><div class="line">        listquota path</div><div class="line">        history</div><div class="line">        setAcl path acl</div><div class="line">        getAcl path</div><div class="line">        sync path</div><div class="line">        redo cmdno</div><div class="line">        addauth scheme auth</div><div class="line">        delete path [version]</div><div class="line">        deleteall path</div><div class="line">        setquota -n|-b val path</div><div class="line">        </div><div class="line"># 查看znode节点[zk: 127.0.0.1:2181(CONNECTED) 0] ls</div><div class="line">[zookeeper]</div><div class="line"></div><div class="line"># 创建新的znode节点，关联到&quot;my_data&quot;</div><div class="line">[zk: 127.0.0.1:2181(CONNECTED) 3] create /zk_test my_dataCreated /zk_test[zk: 127.0.0.1:2181(CONNECTED) 4] ls /</div><div class="line">[zookeeper, zk_test]</div><div class="line"></div><div class="line"># 验证/zk_test节点已经关联到&quot;my_data&quot;</div><div class="line">[zk: 127.0.0.1:2181(CONNECTED) 5] get /zk_testmy_datacZxid = 0x6ctime = Mon Aug 29 20:42:40 PDT 2016mZxid = 0x6mtime = Mon Aug 29 20:42:40 PDT 2016pZxid = 0x6cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 7numChildren = 0</div><div class="line"></div><div class="line"># 修改/zk_test节点的数据关联</div><div class="line">[zk: 127.0.0.1:2181(CONNECTED) 6] set /zk_test junkcZxid = 0x6ctime = Mon Aug 29 20:42:40 PDT 2016mZxid = 0x7mtime = Mon Aug 29 20:47:08 PDT 2016pZxid = 0x6cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 0</div><div class="line"></div><div class="line">[zk: 127.0.0.1:2181(CONNECTED) 7] get /zk_testjunkcZxid = 0x6ctime = Mon Aug 29 20:42:40 PDT 2016mZxid = 0x7mtime = Mon Aug 29 20:47:08 PDT 2016pZxid = 0x6cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 0</div><div class="line"></div><div class="line"># 删除/zk_test节点</div><div class="line">[zk: 127.0.0.1:2181(CONNECTED) 8] delete /zk_test[zk: 127.0.0.1:2181(CONNECTED) 9] ls /[zookeeper]</div></pre></td></tr></table></figure>
<p>OK，Zookeeper的cluster模式的配置就大功告成了 ^_^</p>
<p>参考文章：</p>
<ul>
<li><a href="http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html" target="_blank" rel="external">http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/data/library/bd-zookeeper/" target="_blank" rel="external">http://www.ibm.com/developerworks/cn/data/library/bd-zookeeper/</a></li>
<li><a href="https://segmentfault.com/a/1190000003994382" target="_blank" rel="external">https://segmentfault.com/a/1190000003994382</a></li>
<li><a href="http://blog.csdn.net/cruise_h/article/details/19046357" target="_blank" rel="external">http://blog.csdn.net/cruise_h/article/details/19046357</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Zookeeper/">Zookeeper</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Docker/Docker实战（十八）Docker安装Zookeeper集群环境" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/02/Docker/Docker实战（十八）Docker安装Zookeeper集群环境/" class="article-date">
  	<time datetime="2016-09-02T02:50:49.000Z" itemprop="datePublished">2016-09-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/02/Docker/Docker实战（十八）Docker安装Zookeeper集群环境/">Docker实战（十八）Docker安装Zookeeper集群环境</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h5 id="Dockerfile文件"><a href="#Dockerfile文件" class="headerlink" title="Dockerfile文件"></a>Dockerfile文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">############################################</div><div class="line"># version : birdben/zookeeper_cluster:v1</div><div class="line"># desc : 当前版本安装的zookeeper_cluster</div><div class="line">############################################</div><div class="line"># 设置继承自我们创建的 jdk7 镜像</div><div class="line">FROM birdben/jdk7:v1</div><div class="line"></div><div class="line"># 下面是一些创建者的基本信息</div><div class="line">MAINTAINER birdben (191654006@163.com)</div><div class="line"></div><div class="line"># 设置环境变量，所有操作都是非交互式的</div><div class="line">ENV DEBIAN_FRONTEND noninteractive</div><div class="line"></div><div class="line"># 添加 supervisord 的配置文件，并复制配置文件到对应目录下面。（supervisord.conf文件和Dockerfile文件在同一路径）</div><div class="line">COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf</div><div class="line"></div><div class="line"># 设置 zookeeper 的环境变量，若读者有其他的环境变量需要设置，也可以在这里添加。</div><div class="line">ENV ZOOKEEPER_HOME /software/zookeeper-3.4.8</div><div class="line">ENV PATH $&#123;ZOOKEEPER_HOME&#125;/bin:$PATH</div><div class="line"></div><div class="line"># 复制 zookeeper-3.4.8 文件到镜像中（zookeeper-3.4.8 文件夹要和Dockerfile文件在同一路径）</div><div class="line">ADD zookeeper-3.4.8 /software/zookeeper-3.4.8</div><div class="line"></div><div class="line"># 创建myid文件存储路径</div><div class="line">RUN mkdir -p /var/zookeeper/myid</div><div class="line"></div><div class="line"># 授权ZOOKEEPER_HOME路径给admin用户</div><div class="line">RUN sudo chown -R admin /software/zookeeper-3.4.8</div><div class="line"></div><div class="line"># 容器需要开放Zookeeper 2181, 2888, 3888端口</div><div class="line">EXPOSE 2181</div><div class="line">EXPOSE 2888</div><div class="line">EXPOSE 3888</div><div class="line"></div><div class="line"># 执行supervisord来同时执行多个命令，使用 supervisord 的可执行路径启动服务。</div><div class="line">CMD [&quot;/usr/bin/supervisord&quot;]</div></pre></td></tr></table></figure>
<h5 id="Dockerfile源文件链接："><a href="#Dockerfile源文件链接：" class="headerlink" title="Dockerfile源文件链接："></a>Dockerfile源文件链接：</h5><p><a href="https://github.com/birdben/birdDocker/blob/master/zookeeper_cluster/Dockerfile">https://github.com/birdben/birdDocker/blob/master/zookeeper_cluster/Dockerfile</a></p>
<h5 id="supervisor配置文件内容"><a href="#supervisor配置文件内容" class="headerlink" title="supervisor配置文件内容"></a>supervisor配置文件内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 配置文件包含目录和进程</div><div class="line"># 第一段 supervsord 配置软件本身，使用 nodaemon 参数来运行。</div><div class="line"># 第二段包含要控制的 2 个服务。每一段包含一个服务的目录和启动这个服务的命令。</div><div class="line"></div><div class="line">[supervisord]</div><div class="line">nodaemon=true</div><div class="line"></div><div class="line">[program:sshd]</div><div class="line">command=/usr/sbin/sshd -D</div><div class="line"></div><div class="line">[program:zookeeper]</div><div class="line">command=/bin/bash -c &quot;exec $&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh start-foreground&quot;</div></pre></td></tr></table></figure>
<h5 id="配置ZOOKEEPER-HOME-conf-zoo-cfg并不在conf目录-需要复制zoo-sample-cfg并改名为zoo-cfg"><a href="#配置ZOOKEEPER-HOME-conf-zoo-cfg并不在conf目录-需要复制zoo-sample-cfg并改名为zoo-cfg" class="headerlink" title="配置ZOOKEEPER_HOME/conf/zoo.cfg并不在conf目录, 需要复制zoo_sample.cfg并改名为zoo.cfg"></a>配置ZOOKEEPER_HOME/conf/zoo.cfg并不在conf目录, 需要复制zoo_sample.cfg并改名为zoo.cfg</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"># The number of milliseconds of each tick</div><div class="line">tickTime=2000</div><div class="line"># The number of ticks that the initial </div><div class="line"># synchronization phase can take</div><div class="line">initLimit=10</div><div class="line"># The number of ticks that can pass between </div><div class="line"># sending a request and getting an acknowledgement</div><div class="line">syncLimit=5</div><div class="line"># the directory where the snapshot is stored.</div><div class="line"># do not use /tmp for storage, /tmp here is just </div><div class="line"># example sakes.</div><div class="line">dataDir=/var/zookeeper</div><div class="line"># the port at which the clients will connect</div><div class="line">clientPort=2181</div><div class="line"># the maximum number of client connections.</div><div class="line"># increase this if you need to handle more clients</div><div class="line">#maxClientCnxns=60</div><div class="line">#</div><div class="line"># Be sure to read the maintenance section of the </div><div class="line"># administrator guide before turning on autopurge.</div><div class="line">#</div><div class="line"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</div><div class="line">#</div><div class="line"># The number of snapshots to retain in dataDir</div><div class="line">#autopurge.snapRetainCount=3</div><div class="line"># Purge task interval in hours</div><div class="line"># Set to &quot;0&quot; to disable auto purge feature</div><div class="line">#autopurge.purgeInterval=1</div><div class="line"></div><div class="line"># 第一个port是从机器（follower）连接到主机器（leader）的端口号，第二个port是进行leadership选举的端口号。</div><div class="line">server.1=zoo1:2888:3888</div><div class="line">server.2=zoo2:2888:3888</div><div class="line">server.3=zoo3:2888:3888</div></pre></td></tr></table></figure>
<h5 id="控制台终端"><a href="#控制台终端" class="headerlink" title="控制台终端"></a>控制台终端</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"># 构建镜像</div><div class="line">$ docker build -t &quot;birdben/zookeeper_cluster:v1&quot; .</div><div class="line"># 启动Docker容器，这里分别对每个docker容器指定了不同的hostname</div><div class="line"># 需要暴露2181客户端连接端口号，否则Docker容器外无法连接到zookeeper集群</div><div class="line">$ sudo docker run -p 2181:2181 -p 2888:2888 -p 3888:3888 -h zoo1 --name zoo1 -t -i &apos;birdben/zookeeper_cluster:v1&apos;</div><div class="line">$ sudo docker run -p 2181:2181 -p 2888:2888 -p 3888:3888 -h zoo2 --name zoo2 -t -i &apos;birdben/zookeeper_cluster:v1&apos;</div><div class="line">$ sudo docker run -p 2181:2181 -p 2888:2888 -p 3888:3888 -h zoo3 --name zoo3 -t -i &apos;birdben/zookeeper_cluster:v1&apos;</div><div class="line"></div><div class="line"># 查询Docker容器对应的IP地址</div><div class="line">$ sudo docker inspect --format=&apos;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&apos; $&#123;CONTAINER_ID&#125;</div><div class="line"></div><div class="line"># 需要exec进入Docker容器配置myid和hosts文件</div><div class="line">$ sudo docker exec -it $&#123;CONTAINER_ID&#125; /bin/bash</div><div class="line"></div><div class="line"># 配置每个Docker容器的myid，对应zoo序号执行</div><div class="line">$ echo 1 &gt; /var/zookeeper/myid</div><div class="line">$ echo 2 &gt; /var/zookeeper/myid</div><div class="line">$ echo 3 &gt; /var/zookeeper/myid</div><div class="line"></div><div class="line"># 配置每个Docker容器的/etc/hosts文件</div><div class="line">172.17.0.51     zoo1</div><div class="line">172.17.0.52     zoo2</div><div class="line">172.17.0.53     zoo3</div><div class="line"></div><div class="line"># 分别启动每个Docker容器中的zookeeper服务</div><div class="line">$ ./&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh start</div><div class="line"></div><div class="line"># 查看每个Docker容器的zookeeper运行状态</div><div class="line">$ ./&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh status</div><div class="line"></div><div class="line"># 下面是我查看每个zookeeper的状态，zoo2的Docker容器的zk是leader，zoo1和zoo3是follower</div><div class="line">root@zoo1:/software/zookeeper-3.4.8/bin# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /software/zookeeper-3.4.8/bin/../conf/zoo.cfgMode: follower</div><div class="line"></div><div class="line">root@zoo2:/software/zookeeper-3.4.8/bin# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /software/zookeeper-3.4.8/bin/../conf/zoo.cfgMode: leader</div><div class="line"></div><div class="line">root@zoo3:/software/zookeeper-3.4.8/bin# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /software/zookeeper-3.4.8/bin/../conf/zoo.cfgMode: follower</div></pre></td></tr></table></figure>
<h5 id="使用zkCli-sh连接服务端进行操作"><a href="#使用zkCli-sh连接服务端进行操作" class="headerlink" title="使用zkCli.sh连接服务端进行操作"></a>使用zkCli.sh连接服务端进行操作</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">$ ./zkCli.sh -server 10.10.1.167:2181</div><div class="line">Connecting to 10.10.1.167:2181</div><div class="line">2016-09-02 11:01:56,761 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT</div><div class="line">2016-09-02 11:01:56,764 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=localhost</div><div class="line">2016-09-02 11:01:56,764 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_79</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/Users/yunyu/dev/zookeeper-3.4.8/bin/../build/classes:/Users/yunyu/dev/zookeeper-3.4.8/bin/../build/lib/*.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../lib/slf4j-log4j12-1.6.1.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../lib/slf4j-api-1.6.1.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../lib/netty-3.7.0.Final.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../lib/log4j-1.2.16.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../lib/jline-0.9.94.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../zookeeper-3.4.8.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../src/java/lib/*.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../conf:</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/Users/yunyu/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/var/folders/0h/jtjrr7g95mv2pt4ts1tgmzyh0000gn/T/</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Mac OS X</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=x86_64</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=10.11.5</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=yunyu</div><div class="line">2016-09-02 11:01:56,766 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/Users/yunyu</div><div class="line">2016-09-02 11:01:56,767 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/Users/yunyu/dev/zookeeper-3.4.8/bin</div><div class="line">2016-09-02 11:01:56,767 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=10.10.1.167:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@5be7d8b1</div><div class="line">Welcome to ZooKeeper!</div><div class="line">2016-09-02 11:01:56,791 [myid:] - INFO  [main-SendThread(10.10.1.167:2181):ClientCnxn$SendThread@1032] - Opening socket connection to server 10.10.1.167/10.10.1.167:2181. Will not attempt to authenticate using SASL (unknown error)</div><div class="line">JLine support is enabled</div><div class="line">2016-09-02 11:01:56,798 [myid:] - INFO  [main-SendThread(10.10.1.167:2181):ClientCnxn$SendThread@876] - Socket connection established to 10.10.1.167/10.10.1.167:2181, initiating session</div><div class="line">2016-09-02 11:01:56,821 [myid:] - INFO  [main-SendThread(10.10.1.167:2181):ClientCnxn$SendThread@1299] - Session establishment complete on server 10.10.1.167/10.10.1.167:2181, sessionid = 0x156e8d804300000, negotiated timeout = 30000</div><div class="line"></div><div class="line">WATCHER::</div><div class="line"></div><div class="line">WatchedEvent state:SyncConnected type:None path:null</div><div class="line">[zk: 10.10.1.167:2181(CONNECTED) 0]</div></pre></td></tr></table></figure>
<h5 id="需要注意的地方"><a href="#需要注意的地方" class="headerlink" title="需要注意的地方"></a>需要注意的地方</h5><ul>
<li>因为我们zookeeper的启动方式是用的supervisor启动，但是Docker容器启动的时候，我们还不知道Docker容器的IP地址，无法指定hosts文件配置，所以我们要先进入到Docker容器指定好hosts文件配置，然后重新启动zookeeper服务</li>
<li>myid的配置也是每个Docker容器都不一样，最好跟hosts配置对应</li>
<li>需要Docker容器外连接zookeeper集群需要在启动Docker容器时，指定一个Docker容器对外开放2181客户端连接端口号，否则Docker容器外无法连接到zookeeper集群</li>
<li>如果查看zookeeper运行状态提示有问题</li>
<li>值得重点注意的一点是，所有三个机器都应该打开端口 2181、2888 和 3888。在本例中，端口 2181 由 ZooKeeper 客户端使用，用于连接到 ZooKeeper 服务器；端口 2888 由对等 ZooKeeper 服务器使用，用于互相通信；而端口 3888 用于领导者选举。您可以选择自己喜欢的任何端口。通常建议在所有 ZooKeeper 服务器上使用相同的端口。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">JMX enabled by default</div><div class="line">Using config: /root/zookeeper-3.4.6/bin/../conf/zoo.cfg</div><div class="line">Error contacting service. It is probably not running.</div></pre></td></tr></table></figure>
<p>确认下面两点，应该就能排查出问题，我就遇到过重启Docker容器IP<br>地址变化，导致/etc/hosts中的IP地址配置不正确</p>
<ul>
<li>确认/etc/hosts中是否有各个节点域名解析</li>
<li>是否/var/zookeeper/myid有重复值</li>
</ul>
<p>参考文章：</p>
<ul>
<li><a href="https://segmentfault.com/a/1190000003994382" target="_blank" rel="external">https://segmentfault.com/a/1190000003994382</a></li>
<li><a href="http://blog.csdn.net/cruise_h/article/details/19046357" target="_blank" rel="external">http://blog.csdn.net/cruise_h/article/details/19046357</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dockerfile/">Dockerfile</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker命令/">Docker命令</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Zookeeper/">Zookeeper</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Docker/">Docker</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Zookeeper/Zookeeper学习（一）Zookeeper环境搭建" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/01/Zookeeper/Zookeeper学习（一）Zookeeper环境搭建/" class="article-date">
  	<time datetime="2016-09-01T14:18:35.000Z" itemprop="datePublished">2016-09-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/01/Zookeeper/Zookeeper学习（一）Zookeeper环境搭建/">Zookeeper学习（一）Zookeeper环境搭建</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Zookeeper安装"><a href="#Zookeeper安装" class="headerlink" title="Zookeeper安装"></a>Zookeeper安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ wget http://apache.fayea.com/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz</div><div class="line">$ tar -zxvf zookeeper-3.4.8.tar.gz</div><div class="line">$ cd zookeeper-3.4.8</div><div class="line"></div><div class="line"># 修改zookeeper配置文件</div><div class="line">$ cp conf/zoo_sample.cfg conf/zoo.cfg</div></pre></td></tr></table></figure>
<h3 id="ZooKeeper-Standalone模式"><a href="#ZooKeeper-Standalone模式" class="headerlink" title="ZooKeeper Standalone模式"></a>ZooKeeper Standalone模式</h3><h4 id="Zookeeper配置文件"><a href="#Zookeeper配置文件" class="headerlink" title="Zookeeper配置文件"></a>Zookeeper配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># the basic time unit in milliseconds used by ZooKeeper. It is used to do heartbeats and the minimum session timeout will be twice the tickTime.</div><div class="line"># tickTime 这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳。</div><div class="line">tickTime=2000</div><div class="line"></div><div class="line"># the location to store the in-memory database snapshots and, unless specified otherwise, the transaction log of updates to the database.</div><div class="line"># dataDir 顾名思义就是Zookeeper保存数据的目录,默认情况下Zookeeper将写数据的日志文件也保存在这个目录里。</div><div class="line">dataDir=/var/lib/zookeeper</div><div class="line"></div><div class="line"># the port to listen for client connections</div><div class="line"># clientPort 这个端口就是客户端（应用程序）连接Zookeeper服务器的端口，Zookeeper会监听这个端口接受客户端的访问请求。</div><div class="line">clientPort=2181</div><div class="line"></div><div class="line"># initLimit 这个配置项是用来配置Zookeeper接受客户端（这里所说的客户端不是用户连接Zookeeper 服务器的客户端，而是Zookeeper服务器集群中连接到Leader的Follower服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过10个心跳的时间（也就是tickTime）长度后Zookeeper服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 10*2000=20 秒。</div><div class="line">initLimit=10</div><div class="line"></div><div class="line"># syncLimit 这个配置项标识Leader与Follower之间发送消息，请求和应答时间长度，最长不能超过多少个tickTime的时间长度，总的时间长度就是 5*2000=10 秒。</div><div class="line">syncLimit=5</div></pre></td></tr></table></figure>
<h4 id="启动Zookeeper服务端"><a href="#启动Zookeeper服务端" class="headerlink" title="启动Zookeeper服务端"></a>启动Zookeeper服务端</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ ./bin/zkServer.sh start</div><div class="line"></div><div class="line">ZooKeeper JMX enabled by defaultUsing config: /software/zookeeper-3.4.8/bin/../conf/zoo.cfgStarting zookeeper ... STARTED</div></pre></td></tr></table></figure>
<h4 id="检查Zookeeper启动状态"><a href="#检查Zookeeper启动状态" class="headerlink" title="检查Zookeeper启动状态"></a>检查Zookeeper启动状态</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ ./bin/zkServer.sh status</div><div class="line"></div><div class="line">ZooKeeper JMX enabled by defaultUsing config: /software/zookeeper-3.4.8/bin/../conf/zoo.cfgMode: standalone</div><div class="line"></div><div class="line"># 查看zookeeper的PID</div><div class="line">$ ps -ef | grep zookeeper</div><div class="line">yunyu    16845  2068  0 11:06 pts/1    00:00:02 /usr/local/jdk1.7.0_79/bin/java -Dzookeeper.log.dir=. -Dzookeeper.root.logger=INFO,CONSOLE -cp /software/zookeeper-3.4.8/bin/../build/classes:/software/zookeeper-3.4.8/bin/../build/lib/*.jar:/software/zookeeper-3.4.8/bin/../lib/slf4j-log4j12-1.6.1.jar:/software/zookeeper-3.4.8/bin/../lib/slf4j-api-1.6.1.jar:/software/zookeeper-3.4.8/bin/../lib/netty-3.7.0.Final.jar:/software/zookeeper-3.4.8/bin/../lib/log4j-1.2.16.jar:/software/zookeeper-3.4.8/bin/../lib/jline-0.9.94.jar:/software/zookeeper-3.4.8/bin/../zookeeper-3.4.8.jar:/software/zookeeper-3.4.8/bin/../src/java/lib/*.jar:/software/zookeeper-3.4.8/bin/../conf: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /software/zookeeper-3.4.8/bin/../conf/zoo.cfg</div><div class="line">yunyu    21506  2748  0 11:31 pts/1    00:00:00 grep --color=auto zookeeper</div><div class="line"></div><div class="line"># 查看一下在监听2181端口的PID</div><div class="line">$ lsof -i:2181</div><div class="line">COMMAND   PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAMEjava    16845 yunyu   19u  IPv6  50306      0t0  TCP *:2181 (LISTEN)</div></pre></td></tr></table></figure>
<h4 id="启动Zookeeper-Client端"><a href="#启动Zookeeper-Client端" class="headerlink" title="启动Zookeeper Client端"></a>启动Zookeeper Client端</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"># -server：client端连接的IP和端口号</div><div class="line">$ ./zkCli.sh -server 127.0.0.1:2181</div><div class="line"></div><div class="line"># Zookeeper Client端控制台会有类似如下的输出信息</div><div class="line">Connecting to 127.0.0.1:21812016-08-29 20:35:25,389 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT2016-08-29 20:35:25,392 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=ubuntu2016-08-29 20:35:25,393 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_792016-08-29 20:35:25,396 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2016-08-29 20:35:25,396 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/software/jdk1.7.0_79/jre2016-08-29 20:35:25,396 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/software/zookeeper-3.4.8/bin/../build/classes:/software/zookeeper-3.4.8/bin/../build/lib/*.jar:/software/zookeeper-3.4.8/bin/../lib/slf4j-log4j12-1.6.1.jar:/software/zookeeper-3.4.8/bin/../lib/slf4j-api-1.6.1.jar:/software/zookeeper-3.4.8/bin/../lib/netty-3.7.0.Final.jar:/software/zookeeper-3.4.8/bin/../lib/log4j-1.2.16.jar:/software/zookeeper-3.4.8/bin/../lib/jline-0.9.94.jar:/software/zookeeper-3.4.8/bin/../zookeeper-3.4.8.jar:/software/zookeeper-3.4.8/bin/../src/java/lib/*.jar:/software/zookeeper-3.4.8/bin/../conf:2016-08-29 20:35:25,396 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2016-08-29 20:35:25,396 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp2016-08-29 20:35:25,396 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;2016-08-29 20:35:25,397 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux2016-08-29 20:35:25,397 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd642016-08-29 20:35:25,397 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=3.16.0-77-generic2016-08-29 20:35:25,397 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=yunyu2016-08-29 20:35:25,397 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/home/yunyu2016-08-29 20:35:25,397 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/software/zookeeper-3.4.8/bin2016-08-29 20:35:25,398 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@56606032Welcome to ZooKeeper!2016-08-29 20:35:25,416 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@1032] - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)2016-08-29 20:35:25,420 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@876] - Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating sessionJLine support is enabled[zk: 127.0.0.1:2181(CONNECTING) 0] 2016-08-29 20:35:25,447 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn$SendThread@1299] - Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x156d969c5940002, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: 127.0.0.1:2181(CONNECTED) 0][zk: 127.0.0.1:2181(CONNECTED) 0][zk: 127.0.0.1:2181(CONNECTED) 0]</div><div class="line"></div><div class="line"></div><div class="line"># zkShell中输入help会提示出所有的命令参数[zk: 127.0.0.1:2181(CONNECTED) 0] help</div><div class="line">ZooKeeper host:port cmd args</div><div class="line">        get path [watch]</div><div class="line">        ls path [watch]</div><div class="line">        set path data [version]</div><div class="line">        delquota [-n|-b] path</div><div class="line">        quit</div><div class="line">        printwatches on|off</div><div class="line">        create path data acl</div><div class="line">        stat path [watch]</div><div class="line">        listquota path</div><div class="line">        history</div><div class="line">        setAcl path acl</div><div class="line">        getAcl path</div><div class="line">        sync path</div><div class="line">        redo cmdno</div><div class="line">        addauth scheme auth</div><div class="line">        delete path [version]</div><div class="line">        deleteall path</div><div class="line">        setquota -n|-b val path</div><div class="line">        </div><div class="line"># 查看znode节点[zk: 127.0.0.1:2181(CONNECTED) 0] ls</div><div class="line">[zookeeper]</div><div class="line"></div><div class="line"># 创建新的znode节点，关联到&quot;my_data&quot;</div><div class="line">[zk: 127.0.0.1:2181(CONNECTED) 3] create /zk_test my_dataCreated /zk_test[zk: 127.0.0.1:2181(CONNECTED) 4] ls /</div><div class="line">[zookeeper, zk_test]</div><div class="line"></div><div class="line"># 验证/zk_test节点已经关联到&quot;my_data&quot;</div><div class="line">[zk: 127.0.0.1:2181(CONNECTED) 5] get /zk_testmy_datacZxid = 0x6ctime = Mon Aug 29 20:42:40 PDT 2016mZxid = 0x6mtime = Mon Aug 29 20:42:40 PDT 2016pZxid = 0x6cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 7numChildren = 0</div><div class="line"></div><div class="line"># 修改/zk_test节点的数据关联</div><div class="line">[zk: 127.0.0.1:2181(CONNECTED) 6] set /zk_test junkcZxid = 0x6ctime = Mon Aug 29 20:42:40 PDT 2016mZxid = 0x7mtime = Mon Aug 29 20:47:08 PDT 2016pZxid = 0x6cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 0</div><div class="line"></div><div class="line">[zk: 127.0.0.1:2181(CONNECTED) 7] get /zk_test     junkcZxid = 0x6ctime = Mon Aug 29 20:42:40 PDT 2016mZxid = 0x7mtime = Mon Aug 29 20:47:08 PDT 2016pZxid = 0x6cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 0</div><div class="line"></div><div class="line"># 删除/zk_test节点</div><div class="line">[zk: 127.0.0.1:2181(CONNECTED) 8] delete /zk_test[zk: 127.0.0.1:2181(CONNECTED) 9] ls /[zookeeper]</div></pre></td></tr></table></figure>
<h4 id="检查Zookeeper-Client连接后的进程状态"><a href="#检查Zookeeper-Client连接后的进程状态" class="headerlink" title="检查Zookeeper Client连接后的进程状态"></a>检查Zookeeper Client连接后的进程状态</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 查看zookeeper的PID</div><div class="line">$ ps -ef | grep zookeeper</div><div class="line">yunyu@ubuntu:/software/zookeeper-3.4.8/bin$ ps -ef | grep zookeeperyunyu    16845  2068  0 11:06 pts/1    00:00:02 /usr/local/jdk1.7.0_79/bin/java -Dzookeeper.log.dir=. -Dzookeeper.root.logger=INFO,CONSOLE -cp /software/zookeeper-3.4.8/bin/../build/classes:/software/zookeeper-3.4.8/bin/../build/lib/*.jar:/software/zookeeper-3.4.8/bin/../lib/slf4j-log4j12-1.6.1.jar:/software/zookeeper-3.4.8/bin/../lib/slf4j-api-1.6.1.jar:/software/zookeeper-3.4.8/bin/../lib/netty-3.7.0.Final.jar:/software/zookeeper-3.4.8/bin/../lib/log4j-1.2.16.jar:/software/zookeeper-3.4.8/bin/../lib/jline-0.9.94.jar:/software/zookeeper-3.4.8/bin/../zookeeper-3.4.8.jar:/software/zookeeper-3.4.8/bin/../src/java/lib/*.jar:/software/zookeeper-3.4.8/bin/../conf: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /software/zookeeper-3.4.8/bin/../conf/zoo.cfgyunyu    17569 17564  0 11:09 pts/7    00:00:02 /usr/local/jdk1.7.0_79/bin/java -Dzookeeper.log.dir=. -Dzookeeper.root.logger=INFO,CONSOLE -cp /software/zookeeper-3.4.8/bin/../build/classes:/software/zookeeper-3.4.8/bin/../build/lib/*.jar:/software/zookeeper-3.4.8/bin/../lib/slf4j-log4j12-1.6.1.jar:/software/zookeeper-3.4.8/bin/../lib/slf4j-api-1.6.1.jar:/software/zookeeper-3.4.8/bin/../lib/netty-3.7.0.Final.jar:/software/zookeeper-3.4.8/bin/../lib/log4j-1.2.16.jar:/software/zookeeper-3.4.8/bin/../lib/jline-0.9.94.jar:/software/zookeeper-3.4.8/bin/../zookeeper-3.4.8.jar:/software/zookeeper-3.4.8/bin/../src/java/lib/*.jar:/software/zookeeper-3.4.8/bin/../conf: org.apache.zookeeper.ZooKeeperMain -server 127.0.0.1:2181yunyu    21506  2748  0 11:31 pts/1    00:00:00 grep --color=auto zookeeper</div><div class="line"></div><div class="line"># 查看一下在监听2181端口的PID</div><div class="line">yunyu@ubuntu:/software/zookeeper-3.4.8/bin$ lsof -i:2181COMMAND   PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAMEjava    16845 yunyu   19u  IPv6  50306      0t0  TCP *:2181 (LISTEN)java    16845 yunyu   20u  IPv6  51041      0t0  TCP localhost:2181-&gt;localhost:40895 (ESTABLISHED)java    17569 yunyu   13u  IPv6  51020      0t0  TCP localhost:40895-&gt;localhost:2181 (ESTABLISHED)</div></pre></td></tr></table></figure>
<p>OK，Zookeeper的standalone模式的配置就大功告成了 ^_^</p>
<p>参考文章：</p>
<ul>
<li><a href="http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html" target="_blank" rel="external">http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Zookeeper/">Zookeeper</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Docker/Docker实战（十七）Docker安装Zookeeper环境" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/01/Docker/Docker实战（十七）Docker安装Zookeeper环境/" class="article-date">
  	<time datetime="2016-09-01T06:50:49.000Z" itemprop="datePublished">2016-09-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/01/Docker/Docker实战（十七）Docker安装Zookeeper环境/">Docker实战（十七）Docker安装Zookeeper环境</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h5 id="Zookeeper安装"><a href="#Zookeeper安装" class="headerlink" title="Zookeeper安装"></a>Zookeeper安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># 下载Zookeeper</div><div class="line">$ wget http://apache.fayea.com/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz</div><div class="line"></div><div class="line"># 解压Zookeeper压缩包</div><div class="line">$ tar -zxvf zookeeper-3.4.8.tar.gz</div><div class="line"></div><div class="line"># 需要修改下面Zookeeper相关的配置文件</div></pre></td></tr></table></figure>
<h5 id="Dockerfile文件"><a href="#Dockerfile文件" class="headerlink" title="Dockerfile文件"></a>Dockerfile文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">############################################</div><div class="line"># version : birdben/zookeeper:v1</div><div class="line"># desc : 当前版本安装的zookeeper</div><div class="line">############################################</div><div class="line"># 设置继承自我们创建的 jdk7 镜像</div><div class="line">FROM birdben/jdk7:v1</div><div class="line"></div><div class="line"># 下面是一些创建者的基本信息</div><div class="line">MAINTAINER birdben (191654006@163.com)</div><div class="line"></div><div class="line"># 设置环境变量，所有操作都是非交互式的</div><div class="line">ENV DEBIAN_FRONTEND noninteractive</div><div class="line"></div><div class="line"># 添加 supervisord 的配置文件，并复制配置文件到对应目录下面。（supervisord.conf文件和Dockerfile文件在同一路径）</div><div class="line">COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf</div><div class="line"></div><div class="line"># 设置 zookeeper 的环境变量，若读者有其他的环境变量需要设置，也可以在这里添加。</div><div class="line">ENV ZOOKEEPER_HOME /software/zookeeper-3.4.8</div><div class="line">ENV PATH $&#123;ZOOKEEPER_HOME&#125;/bin:$PATH</div><div class="line"></div><div class="line"># 复制 zookeeper-3.4.8 文件到镜像中（zookeeper-3.4.8 文件夹要和Dockerfile文件在同一路径）</div><div class="line">ADD zookeeper-3.4.8 /software/zookeeper-3.4.8</div><div class="line"></div><div class="line"># 创建myid文件存储路径</div><div class="line">RUN mkdir -p /var/zookeeper/myid</div><div class="line"></div><div class="line"># 授权ZOOKEEPER_HOME路径给admin用户</div><div class="line">RUN sudo chown -R admin /software/zookeeper-3.4.8</div><div class="line"></div><div class="line"># 容器需要开放Zookeeper 2181端口</div><div class="line">EXPOSE 2181</div><div class="line"></div><div class="line"># 执行supervisord来同时执行多个命令，使用 supervisord 的可执行路径启动服务。</div><div class="line">CMD [&quot;/usr/bin/supervisord&quot;]</div></pre></td></tr></table></figure>
<h5 id="Dockerfile源文件链接："><a href="#Dockerfile源文件链接：" class="headerlink" title="Dockerfile源文件链接："></a>Dockerfile源文件链接：</h5><p><a href="https://github.com/birdben/birdDocker/blob/master/zookeeper/Dockerfile">https://github.com/birdben/birdDocker/blob/master/zookeeper/Dockerfile</a></p>
<h5 id="supervisor配置文件内容"><a href="#supervisor配置文件内容" class="headerlink" title="supervisor配置文件内容"></a>supervisor配置文件内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 配置文件包含目录和进程</div><div class="line"># 第一段 supervsord 配置软件本身，使用 nodaemon 参数来运行。</div><div class="line"># 第二段包含要控制的 2 个服务。每一段包含一个服务的目录和启动这个服务的命令。</div><div class="line"></div><div class="line">[supervisord]</div><div class="line">nodaemon=true</div><div class="line"></div><div class="line">[program:sshd]</div><div class="line">command=/usr/sbin/sshd -D</div><div class="line"></div><div class="line">[program:zookeeper]</div><div class="line">command=/bin/bash -c &quot;exec $&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh start-foreground&quot;</div></pre></td></tr></table></figure>
<h5 id="配置ZOOKEEPER-HOME-conf-zoo-cfg并不在conf目录-需要复制zoo-sample-cfg并改名为zoo-cfg"><a href="#配置ZOOKEEPER-HOME-conf-zoo-cfg并不在conf目录-需要复制zoo-sample-cfg并改名为zoo-cfg" class="headerlink" title="配置ZOOKEEPER_HOME/conf/zoo.cfg并不在conf目录, 需要复制zoo_sample.cfg并改名为zoo.cfg"></a>配置ZOOKEEPER_HOME/conf/zoo.cfg并不在conf目录, 需要复制zoo_sample.cfg并改名为zoo.cfg</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=/var/zookeeper# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1</div></pre></td></tr></table></figure>
<h5 id="控制台终端"><a href="#控制台终端" class="headerlink" title="控制台终端"></a>控制台终端</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 构建镜像</div><div class="line">$ docker build -t &quot;birdben/zookeeper:v1&quot; .</div><div class="line"># 执行已经构件好的镜像</div><div class="line">$ docker run -p 9999:22 -p 2181:2181 -t -i &quot;birdben/zookeeper:v1&quot;</div></pre></td></tr></table></figure>
<h5 id="supervisor无法监控zookeeper"><a href="#supervisor无法监控zookeeper" class="headerlink" title="supervisor无法监控zookeeper"></a>supervisor无法监控zookeeper</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">supervisor启动的程序必须是非daemon的启动方式，这里找到zookeeper的启动脚本$&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh，正常的参数可以选择start, status, stop等等。这里我们找到参数start-foreground，这个就是非守护进程的方式启动。所以supervisord.conf配置文件修改成下面的方式</div><div class="line">$&#123;ZOOKEEPER_HOME&#125;/bin/zkServer.sh start-foreground</div></pre></td></tr></table></figure>
<h5 id="使用zkCli-sh连接服务端进行操作"><a href="#使用zkCli-sh连接服务端进行操作" class="headerlink" title="使用zkCli.sh连接服务端进行操作"></a>使用zkCli.sh连接服务端进行操作</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"># 我们通过另一个zk客户端连接到Docker容器的zk服务端</div><div class="line">$ ./zkCli.sh -server 10.10.1.167:2181</div><div class="line">Connecting to 10.10.1.167:2181</div><div class="line">2016-09-01 14:32:42,398 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT</div><div class="line">2016-09-01 14:32:42,402 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=localhost</div><div class="line">2016-09-01 14:32:42,402 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_79</div><div class="line">2016-09-01 14:32:42,405 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</div><div class="line">2016-09-01 14:32:42,405 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre</div><div class="line">2016-09-01 14:32:42,406 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/Users/yunyu/dev/zookeeper-3.4.8/bin/../build/classes:/Users/yunyu/dev/zookeeper-3.4.8/bin/../build/lib/*.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../lib/slf4j-log4j12-1.6.1.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../lib/slf4j-api-1.6.1.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../lib/netty-3.7.0.Final.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../lib/log4j-1.2.16.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../lib/jline-0.9.94.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../zookeeper-3.4.8.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../src/java/lib/*.jar:/Users/yunyu/dev/zookeeper-3.4.8/bin/../conf:</div><div class="line">2016-09-01 14:32:42,406 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/Users/yunyu/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.</div><div class="line">2016-09-01 14:32:42,406 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/var/folders/0h/jtjrr7g95mv2pt4ts1tgmzyh0000gn/T/</div><div class="line">2016-09-01 14:32:42,406 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;</div><div class="line">2016-09-01 14:32:42,406 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Mac OS X</div><div class="line">2016-09-01 14:32:42,406 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=x86_64</div><div class="line">2016-09-01 14:32:42,406 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=10.11.5</div><div class="line">2016-09-01 14:32:42,406 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=yunyu</div><div class="line">2016-09-01 14:32:42,406 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/Users/yunyu</div><div class="line">2016-09-01 14:32:42,407 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/Users/yunyu/dev/zookeeper-3.4.8/bin</div><div class="line">2016-09-01 14:32:42,408 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=10.10.1.167:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@3823ed31</div><div class="line">Welcome to ZooKeeper!</div><div class="line">2016-09-01 14:32:42,441 [myid:] - INFO  [main-SendThread(10.10.1.167:2181):ClientCnxn$SendThread@1032] - Opening socket connection to server 10.10.1.167/10.10.1.167:2181. Will not attempt to authenticate using SASL (unknown error)</div><div class="line">JLine support is enabled</div><div class="line">2016-09-01 14:32:42,453 [myid:] - INFO  [main-SendThread(10.10.1.167:2181):ClientCnxn$SendThread@876] - Socket connection established to 10.10.1.167/10.10.1.167:2181, initiating session</div><div class="line">2016-09-01 14:32:42,492 [myid:] - INFO  [main-SendThread(10.10.1.167:2181):ClientCnxn$SendThread@1299] - Session establishment complete on server 10.10.1.167/10.10.1.167:2181, sessionid = 0x156e4682ae30000, negotiated timeout = 30000</div><div class="line"></div><div class="line">WATCHER::</div><div class="line"></div><div class="line">WatchedEvent state:SyncConnected type:None path:null</div><div class="line">[zk: 10.10.1.167:2181(CONNECTED) 0]</div><div class="line"></div><div class="line"># 查看znode节点[zk: 10.10.1.167:2181(CONNECTED) 0] ls</div><div class="line">[zookeeper]</div><div class="line"></div><div class="line"># 创建新的znode节点，关联到&quot;my_data&quot;</div><div class="line">[zk: 10.10.1.167:2181(CONNECTED) 3] create /zk_test my_dataCreated /zk_test[zk: 10.10.1.167:2181(CONNECTED) 4] ls /</div><div class="line">[zookeeper, zk_test]</div></pre></td></tr></table></figure>
<h5 id="遇到问题zkServer-sh-status"><a href="#遇到问题zkServer-sh-status" class="headerlink" title="遇到问题zkServer.sh status"></a>遇到问题zkServer.sh status</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"># SSH登录到Docker容器，查看supervisor的状态</div><div class="line">$ sudo supervisorctl status</div><div class="line">sshd                             RUNNING    pid 8, uptime 0:35:36</div><div class="line">zookeeper                        RUNNING    pid 9, uptime 0:35:36</div><div class="line"></div><div class="line"># 查看zookeeper进程</div><div class="line">$ ps -ef | grep zookeeper</div><div class="line">root         9     1  0 06:20 ?        00:00:04 /software/jdk7/bin/java -Dzookeeper.log.dir=. -Dzookeeper.root.logger=INFO,CONSOLE -cp /software/zookeeper-3.4.8/bin/../build/classes:/software/zookeeper-3.4.8/bin/../build/lib/*.jar:/software/zookeeper-3.4.8/bin/../lib/slf4j-log4j12-1.6.1.jar:/software/zookeeper-3.4.8/bin/../lib/slf4j-api-1.6.1.jar:/software/zookeeper-3.4.8/bin/../lib/netty-3.7.0.Final.jar:/software/zookeeper-3.4.8/bin/../lib/log4j-1.2.16.jar:/software/zookeeper-3.4.8/bin/../lib/jline-0.9.94.jar:/software/zookeeper-3.4.8/bin/../zookeeper-3.4.8.jar:/software/zookeeper-3.4.8/bin/../src/java/lib/*.jar:/software/zookeeper-3.4.8/bin/../conf: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /software/zookeeper-3.4.8/bin/../conf/zoo.cfg</div><div class="line">admin      198   191  0 06:57 pts/0    00:00:00 grep zookeeper</div><div class="line"></div><div class="line"># 使用zkServer.sh status查看zk的状态，发现提示zk并没有运行</div><div class="line">$ ./zkServer.sh status</div><div class="line">bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)</div><div class="line">ZooKeeper JMX enabled by default</div><div class="line">Using config: /software/zookeeper-3.4.8/bin/../conf/zoo.cfg</div><div class="line">Error contacting service. It is probably not running.</div><div class="line"></div><div class="line"># 这个问题是因为zk需要依赖jdk环境，我们需要配置java环境变量，因为Dockerfile中的配置对于我们ssh的admin用户是不生效的，需要单独export一下</div><div class="line">$ export JAVA_HOME=/software/jdk7</div><div class="line"></div><div class="line"># 再次查看就OK了</div><div class="line">$ ./zkServer.sh status</div><div class="line">bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)</div><div class="line">ZooKeeper JMX enabled by default</div><div class="line">Using config: /software/zookeeper-3.4.8/bin/../conf/zoo.cfg</div><div class="line">Mode: standalone</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html" target="_blank" rel="external">http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dockerfile/">Dockerfile</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker命令/">Docker命令</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Zookeeper/">Zookeeper</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Docker/">Docker</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Others/Ubuntu安装shadowsocks" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/08/29/Others/Ubuntu安装shadowsocks/" class="article-date">
  	<time datetime="2016-08-28T19:17:11.000Z" itemprop="datePublished">2016-08-29</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/08/29/Others/Ubuntu安装shadowsocks/">Ubuntu安装Shadowsocks</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="安装Shadowsocks和Supervisor"><a href="#安装Shadowsocks和Supervisor" class="headerlink" title="安装Shadowsocks和Supervisor"></a>安装Shadowsocks和Supervisor</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install python-pip</div><div class="line">sudo pip install --upgrade pip</div><div class="line">sudo pip install shadowsocks</div><div class="line">sudo apt-get install supervisor</div></pre></td></tr></table></figure>
<h3 id="添加Shadowsocks配置文件"><a href="#添加Shadowsocks配置文件" class="headerlink" title="添加Shadowsocks配置文件"></a>添加Shadowsocks配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">$ vi /etc/shadowsocks.json</div><div class="line"></div><div class="line"># 具体配置内容</div><div class="line">&#123;</div><div class="line">    &quot;server&quot;:&quot;0.0.0.0&quot;,</div><div class="line">    &quot;server_port&quot;:443,</div><div class="line">    &quot;local_address&quot;:&quot;127.0.0.1&quot;,</div><div class="line">    &quot;local_port&quot;:1080,</div><div class="line">    &quot;password&quot;:&quot;123456&quot;,</div><div class="line">    &quot;timeout&quot;:500,</div><div class="line">    &quot;method&quot;:&quot;aes-256-cfb&quot;,</div><div class="line">    &quot;fast_open&quot;:false</div><div class="line">&#125;</div><div class="line"></div><div class="line"># 配置内容描述</div><div class="line">server : 服务端监听的地址，服务端可填写 0.0.0.0</div><div class="line">server_port : 服务端的端口</div><div class="line">local_address : 本地端监听的地址</div><div class="line">local_port : 本地端的端口</div><div class="line">password : 用于加密的密码</div><div class="line">timeout : 超时时间，单位秒</div><div class="line">method : 默认&quot;aes-256-cfb&quot;，建议chacha20或者rc4-md5，因为这两个速度快</div><div class="line">fast_open : 是否使用 TCP_FASTOPEN, true / false（后面优化部分会打开系统的 TCP_FASTOPEN，所以这里填 true，否则填 false)</div></pre></td></tr></table></figure>
<h3 id="启动或停止Shadowsocks"><a href="#启动或停止Shadowsocks" class="headerlink" title="启动或停止Shadowsocks"></a>启动或停止Shadowsocks</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ ssserver -c /etc/shadowsocks.json -d start</div><div class="line">$ ssserver -c /etc/shadowsocks.json -d stop</div></pre></td></tr></table></figure>
<h3 id="检查Shadowsocks日志"><a href="#检查Shadowsocks日志" class="headerlink" title="检查Shadowsocks日志"></a>检查Shadowsocks日志</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ tailf /var/log/shadowsocks.log</div></pre></td></tr></table></figure>
<h3 id="将shadowsocks加入开机启动"><a href="#将shadowsocks加入开机启动" class="headerlink" title="将shadowsocks加入开机启动"></a>将shadowsocks加入开机启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo vi /etc/rc.local</div><div class="line">/usr/bin/python /usr/local/bin/ssserver -c /etc/shadowsocks.json -d start</div></pre></td></tr></table></figure>
<h3 id="添加supervisord配置文件"><a href="#添加supervisord配置文件" class="headerlink" title="添加supervisord配置文件"></a>添加supervisord配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ sudo vi /etc/supervisord.conf</div><div class="line"></div><div class="line"># 添加配置</div><div class="line">[program:shadowsocks]</div><div class="line">command=ssserver -c /etc/shadowsocks.json</div><div class="line">autostart=true</div><div class="line">autorestart=true</div><div class="line">user=root</div><div class="line">stderr_logfile=/var/log/supervisor/supervisor.log</div><div class="line">stopsignal=INT</div><div class="line"></div><div class="line">[supervisord]</div></pre></td></tr></table></figure>
<h3 id="启动-supervisord"><a href="#启动-supervisord" class="headerlink" title="启动 supervisord"></a>启动 supervisord</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo supervisord -c /etc/supervisord.conf</div></pre></td></tr></table></figure>
<h3 id="检查supervisor日志"><a href="#检查supervisor日志" class="headerlink" title="检查supervisor日志"></a>检查supervisor日志</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ tailf /var/log/supervisor/supervisor.log</div></pre></td></tr></table></figure>
<h3 id="把-supervisor-加入开机启动进程"><a href="#把-supervisor-加入开机启动进程" class="headerlink" title="把 supervisor 加入开机启动进程"></a>把 supervisor 加入开机启动进程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ vi /etc/rc.local</div><div class="line"></div><div class="line"># 添加配置</div><div class="line">supervisord -c /etc/supervisord.conf</div></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Shadowsocks/">Shadowsocks</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Linux/">Linux</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/7/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/9/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 birdben
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>



<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82900755-1', 'auto');
  ga('send', 'pageview');

</script>
<!-- End Google Analytics -->




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>