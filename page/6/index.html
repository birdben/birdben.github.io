<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>birdben</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="birdben">
<meta property="og:url" content="https://github.com/birdben/page/6/index.html">
<meta property="og:site_name" content="birdben">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="birdben">
  
    <link rel="alternative" href="/atom.xml" title="birdben" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
<script type="text/javascript">
var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1260188951'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s4.cnzz.com/z_stat.php%3Fid%3D1260188951' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/images/logo.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">birdben</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						<li>Links</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/AWK/" style="font-size: 10.83px;">AWK</a> <a href="/tags/Akka/" style="font-size: 10.83px;">Akka</a> <a href="/tags/Dockerfile/" style="font-size: 20px;">Dockerfile</a> <a href="/tags/Docker命令/" style="font-size: 19.17px;">Docker命令</a> <a href="/tags/Docker环境/" style="font-size: 15px;">Docker环境</a> <a href="/tags/ELK/" style="font-size: 16.67px;">ELK</a> <a href="/tags/ElasticSearch/" style="font-size: 10.83px;">ElasticSearch</a> <a href="/tags/Elasticsearch/" style="font-size: 12.5px;">Elasticsearch</a> <a href="/tags/Flume/" style="font-size: 17.5px;">Flume</a> <a href="/tags/Git命令/" style="font-size: 13.33px;">Git命令</a> <a href="/tags/Go/" style="font-size: 14.17px;">Go</a> <a href="/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/tags/HDFS/" style="font-size: 18.33px;">HDFS</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Hadoop原理架构体系/" style="font-size: 13.33px;">Hadoop原理架构体系</a> <a href="/tags/Hive/" style="font-size: 16.67px;">Hive</a> <a href="/tags/JVM/" style="font-size: 11.67px;">JVM</a> <a href="/tags/Java-Web，Socket，Python/" style="font-size: 10px;">Java Web，Socket，Python</a> <a href="/tags/Jenkins环境/" style="font-size: 10px;">Jenkins环境</a> <a href="/tags/Kafka/" style="font-size: 15.83px;">Kafka</a> <a href="/tags/Kibana/" style="font-size: 14.17px;">Kibana</a> <a href="/tags/Linux命令/" style="font-size: 12.5px;">Linux命令</a> <a href="/tags/Logstash/" style="font-size: 15.83px;">Logstash</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/MapReduce/" style="font-size: 11.67px;">MapReduce</a> <a href="/tags/Maven配置/" style="font-size: 11.67px;">Maven配置</a> <a href="/tags/MongoDB/" style="font-size: 11.67px;">MongoDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Nginx/" style="font-size: 10px;">Nginx</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/Shell/" style="font-size: 16.67px;">Shell</a> <a href="/tags/Spring/" style="font-size: 10.83px;">Spring</a> <a href="/tags/Storm/" style="font-size: 12.5px;">Storm</a> <a href="/tags/Zookeeper/" style="font-size: 12.5px;">Zookeeper</a> <a href="/tags/其他/" style="font-size: 10px;">其他</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://blog.csdn.net/birdben">我的CSDN的博客</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">birdben</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="/images/logo.png" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">birdben</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-Logstash/Logstash学习（一）基本用法" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/28/Logstash/Logstash学习（一）基本用法/" class="article-date">
  	<time datetime="2016-10-28T13:31:08.000Z" itemprop="datePublished">2016-10-28</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/28/Logstash/Logstash学习（一）基本用法/">Logstash学习（一）基本用法</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Logstash"><a href="#Logstash" class="headerlink" title="Logstash"></a>Logstash</h3><p>简单介绍一下logstash的配置文件由 input filter output 等几个基本的部分组成，顾名思义 input 就是从哪收集数据，output就是输出到哪，filter代表一个过滤规则意思是什么内容会被收集。Logstash基本上用于收集，解析和存储日志。</p>
<h3 id="Gemfile文件"><a href="#Gemfile文件" class="headerlink" title="Gemfile文件"></a>Gemfile文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"># 指定更新ruby插件的数据源</div><div class="line">source &quot;https://ruby.taobao.org/&quot;</div><div class="line">gem &quot;logstash-core&quot;, &quot;1.5.6&quot;</div><div class="line">gem &quot;file-dependencies&quot;, &quot;0.1.6&quot;</div><div class="line">gem &quot;ci_reporter_rspec&quot;, &quot;1.0.0&quot;, :group =&gt; :development</div><div class="line">gem &quot;simplecov&quot;, :group =&gt; :development</div><div class="line">gem &quot;coveralls&quot;, :group =&gt; :development</div><div class="line">gem &quot;tins&quot;, &quot;1.6&quot;, :group =&gt; :development</div><div class="line">gem &quot;rspec&quot;, &quot;~&gt; 3.1.0&quot;, :group =&gt; :development</div><div class="line">gem &quot;logstash-devutils&quot;, &quot;~&gt; 0&quot;, :group =&gt; :development</div><div class="line">gem &quot;benchmark-ips&quot;, :group =&gt; :development</div><div class="line">gem &quot;octokit&quot;, &quot;3.8.0&quot;, :group =&gt; :build</div><div class="line">gem &quot;stud&quot;, &quot;0.0.21&quot;, :group =&gt; :build</div><div class="line">gem &quot;fpm&quot;, &quot;~&gt; 1.3.3&quot;, :group =&gt; :build</div><div class="line">gem &quot;rubyzip&quot;, &quot;~&gt; 1.1.7&quot;, :group =&gt; :build</div><div class="line">gem &quot;gems&quot;, &quot;~&gt; 0.8.3&quot;, :group =&gt; :build</div><div class="line">gem &quot;flores&quot;, &quot;~&gt; 0.0.6&quot;, :group =&gt; :development</div><div class="line">gem &quot;logstash-input-heartbeat&quot;</div><div class="line">gem &quot;logstash-output-zeromq&quot;</div><div class="line">gem &quot;logstash-codec-collectd&quot;</div><div class="line">gem &quot;logstash-output-xmpp&quot;</div><div class="line">gem &quot;logstash-codec-dots&quot;</div><div class="line">...</div><div class="line">gem &quot;logstash-input-beats&quot;</div></pre></td></tr></table></figure>
<h3 id="下面主要列出了一些常用的插件"><a href="#下面主要列出了一些常用的插件" class="headerlink" title="下面主要列出了一些常用的插件"></a>下面主要列出了一些常用的插件</h3><h3 id="Input-Plugin"><a href="#Input-Plugin" class="headerlink" title="Input Plugin"></a>Input Plugin</h3><p>#### </p>
<p>下面列举出了常用的input插件，*开头的是logstash_1.5默认安装的插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">*logstash-input-beats</div><div class="line">*logstash-input-file</div><div class="line">*logstash-input-http</div><div class="line"> logstash-input-jdbc</div><div class="line"> logstash-input-jmx</div><div class="line">*logstash-input-kafka</div><div class="line">*logstash-input-log4j</div><div class="line">*logstash-input-rabbitmq</div><div class="line">*logstash-input-redis</div><div class="line">*logstash-input-stdin</div><div class="line"> logstash-input-sqlite</div><div class="line">*logstash-input-syslog</div><div class="line">*logstash-input-tcp</div><div class="line"> logstash-input-websocket</div></pre></td></tr></table></figure>
<h3 id="Codec-Plugin"><a href="#Codec-Plugin" class="headerlink" title="Codec Plugin"></a>Codec Plugin</h3><p>下面列举出了常用的codec插件，*开头的是logstash_1.5默认安装的插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">*logstash-codec-collectd</div><div class="line">*logstash-codec-json_lines</div><div class="line">*logstash-codec-json</div><div class="line">*logstash-codec-line</div><div class="line">*logstash-codec-multiline</div><div class="line">*logstash-codec-plain</div><div class="line">*logstash-codec-rubydebug</div></pre></td></tr></table></figure>
<h3 id="Filter-Plugin"><a href="#Filter-Plugin" class="headerlink" title="Filter Plugin"></a>Filter Plugin</h3><p>下面列举出了常用的filter插件，*开头的是logstash_1.5默认安装的插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">*logstash-filter-date</div><div class="line">*logstash-filter-drop</div><div class="line">*logstash-filter-geoip</div><div class="line">*logstash-filter-grok</div><div class="line"> logstash-filter-i18n</div><div class="line">*logstash-filter-json</div><div class="line"> logstash-filter-json_encode</div><div class="line">*logstash-filter-kv</div><div class="line">*logstash-filter-mutate</div><div class="line">*logstash-filter-metrics</div><div class="line">*logstash-filter-multiline</div><div class="line">*logstash-filter-ruby</div><div class="line"> logstash-filter-range</div><div class="line">*logstash-filter-split</div><div class="line">*logstash-filter-uuid</div><div class="line">*logstash-filter-xml</div></pre></td></tr></table></figure>
<h3 id="Output-Plugin"><a href="#Output-Plugin" class="headerlink" title="Output Plugin"></a>Output Plugin</h3><p>下面列举出了常用的output插件，*开头的是logstash_1.5默认安装的插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">*logstash-output-elasticsearch</div><div class="line">*logstash-output-file</div><div class="line">*logstash-output-http</div><div class="line">*logstash-output-kafka</div><div class="line"> logstash-output-mongodb</div><div class="line">*logstash-output-rabbitmq</div><div class="line">*logstash-output-redis</div><div class="line"> logstash-output-solr_http</div><div class="line"> logstash-output-syslog</div><div class="line">*logstash-output-stdout</div><div class="line">*logstash-output-tcp</div><div class="line"> logstash-output-websocket</div><div class="line"> logstash-output-zabbix</div><div class="line">*logstash-output-zeromq</div></pre></td></tr></table></figure>
<h2 id="logstash-input-file插件的用法"><a href="#logstash-input-file插件的用法" class="headerlink" title="logstash-input-file插件的用法"></a>logstash-input-file插件的用法</h2><h4 id="start-position用法"><a href="#start-position用法" class="headerlink" title="start_position用法"></a>start_position用法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># start_position是监听的位置，默认是end，即一个文件如果没有记录它的读取信息，则从文件的末尾开始读取，也就是说，仅仅读取新添加的内容。对于一些更新的日志类型的监听，通常直接使用end就可以了；相反，beginning就会从一个文件的头开始读取。但是如果记录过文件的读取信息，这个配置也就失去作用了。</div><div class="line">start_position =&gt; &quot;beginning&quot;</div></pre></td></tr></table></figure>
<h4 id="sincedb用法"><a href="#sincedb用法" class="headerlink" title="sincedb用法"></a>sincedb用法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># sincedb文件使用来保存logstash读取日志文件的进度的</div><div class="line"># 默认存储在home路径下.sincedb_c9a33fda01005ad7430b6ef4a0d51f8b，可以设置sincedb_path指定该文件的路径</div><div class="line"># c9a33fda01005ad7430b6ef4a0d51f8b是log文件路径&quot;/Users/ben/Downloads/command.log&quot;做MD5后的值</div></pre></td></tr></table></figure>
<h2 id="logstash-filter-grok插件的用法"><a href="#logstash-filter-grok插件的用法" class="headerlink" title="logstash-filter-grok插件的用法"></a>logstash-filter-grok插件的用法</h2><h4 id="grok使用自定义正则表达式"><a href="#grok使用自定义正则表达式" class="headerlink" title="grok使用自定义正则表达式"></a>grok使用自定义正则表达式</h4><h4 id="LOGSTASH-HOME-patterns-postfix"><a href="#LOGSTASH-HOME-patterns-postfix" class="headerlink" title="${LOGSTASH_HOME}/patterns/postfix"></a>${LOGSTASH_HOME}/patterns/postfix</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">BDP_LOGMESSAGE %&#123;DATA:logInfo.startTimestamp&#125;\|%&#123;DATA:logInfo.endTimestamp&#125;\|%&#123;INT:logInfo.cost&#125;\|%&#123;DATA:logInfo.userID&#125;\|%&#123;DATA:logInfo.userName&#125;\|%&#123;DATA:logInfo.departmentName&#125;\|%&#123;DATA:logInfo.module&#125;\|%&#123;DATA:logInfo.function&#125;\|%&#123;DATA:logInfo.op&#125;\|%&#123;DATA:logInfo.status&#125;\|%&#123;DATA:logInfo.message&#125;\|%&#123;DATA:logInfo.target&#125;\|%&#123;DATA:logInfo.targetDetail&#125;\|</div></pre></td></tr></table></figure>
<h4 id="logstash-conf文件中的filter需要指定自定义grok表达式的文件路径"><a href="#logstash-conf文件中的filter需要指定自定义grok表达式的文件路径" class="headerlink" title="logstash.conf文件中的filter需要指定自定义grok表达式的文件路径"></a>logstash.conf文件中的filter需要指定自定义grok表达式的文件路径</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">filter &#123;</div><div class="line">	# 指定自定义grok正则表达式文件的路径</div><div class="line">	patterns_dir =&gt; &quot;./patterns&quot;</div><div class="line">	# 使用了自定义的BDP_LOGMESSAGE表达式去匹配message字段，将message中匹配BDP_LOGMESSAG表达式的内容拆分成指定的字段</div><div class="line">	grok &#123;</div><div class="line">		match =&gt; &#123;</div><div class="line">			&quot;message&quot; =&gt; &quot;%&#123;TIMESTAMP_ISO8601:date&#125; %&#123;LOGLEVEL:level&#125;  \[%&#123;WORD:priotiy&#125;\] \- %&#123;BDP_LOGMESSAGE&#125;&quot;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">filter &#123;</div><div class="line">  if [type] == &quot;tomcatlog&quot; &#123;</div><div class="line">     multiline &#123;</div><div class="line">       pattern =&gt; &quot;^%&#123;TIMESTAMP_ISO8601&#125;&quot;</div><div class="line">       negate =&gt; true</div><div class="line">       what=&gt; &quot;previous&quot;</div><div class="line">     &#125;</div><div class="line">     if &quot;_grokparsefailure&quot; in [tags] &#123;</div><div class="line">       drop &#123; &#125;</div><div class="line">     &#125;</div><div class="line">     grok &#123;</div><div class="line">       match =&gt; &#123; &quot;message&quot; =&gt;</div><div class="line">         &quot;%&#123;TIMESTAMP_ISO8601:date&#125; \[(?&lt;thread_name&gt;.+?)\] (?&lt;log_level&gt;\w+)\s*(?&lt;content&gt;.*)&quot;</div><div class="line">            &#125;</div><div class="line">     &#125; </div><div class="line">     </div><div class="line">     date &#123;</div><div class="line">       match =&gt; [ &quot;timestamp&quot;, &quot;yyyy-MM-dd HH:mm:ss,SSS Z&quot;, &quot;MMM dd, yyyy HH:mm:ss a&quot; ]</div><div class="line">     &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logstash/">Logstash</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Shell/Shell脚本学习（七）Shell中的特殊用法" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/22/Shell/Shell脚本学习（七）Shell中的特殊用法/" class="article-date">
  	<time datetime="2016-10-22T09:53:17.000Z" itemprop="datePublished">2016-10-22</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/22/Shell/Shell脚本学习（七）Shell中的特殊用法/">Shell脚本学习（七）Shell中的特殊用法</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近在网上看了别人写的Shell脚本，发现还是有很多语法看不懂需要百度才行，今天就总结一下我遇到的一些Shell特殊符号的用法问题</p>
<h3 id="Shell的特殊符号-amp-amp-amp-的用法"><a href="#Shell的特殊符号-amp-amp-amp-的用法" class="headerlink" title="Shell的特殊符号 $, $$, &amp;, &amp;&amp; 的用法"></a>Shell的特殊符号 $, $$, &amp;, &amp;&amp; 的用法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">$$ : Shell本身的PID（ProcessID）</div><div class="line">$! : Shell最后运行的后台Process的PID</div><div class="line">$? : 最后运行的命令的结束代码（返回值）</div><div class="line">$- : 使用Set命令设定的Flag一览，显示Shell使用的当前选项。</div><div class="line">$* : Shell的所有参数列表</div><div class="line">$@ : Shell的所有参数列表</div><div class="line">$# : Shell的所有参数个数</div><div class="line">$0 : Shell本身的文件名</div><div class="line">$1～$n : Shell的各个参数值。$1是第1参数、$2是第2参数…</div><div class="line">` : 反引号。反引号括起来的字符串被shell解释为命令行，在执行时，shell首先执行该命令行，并以它的标准输出结果取代整个反引号（包括两个反引号）部分。即`command`和$(command)的含义相同，都返回当前执行命令的结果。</div><div class="line">&amp; : 放在启动参数后面表示设置此进程为后台进程</div><div class="line">| : 管道 (pipeline) 连结上个指令的标准输出，做为下个指令的标准输入</div><div class="line">&amp;&amp; : Shell命令之间使用 &amp;&amp; 连接，实现逻辑与的功能</div><div class="line">|| : Shell命令之间使用 || 连接，实现逻辑或的功能</div><div class="line">1.命令之间使用 &amp;&amp; 连接，实现逻辑与的功能。</div><div class="line">2.如果左边的命令有返回值，该返回值保存在Shell变量 $? 中，只有在 &amp;&amp; 左边的命令返回真（命令返回值 $? == 0），&amp;&amp; 右边的命令才会被执行。</div><div class="line">3.只要有一个命令返回假（命令返回值 $? == 1），表示左边的命令执行失败，后面的命令就不会被执行。</div><div class="line">下一条命令依赖前一条命令是否执行成功。如：在成功地执行一条命令之后再执行另一条命令，或者在一条命令执行失败后再执行另一条命令等。shell 提供了 &amp;&amp; 和 || 来实现命令执行控制的功能，shell 将根据 &amp;&amp; 或 || 前面命令的返回值来控制其后面命令的执行。</div></pre></td></tr></table></figure>
<p>举例说明上述的Shell特殊符号的用法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line"></div><div class="line">#</div><div class="line"># AUTHOR: Yanpeng Lin</div><div class="line"># DATE:   Mar 30 2014</div><div class="line"># DESC:   lock a rotating file(static filename) and tail</div><div class="line">#</div><div class="line"></div><div class="line">PID=$( mktemp )</div><div class="line">echo $PID</div><div class="line">echo $(eval &quot;cat $PID&quot;)</div><div class="line">while true;</div><div class="line">do</div><div class="line">    CURRENT_TARGET=$( eval &quot;echo $1&quot; )</div><div class="line">    echo $CURRENT_TARGET</div><div class="line">    if [ -e $&#123;CURRENT_TARGET&#125; ]; then</div><div class="line">        IO=`stat $&#123;CURRENT_TARGET&#125;`</div><div class="line">        # 在后台运行监听&#123;$CURRENT_TARGET&#125;文件的变化，如果出错不输出错误信息，将最后执行的后台进程的ID输出到$&#123;PID&#125;中（也就是tail -f &#123;$CURRENT_TARGET&#125; 2&gt; /dev/null &amp;这个命令的后台进程ID）</div><div class="line">        tail -f &#123;$CURRENT_TARGET&#125; 2&gt; /dev/null &amp; echo $! &gt; $PID;</div><div class="line">        echo $!</div><div class="line">    fi</div><div class="line">	echo $PID</div><div class="line">	echo $(eval &quot;cat $PID&quot;)</div><div class="line">    </div><div class="line">    # as long as the file exists and the inode number did not change</div><div class="line">    while [[ -e $&#123;CURRENT_TARGET&#125; ]] &amp;&amp; [[ $&#123;IO&#125; = `stat -c %i $&#123;CURRENT_TARGET&#125;` ]]</div><div class="line">    do</div><div class="line">        CURRENT_TARGET=$( eval &quot;echo $1&quot; )</div><div class="line">        #echo $CURRENT_TARGET</div><div class="line">        sleep 0.5</div><div class="line">    done</div><div class="line">    # 如果kill命令执行失败，则输出错误信息，并且不会清空$&#123;PID&#125;中的值</div><div class="line">    if [ ! -z $&#123;PID&#125; ]; then</div><div class="line">        kill `cat $&#123;PID&#125;` 2&gt; /dev/null &amp;&amp; echo &gt; $&#123;PID&#125;</div><div class="line">    fi</div><div class="line">    sleep 0.5</div><div class="line">done 2&gt; /dev/null</div><div class="line">rm -rf $&#123;PID&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$&#123;var:-default&#125; : 使用一个默认值（一般是空值）来代替那些空的或者没有赋值的变量var</div><div class="line">$&#123;var:=default&#125; : 使用指定值来代替空的或者没有赋值的变量var</div><div class="line">$&#123;var:?message&#125; : 如果变量为空或者未赋值，那么会显示出错误信息并终止脚本的执行同时返回退出码1</div><div class="line">$&#123;#var&#125; : 给出var的长度</div><div class="line">$&#123;var%pattern&#125; : 表示从var最右边(即结尾)开始删除与pattern匹配的最小部分，然后返回剩余部分</div><div class="line">$&#123;var%%pattern&#125; : 表示从var最右边(即结尾)开始删除与pattern匹配的最长部分，然后返回剩余部分</div><div class="line">$&#123;var#pattern&#125; : 表示从var最左边(即开始)开始删除与pattern匹配的最小部分，然后返回剩余部分</div><div class="line">$&#123;var##pattern&#125; : 表示从var最左边(即开始)开始删除与pattern匹配的最长部分，然后返回剩余部分</div><div class="line"></div><div class="line">注意：只有在pattern中使用了通配符才能有最长最短的匹配，否则没有最长最短匹配之分。</div><div class="line"></div><div class="line">例如：</div><div class="line">$&#123;1#-&#125; : $&#123;1#-&#125;是判断第一个参数是否以&quot;-&quot;开头</div><div class="line">$&#123;1%.conf&#125; : $&#123;1%.conf&#125;是判断第一个参数是否以&quot;.conf&quot;结尾</div></pre></td></tr></table></figure>
<p>test.sh脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">echo $&#123;test_path:-&#125;</div><div class="line">echo $&#123;test_path:=test&#125;</div><div class="line"># 运行会报错error_message</div><div class="line"># echo $&#123;test_path1:?error_message&#125;</div><div class="line">echo $&#123;#test_path&#125;</div><div class="line"></div><div class="line">test_pattern=&quot;stxxa_styya_stzzd&quot;</div><div class="line">echo $&#123;test_pattern#st*a&#125;</div><div class="line">echo $&#123;test_pattern##st*a&#125;</div><div class="line"></div><div class="line">test_pattern=&quot;ccc_aaab.conf_ab.conf&quot;</div><div class="line">echo $&#123;test_pattern%a*.conf&#125;</div><div class="line">echo $&#123;test_pattern%%a*.conf&#125;</div></pre></td></tr></table></figure>
<p>运行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">（空）</div><div class="line">test</div><div class="line">4</div><div class="line">_styya_stzzd</div><div class="line">_stzzd</div><div class="line">ccc_aaab.conf_</div><div class="line">ccc_</div></pre></td></tr></table></figure>
<p>再看一个DockerHub上的Redis官方的Dockerfile和Shell脚本</p>
<p>Dockerfile文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line">FROM debian:jessie</div><div class="line"></div><div class="line"># add our user and group first to make sure their IDs get assigned consistently, regardless of whatever dependencies get added</div><div class="line">RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis</div><div class="line"></div><div class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</div><div class="line">        ca-certificates \</div><div class="line">        wget \</div><div class="line">    &amp;&amp; rm -rf /var/lib/apt/lists/*</div><div class="line"></div><div class="line"># grab gosu for easy step-down from root</div><div class="line">ENV GOSU_VERSION 1.7</div><div class="line">RUN set -x \</div><div class="line">    &amp;&amp; wget -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)&quot; \</div><div class="line">    &amp;&amp; wget -O /usr/local/bin/gosu.asc &quot;https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture).asc&quot; \</div><div class="line">    &amp;&amp; export GNUPGHOME=&quot;$(mktemp -d)&quot; \</div><div class="line">    &amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4 \</div><div class="line">    &amp;&amp; gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu \</div><div class="line">    &amp;&amp; rm -r &quot;$GNUPGHOME&quot; /usr/local/bin/gosu.asc \</div><div class="line">    &amp;&amp; chmod +x /usr/local/bin/gosu \</div><div class="line">    &amp;&amp; gosu nobody true</div><div class="line"></div><div class="line">ENV REDIS_VERSION 3.2.8</div><div class="line">ENV REDIS_DOWNLOAD_URL http://download.redis.io/releases/redis-3.2.8.tar.gz</div><div class="line">ENV REDIS_DOWNLOAD_SHA1 6780d1abb66f33a97aad0edbe020403d0a15b67f</div><div class="line"></div><div class="line"># for redis-sentinel see: http://redis.io/topics/sentinel</div><div class="line">RUN set -ex \</div><div class="line">    \</div><div class="line">    &amp;&amp; buildDeps=&apos; \</div><div class="line">        gcc \</div><div class="line">        libc6-dev \</div><div class="line">        make \</div><div class="line">    &apos; \</div><div class="line">    &amp;&amp; apt-get update \</div><div class="line">    &amp;&amp; apt-get install -y $buildDeps --no-install-recommends \</div><div class="line">    &amp;&amp; rm -rf /var/lib/apt/lists/* \</div><div class="line">    \</div><div class="line">    &amp;&amp; wget -O redis.tar.gz &quot;$REDIS_DOWNLOAD_URL&quot; \</div><div class="line">    &amp;&amp; echo &quot;$REDIS_DOWNLOAD_SHA1 *redis.tar.gz&quot; | sha1sum -c - \</div><div class="line">    &amp;&amp; mkdir -p /usr/src/redis \</div><div class="line">    &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \</div><div class="line">    &amp;&amp; rm redis.tar.gz \</div><div class="line">    &amp;&amp; grep -q &apos;^#define CONFIG_DEFAULT_PROTECTED_MODE 1$&apos; /usr/src/redis/src/server.h \</div><div class="line">    &amp;&amp; sed -ri &apos;s!^(#define CONFIG_DEFAULT_PROTECTED_MODE) 1$!\1 0!&apos; /usr/src/redis/src/server.h \</div><div class="line">    &amp;&amp; grep -q &apos;^#define CONFIG_DEFAULT_PROTECTED_MODE 0$&apos; /usr/src/redis/src/server.h \</div><div class="line">    &amp;&amp; make -C /usr/src/redis \</div><div class="line">    &amp;&amp; make -C /usr/src/redis install \</div><div class="line">    \</div><div class="line">    &amp;&amp; rm -r /usr/src/redis \</div><div class="line">    \</div><div class="line">    &amp;&amp; apt-get purge -y --auto-remove $buildDeps</div><div class="line"></div><div class="line">RUN mkdir /data &amp;&amp; chown redis:redis /data</div><div class="line">VOLUME /data</div><div class="line">WORKDIR /data</div><div class="line"></div><div class="line">COPY docker-entrypoint.sh /usr/local/bin/</div><div class="line">ENTRYPOINT [&quot;docker-entrypoint.sh&quot;]</div><div class="line"></div><div class="line">EXPOSE 6379</div><div class="line">CMD [ &quot;redis-server&quot; ]</div></pre></td></tr></table></figure>
<p>docker-entrypoint.sh脚本文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#!/bin/sh</div><div class="line">set -e</div><div class="line"></div><div class="line"># first arg is `-f` or `--some-option`</div><div class="line"># or first arg is `something.conf`</div><div class="line">if [ &quot;$&#123;1#-&#125;&quot; != &quot;$1&quot; ] || [ &quot;$&#123;1%.conf&#125;&quot; != &quot;$1&quot; ]; then</div><div class="line">    set -- redis-server &quot;$@&quot;</div><div class="line">fi</div><div class="line"></div><div class="line"># allow the container to be started with `--user`</div><div class="line">if [ &quot;$1&quot; = &apos;redis-server&apos; -a &quot;$(id -u)&quot; = &apos;0&apos; ]; then</div><div class="line">    chown -R redis .</div><div class="line">    exec gosu redis &quot;$0&quot; &quot;$@&quot;</div><div class="line">fi</div><div class="line"></div><div class="line">exec &quot;$@&quot;</div></pre></td></tr></table></figure>
<p>这里Redis的Docker容器使用ENTRYPOINT的方式启动，ENTRYPOINT配置容器启动后执行的命令，构建镜像build时不执行，并且不可被 docker run 提供的参数覆盖。这里docker run的参数”redis-server”会添加到ENTRYPOINT后面，就成了这样docker-entrypoint.sh “redis-server”。相当于下面的两条命令的效果是一样，因为默认的CMD参数是”redis-server”。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># redis:3.2是Redis的Docker镜像名称</div><div class="line"></div><div class="line">$ docker run -p 6379:6379 -d redis:3.2</div><div class="line">$ docker run -p 6379:6379 -d redis:3.2 redis-server</div></pre></td></tr></table></figure>
<p>当然我们也可以指定额外的启动参数，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 这里我们启动的时候指定了挂载的配置文件</div><div class="line">$ docker run -v /Users/yunyu/Downloads/redis:/data -p 6379:6379 -d redis:3.2 redis-server /data/conf/redis.conf</div></pre></td></tr></table></figure>
<p>具体在看一下docker-entrypoint.sh脚本的实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 这里是$&#123;1#-&#125;是判断第一个参数是否以&quot;-&quot;开头，$&#123;1%.conf&#125;是判断第一个参数是否以&quot;.conf&quot;结尾</div><div class="line">if [ &quot;$&#123;1#-&#125;&quot; != &quot;$1&quot; ] || [ &quot;$&#123;1%.conf&#125;&quot; != &quot;$1&quot; ]; then</div><div class="line">    # &quot;set -- redis-server&quot;的意思是把&quot;redis-server&quot;加入到原来的参数列表中，并且放在参数列表中的第一个，可以通过&quot;$1&quot;获取，其他参数获取索引顺延</div><div class="line">    # 这里把&quot;redis-server&quot;加入到原来的参数列表并且放在一个位置，是因为如果docker run传递了参数，而且第一个参数是以&quot;-&quot;开始，就说明没有redis-server参数，需要添加到参数列表的第一参数位置。</div><div class="line">    set -- redis-server &quot;$@&quot;</div><div class="line">fi</div><div class="line"></div><div class="line">...</div><div class="line"></div><div class="line"># 这里是把所有参数列表组成命令给执行器，执行该命令</div><div class="line"># 也就是如果参数中没有redis-server，表示用户希望运行自己的其他进程</div><div class="line">exec &quot;$@&quot;</div></pre></td></tr></table></figure>
<p>通过下面的小例子来体会”set”和”set –”区别，”set –”可以将后面参数的”-“转义，不当成命令选项来解析，当成一个普通参数来解析。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ set -- -z 2 3 4</div><div class="line">$ echo $1</div><div class="line">-z</div><div class="line"></div><div class="line">$ set -z 2 3 4</div><div class="line">set: bad option: -z</div></pre></td></tr></table></figure>
<p>源文件连接地址：</p>
<ul>
<li><a href="https://github.com/docker-library/redis/blob/3f926a47370a19fc88d57d0245823758cbf19b2d/3.2/Dockerfile">https://github.com/docker-library/redis/blob/3f926a47370a19fc88d57d0245823758cbf19b2d/3.2/Dockerfile</a></li>
<li><a href="https://github.com/docker-library/redis/blob/3f926a47370a19fc88d57d0245823758cbf19b2d/3.2/docker-entrypoint.sh">https://github.com/docker-library/redis/blob/3f926a47370a19fc88d57d0245823758cbf19b2d/3.2/docker-entrypoint.sh</a></li>
</ul>
<p>参考文章：</p>
<ul>
<li><a href="http://unix.stackexchange.com/questions/308260/what-does-set-do-in-this-dockerfile-entrypoint" target="_blank" rel="external">http://unix.stackexchange.com/questions/308260/what-does-set-do-in-this-dockerfile-entrypoint</a></li>
<li><a href="http://www.360doc.com/content/13/0524/12/7044580_287728900.shtml" target="_blank" rel="external">http://www.360doc.com/content/13/0524/12/7044580_287728900.shtml</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Shell/">Shell</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Shell/">Shell</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hive/Hive学习（五）Hive外部表使用Partitions（译文）" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/18/Hive/Hive学习（五）Hive外部表使用Partitions（译文）/" class="article-date">
  	<time datetime="2016-10-18T12:36:31.000Z" itemprop="datePublished">2016-10-18</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/18/Hive/Hive学习（五）Hive外部表使用Partitions（译文）/">Hive学习（五）Hive外部表使用Partitions（译文）</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h5 id="普通的Hive表"><a href="#普通的Hive表" class="headerlink" title="普通的Hive表"></a>普通的Hive表</h5><p>可以用下面的script创建普通的Hive表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE user (</div><div class="line">  userId BIGINT,</div><div class="line">  type INT,</div><div class="line">  level TINYINT,</div><div class="line">  date String</div><div class="line">)</div><div class="line">COMMENT &apos;User Infomation&apos;</div></pre></td></tr></table></figure>
<p>这个表是没有数据的，直到我们load数据之前这个表是没什么用的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LOAD INPATH &apos;/user/chris/data/testdata&apos; OVERWRITE INTO TABLE user</div></pre></td></tr></table></figure>
<p>默认情况下，当数据文件被加载，/user/${USER}/warehouse/user 会被自动创建。</p>
<p>对我来说，目录是 /user/chris/warehouse/user ，user是表名，user表的数据文件都被定位到这个目录下。</p>
<p>现在，我们可以随意执行SQL来分析数据了。</p>
<h5 id="假如"><a href="#假如" class="headerlink" title="假如"></a>假如</h5><p>假如我们想要通过ETL程序处理这些数据，并且加载结果数据到Hive中，但是我们不想手工加载这些结果数据。</p>
<p>假如这些数据不仅仅是被Hive使用，还有一些其他应用程序也使用，可能还会被MapReduce处理。</p>
<p>External Table外部表就是来拯救我们的，通过下面的语法来创建外置表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">CREATE EXTERNAL TABLE user (</div><div class="line">  userId BIGINT,</div><div class="line">  type INT,</div><div class="line">  level TINYINT,</div><div class="line">  date String</div><div class="line">)</div><div class="line">COMMENT &apos;User Infomation&apos;</div><div class="line">LOCATION &apos;/user/chris/datastore/user/&apos;;</div></pre></td></tr></table></figure>
<p>Location配置是设置我们要将数据文件存储的位置，目录的名称必须和表名一样（就像Hive的普通表一样）。在这个例子中，表名就是user。</p>
<p>然后，我们可以导入任何符合user表声明的pattern表达式的数据文件到user目录下。</p>
<p>所有的数据都可以被Hive SQL立即访问。</p>
<h5 id="不够理想的地方"><a href="#不够理想的地方" class="headerlink" title="不够理想的地方"></a>不够理想的地方</h5><p>当数据文件变大（数量和大小），我们可能需要用Partition分区来优化数据处理的效率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE user (</div><div class="line">  userId BIGINT,</div><div class="line">  type INT,</div><div class="line">  level TINYINT,</div><div class="line">)</div><div class="line">COMMENT &apos;User Infomation&apos;</div><div class="line">PARTITIONED BY (date String)</div></pre></td></tr></table></figure>
<p>date String 被移动到 PARTITIONED BY，当我们需要加载数据到Hive时，partition一定要被分配。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LOAD INPATH &apos;/user/chris/data/testdata&apos; OVERWRITE INTO TABLE user PARTITION (date=&apos;2012-02-22&apos;)</div></pre></td></tr></table></figure>
<p>当数据加载完之后，我们可以看到一个名称是date=2010-02-22的新目录被创建在 /user/chris/warehouse/user/ 下。</p>
<p>所以，我们要如何使用External Table的Partition来优化数据处理呢？</p>
<p>和之前一样，首先要创建外部表user，并且分配好Location。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">CREATE EXTERNAL TABLE user (</div><div class="line">  userId BIGINT,</div><div class="line">  type INT,</div><div class="line">  level TINYINT,</div><div class="line">  date String</div><div class="line">)</div><div class="line">COMMENT &apos;User Infomation&apos;</div><div class="line">PARTITIONED BY (date String)</div><div class="line">LOCATION &apos;/user/chris/datastore/user/&apos;;</div></pre></td></tr></table></figure>
<p>然后，在 /user/chris/datastore/user/ 下创建目录date=2010-02-22</p>
<p>最后，把date是2010-02-22数据文件存储在这个目录下，完成。</p>
<p>但是，</p>
<p>当我们执行select * from user;没有任何结果数据。</p>
<p>为什么呢？</p>
<p>我花了很长时间搜寻答案。</p>
<p>最终，解决了。</p>
<p>因为当外部表被创建，Metastore包含Hive元数据信息，Hive元数据中外置表的默认表路径是被更改到指定的Location，但是关于partition，不做任何更改，所以我们必须手工添加这些元数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ALTER TABLE user ADD PARTITION(date=&apos;2010-02-22&apos;);</div></pre></td></tr></table></figure>
<p>每次有一个新的 date=… 目录（partition）被创建，我们都必须手工alter table来添加partition信息。</p>
<p>这个真的不是很好的方式！</p>
<p>但是幸运的是，我们有Hive JDBC/Thrift, 我们可以使用 <a href="https://github.com/don9z/hadoop-tools/blob/master/hive/addpartition.py">script</a> 脚本来做这些。</p>
<p>原文链接：</p>
<ul>
<li><a href="http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/" target="_blank" rel="external">http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hive/Hive学习（四）Hive内部表和外部表" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/18/Hive/Hive学习（四）Hive内部表和外部表/" class="article-date">
  	<time datetime="2016-10-18T06:39:56.000Z" itemprop="datePublished">2016-10-18</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/18/Hive/Hive学习（四）Hive内部表和外部表/">Hive学习（四）Hive内部表和外部表</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上一篇我们介绍了Hive导入数据的两种方式，本篇我们对Hive的表进行重点介绍。上一篇我们使用的都是Hive的内部表，如何区分Hive的内部表和外部表呢？create （external） table语句是否带有external关键字，如果带有external关键字就是外部表，所以上一篇我们导入的数据都是导入到Hive的内部表，也就是文件都存储在/hive/warehouse的HDFS目录中，即Hive默认配置的数据仓库。External Table允许我们将文件保存在任意的HDFS目录下，下面将详细介绍内部表和外部表的区别。</p>
<h3 id="Hive内部表"><a href="#Hive内部表" class="headerlink" title="Hive内部表"></a>Hive内部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># 创建内部表test_internal_table，这里创建好的表的数据文件是默认存储在/hive/warehouse目录下，全路径是/hive/warehouse/test_hdfs.db/test_internal_table</div><div class="line"># test_hdfs是我们的数据库</div><div class="line"># 如果删除test_internal_table，元数据表结构和数据文件都将会被删除</div><div class="line">CREATE TABLE IF NOT EXISTS test_internal_table(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE;</div></pre></td></tr></table></figure>
<h3 id="Hive外部表"><a href="#Hive外部表" class="headerlink" title="Hive外部表"></a>Hive外部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 创建外部表test_external_table，这里创建好的表是读取的Location属性指定文件目录下的数据文件，而不是默认的/hive/warehouse下，这样我们就可以使用External Table结合外部的Application使用（这里读取的是Flume采集并写入HDFS的数据文件），Hive同样可以读取Hive默认配置的数据仓库之外的HDFS目录下的数据文件。</div><div class="line"># Location是指定的数据文件路径</div><div class="line"># 如果删除test_external_table，元数据表结构会被删除，但是数据文件不会被删除</div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS test_external_table(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.view_ad&apos;;</div></pre></td></tr></table></figure>
<p>最后总结一下Hive内部表与外部表的区别：</p>
<ul>
<li>在导入数据时，导入到内部表，数据文件是存储在Hive的默认的数据仓库下的。导入到外部表，数据文件是存储在External Table指定的Location目录下的。</li>
<li>在删除内部表时，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时，Hive仅仅删除外部表的元数据，数据是不会删除的。</li>
</ul>
<p>如何选择使用哪种表呢？</p>
<ul>
<li>如果所有的数据处理都需要由Hive完成，那么建议你应该使用内部表，如果所有的数据处理需要整合其他Application一起应用（例如：Flume负责采集数据文件，并且根据Header写入到HDFS的不同目录下的数据文件），此时建议使用外部表。</li>
</ul>
<p>原文链接：</p>
<ul>
<li><a href="http://blog.csdn.net/yeruby/article/details/23033273" target="_blank" rel="external">http://blog.csdn.net/yeruby/article/details/23033273</a></li>
<li><a href="http://www.aboutyun.com/thread-7458-1-1.html" target="_blank" rel="external">http://www.aboutyun.com/thread-7458-1-1.html</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hive/Hive学习（三）Hive导入数据的几种方式" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/18/Hive/Hive学习（三）Hive导入数据的几种方式/" class="article-date">
  	<time datetime="2016-10-18T03:59:02.000Z" itemprop="datePublished">2016-10-18</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/18/Hive/Hive学习（三）Hive导入数据的几种方式/">Hive学习（三）Hive导入数据的几种方式</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Hive导入数据的几种方式"><a href="#Hive导入数据的几种方式" class="headerlink" title="Hive导入数据的几种方式"></a>Hive导入数据的几种方式</h3><ul>
<li>从本地文件系统中导入数据到Hive表</li>
<li>从HDFS中导入数据到Hive表</li>
</ul>
<p>上面的两种方式都是使用Hive的load语句导入数据的，具体格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</div></pre></td></tr></table></figure>
<ul>
<li><p>如果使用了LOCAL关键字，则会在本地文件系统中寻找filepath，如果filepath是相对路径，则该路径会被解释为相对于用户的当前工作目录，用户也可以指定为本地文件指定完整URI，例如：file:///data/track.log，或者直接写为/data/track.log。Load语句将会复制由filepath指定的所有文件到目标文件系统（目标文件系统由表的location属性推断得出），然后移动文件到表中。</p>
</li>
<li><p>如果未使用LOCAL关键字，filepath必须指的是与目标表的location文件系统相同的文件系统上的文件（例如：HDFS文件系统）。这里Load的本质实际就是一个HDFS目录下的数据文件转移到另一个HDFS目录下的操作。</p>
</li>
</ul>
<p>当然还有其他的Hive导入数据的方式，但这里我们重点介绍这两种，其他的导入数据方式可以参考：<a href="https://www.iteblog.com/archives/949" target="_blank" rel="external">https://www.iteblog.com/archives/949</a></p>
<p>下面我们将具体举例分析上面两种Hive导入数据的方式，下面是我们要分析的日志文件track.log的内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div></pre></td></tr></table></figure>
<h4 id="从本地文件系统中导入数据到Hive表"><a href="#从本地文件系统中导入数据到Hive表" class="headerlink" title="从本地文件系统中导入数据到Hive表"></a>从本地文件系统中导入数据到Hive表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># 查看需要导入Hive的track.log文件内容</div><div class="line">$ cat /data/track.log&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div><div class="line"></div><div class="line"># 创建表test_local_table</div><div class="line">hive&gt; CREATE TABLE IF NOT EXISTS test_local_table(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE;</div><div class="line"></div><div class="line"># 从本地文件系统中导入数据到Hive表</div><div class="line">hive&gt; load data local inpath &apos;/data/track.log&apos; into table test_local_table partition (dt=&apos;2016-10-18&apos;);</div><div class="line"></div><div class="line"># 导入完成之后，查询test_local_table表中的数据</div><div class="line">hive&gt; select * from test_local_table;OK[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	2016-10-18Time taken: 0.106 seconds, Fetched: 1 row(s)</div><div class="line"></div><div class="line"># 在HDFS的/hive/warehouse目录中查看track.log文件，这就是我们将本地系统文件导入到Hive之后，存储在HDFS的路径</div><div class="line"># test_hdfs.db是我们的数据库</div><div class="line"># test_local_table是我们创建的表</div><div class="line"># dt=2016-10-18是我们创建的Partition</div><div class="line">$ hdfs dfs -ls /hive/warehouse/test_hdfs.db/test_local_table/dt=2016-10-18Found 1 items-rwxr-xr-x   2 yunyu supergroup        268 2016-10-17 21:19 /hive/warehouse/test_hdfs.db/test_local_table/dt=2016-10-18/track.log</div><div class="line"></div><div class="line"># 查看文件内容</div><div class="line">$ hdfs dfs -cat /hive/warehouse/test_hdfs.db/test_local_table/dt=2016-10-18/track.log&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div></pre></td></tr></table></figure>
<h4 id="从HDFS中导入数据到Hive表"><a href="#从HDFS中导入数据到Hive表" class="headerlink" title="从HDFS中导入数据到Hive表"></a>从HDFS中导入数据到Hive表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># 在HDFS中查看要导入到Hive的文件（这里我们使用之前Flume收集到HDFS的track.log的日志文件）</div><div class="line">$ hdfs dfs -ls /flume/events/birdben.ad.click_ad/201610/</div><div class="line">Found 1 items-rw-r--r--   2 yunyu supergroup       6776 2016-10-13 06:18 /flume/events/birdben.ad.click_ad/201610/events-.1476364421957</div><div class="line"></div><div class="line"># 创建表test_partition_table</div><div class="line">hive&gt; CREATE TABLE IF NOT EXISTS test_partition_table(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE;</div><div class="line"></div><div class="line"># 从HDFS导入数据到Hive表</div><div class="line">hive&gt; load data inpath &apos;/flume/events/birdben.ad.click_ad/201610/events-.1476364421957&apos; into table test_partition_table partition (dt=&apos;2016-10-18&apos;);</div><div class="line"></div><div class="line"># 导入完成之后，查询test_partition_table表中的数据</div><div class="line">hive&gt; select * from test_partition_table;OK[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;59948935480868864&quot;,&quot;bid&quot;:null,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475150396804&#125;]	info	logs	NULL	2016-10-18[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;59948935480868864&quot;,&quot;bid&quot;:null,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475150470244&#125;]	info	logs	NULL	2016-10-18</div><div class="line">...</div><div class="line">Time taken: 0.102 seconds, Fetched: 26 row(s)</div><div class="line"></div><div class="line"># 在HDFS中再次查看源文件，此时源文件已经在此目录下不存在了，因为已经被移动到/hive/warehouse下，所以说使用load从HDFS中导入数据到Hive的方式，是将原来HDFS文件移动到Hive默认配置的数据仓库下（即:/hive/warehouse下，此目录是在hive-site.xml配置文件中配置的）</div><div class="line">$ hdfs dfs -ls /flume/events/rp.hb.ad.view_ad/201610</div><div class="line"></div><div class="line"># 查看Hive默认配置的数据仓库的HDFS目录下，即可找到我们导入的文件</div><div class="line">$ hdfs dfs -ls /hive/warehouse/test_hdfs.db/test_partition_table/dt=2016-10-18Found 1 items-rwxr-xr-x   2 yunyu supergroup       6776 2016-10-13 06:18 /hive/warehouse/test_hdfs.db/test_partition_table/dt=2016-10-18/events-.1476364421957</div></pre></td></tr></table></figure>
<p>原文链接：</p>
<ul>
<li><a href="http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/" target="_blank" rel="external">http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/</a></li>
<li><a href="http://www.cnblogs.com/luogankun/p/4111145.html" target="_blank" rel="external">http://www.cnblogs.com/luogankun/p/4111145.html</a></li>
<li><a href="http://stackoverflow.com/questions/30907657/add-partition-after-creating-table-in-hive" target="_blank" rel="external">http://stackoverflow.com/questions/30907657/add-partition-after-creating-table-in-hive</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Flume/Flume学习（十四）Flume整合Kafka" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/17/Flume/Flume学习（十四）Flume整合Kafka/" class="article-date">
  	<time datetime="2016-10-17T05:32:03.000Z" itemprop="datePublished">2016-10-17</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/17/Flume/Flume学习（十四）Flume整合Kafka/">Flume学习（十四）Flume整合Kafka</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="环境简介"><a href="#环境简介" class="headerlink" title="环境简介"></a>环境简介</h3><ul>
<li>JDK1.7.0_79</li>
<li>Flume1.6.0</li>
<li>kafka_2.11-0.9.0.0</li>
</ul>
<h3 id="Flume整合Kafka的相关配置"><a href="#Flume整合Kafka的相关配置" class="headerlink" title="Flume整合Kafka的相关配置"></a>Flume整合Kafka的相关配置</h3><h4 id="flume-agent-file-conf配置文件"><a href="#flume-agent-file-conf配置文件" class="headerlink" title="flume_agent_file.conf配置文件"></a>flume_agent_file.conf配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">agentX.sources = sX</div><div class="line">agentX.channels = chX</div><div class="line">agentX.sinks = sk1 sk2</div><div class="line"></div><div class="line">agentX.sources.sX.channels = chX</div><div class="line">agentX.sources.sX.type = exec</div><div class="line">agentX.sources.sX.command = tail -F -n +0 /Users/yunyu/Downloads/track.log</div><div class="line"></div><div class="line">agentX.channels.chX.type = memory</div><div class="line">agentX.channels.chX.capacity = 1000</div><div class="line">agentX.channels.chX.transactionCapacity = 100</div><div class="line"></div><div class="line"># Configure sinks</div><div class="line">agentX.sinks.sk1.channel = chX</div><div class="line">agentX.sinks.sk1.type = avro</div><div class="line">agentX.sinks.sk1.hostname = hadoop1</div><div class="line">agentX.sinks.sk1.port = 41414</div><div class="line"></div><div class="line">agentX.sinks.sk2.channel = chX</div><div class="line">agentX.sinks.sk2.type = avro</div><div class="line">agentX.sinks.sk2.hostname = hadoop2</div><div class="line">agentX.sinks.sk2.port = 41414</div><div class="line"></div><div class="line"># Configure loadbalance</div><div class="line">agentX.sinkgroups = g1</div><div class="line">agentX.sinkgroups.g1.sinks = sk1 sk2</div><div class="line">agentX.sinkgroups.g1.processor.type = load_balance</div><div class="line">agentX.sinkgroups.g1.processor.backoff = true</div><div class="line">agentX.sinkgroups.g1.processor.selector = round_robin</div></pre></td></tr></table></figure>
<h4 id="flume-collector-kafka-conf配置文件"><a href="#flume-collector-kafka-conf配置文件" class="headerlink" title="flume_collector_kafka.conf配置文件"></a>flume_collector_kafka.conf配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">agentX.sources = flume-avro-sinkagentX.channels = chXagentX.sinks = flume-kafka-sinkagentX.sources.flume-avro-sink.channels = chXagentX.sources.flume-avro-sink.type = avroagentX.sources.flume-avro-sink.bind = hadoop1agentX.sources.flume-avro-sink.port = 41414agentX.sources.flume-avro-sink.threads = 8agentX.channels.chX.type = memoryagentX.channels.chX.capacity = 10000agentX.channels.chX.transactionCapacity = 100agentX.sinks.flume-kafka-sink.type = org.apache.flume.sink.kafka.KafkaSinkagentX.sinks.flume-kafka-sink.topic = kafka_cluster_topicagentX.sinks.flume-kafka-sink.brokerList = hadoop1:9092,hadoop2:9092,hadoop3:9092agentX.sinks.flume-kafka-sink.requiredAcks = 1agentX.sinks.flume-kafka-sink.batchSize = 20agentX.sinks.flume-kafka-sink.channel = chX</div></pre></td></tr></table></figure>
<h4 id="启动Flume-Agent"><a href="#启动Flume-Agent" class="headerlink" title="启动Flume Agent"></a>启动Flume Agent</h4><p>启动Flume Agent监听track.log日志文件的变化，并且上报的Flume Collector</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/flume-ng agent --conf ./conf/ -f conf/flume_agent_file.conf -Dflume.root.logger=DEBUG,console -n agentX</div></pre></td></tr></table></figure>
<h4 id="启动Flume-Collector"><a href="#启动Flume-Collector" class="headerlink" title="启动Flume Collector"></a>启动Flume Collector</h4><p>启动Flume Collector监听Agent上报的消息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/flume-ng agent --conf ./conf/ -f conf/flume_collector_kafka.conf -Dflume.root.logger=DEBUG,console -n agentX</div></pre></td></tr></table></figure>
<h4 id="启动Kafka"><a href="#启动Kafka" class="headerlink" title="启动Kafka"></a>启动Kafka</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 启动Zookeeper服务（我这里是启动的外置Zookeeper集群，不是Kafka内置的Zookeeper）</div><div class="line">$ ./bin zkServer.sh start</div><div class="line"></div><div class="line"># 启动Kafka服务</div><div class="line">$ ./bin/kafka-server-start.sh -daemon config/server.properties</div><div class="line"></div><div class="line"># 如果是第一次启动Kafka，需要创建一个Topic，用于存储Flume收集上来的日志消息</div><div class="line">$ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic kafka_cluster_topic</div></pre></td></tr></table></figure>
<h4 id="启动Kafka-Consumer"><a href="#启动Kafka-Consumer" class="headerlink" title="启动Kafka Consumer"></a>启动Kafka Consumer</h4><p>启动Kafka Consumer来消费Kafka中的消息，这时候如果track.log日志文件有新日志写入，通过Flume上传并且写入到Kafka，最终可以在Kafka Consumer消费端看到日志文件中的内容。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic kafka_cluster_topic --from-beginning</div><div class="line"></div><div class="line">this is a message</div><div class="line">birdben is my name</div><div class="line">...</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="https://flume.apache.org/FlumeUserGuide.html#kafka-sink" target="_blank" rel="external">https://flume.apache.org/FlumeUserGuide.html#kafka-sink</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Kafka/Kafka学习（二）KafkaOffsetMonitor监控工具使用" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/17/Kafka/Kafka学习（二）KafkaOffsetMonitor监控工具使用/" class="article-date">
  	<time datetime="2016-10-17T02:32:45.000Z" itemprop="datePublished">2016-10-17</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/17/Kafka/Kafka学习（二）KafkaOffsetMonitor监控工具使用/">Kafka学习（二）KafkaOffsetMonitor监控工具使用</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="启动Zookeeper"><a href="#启动Zookeeper" class="headerlink" title="启动Zookeeper"></a>启动Zookeeper</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 分别启动Hadoop1，Hadoop2，Hadoop3三台服务器的Zookeeper服务</div><div class="line">$ ./bin/zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /data/zookeeper-3.4.8/bin/../conf/zoo.cfgStarting zookeeper ... already running as process 4468.</div><div class="line"></div><div class="line"># 分别查看一下Zookeeper服务的状态</div><div class="line">$ ./bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /data/zookeeper-3.4.8/bin/../conf/zoo.cfgMode: leader</div></pre></td></tr></table></figure>
<h3 id="启动Kafka"><a href="#启动Kafka" class="headerlink" title="启动Kafka"></a>启动Kafka</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 分别启动Hadoop1，Hadoop2，Hadoop3三台服务器的Kafka服务</div><div class="line">$ ./bin/kafka-server-start.sh config/server.properties &amp;</div></pre></td></tr></table></figure>
<h3 id="运行KafkaOffsetMonitor监控服务"><a href="#运行KafkaOffsetMonitor监控服务" class="headerlink" title="运行KafkaOffsetMonitor监控服务"></a>运行KafkaOffsetMonitor监控服务</h3><p>下载 <a href="https://github.com/quantifind/KafkaOffsetMonitor/releases/latest">KafkaOffsetMonitor</a> 的jar包，然后执行下面的运行命令，然后我们就能够访问 <a href="http://localhost:9999/" target="_blank" rel="external">http://localhost:9999/</a> 来进入KafkaOffsetMonitor的监控后台。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">java -cp KafkaOffsetMonitor-assembly-0.2.1.jar \</div><div class="line">     com.quantifind.kafka.offsetapp.OffsetGetterWeb \</div><div class="line">     --zk hadoop1,hadoop2,hadoop3 \</div><div class="line">     --port 9999 \</div><div class="line">     --refresh 10.seconds \</div><div class="line">     --retain 2.days</div></pre></td></tr></table></figure>
<ul>
<li>offsetStorage : 已取消</li>
<li>zk : Zookeeper服务器地址</li>
<li>port : KafkaOffsetMonitor监控服务使用的Web服务器端口</li>
<li>refresh : 多长时间将app数据刷新一次到DB</li>
<li>retain : 保存多久的数据到DB</li>
<li>dbName : 历史数据存储的数据库名(default ‘offsetapp’)</li>
<li>kafkaOffsetForceFromStart : 已取消</li>
<li>stormZKOffsetBase : 已取消</li>
<li>pluginsArgs : 扩展使用</li>
</ul>
<p>注意：这里使用的0.2.1版本，0.2.1版本已经没有offsetStorage参数了，所以网上搜索的一些文章中使用的老版本还配置了offsetStorage参数，这里需要注意一下。</p>
<p><img src="http://img.blog.csdn.net/20161017111332769?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="KafkaOffsetMonitor效果图"></p>
<p>参考文章：</p>
<ul>
<li><a href="https://github.com/quantifind/KafkaOffsetMonitor">https://github.com/quantifind/KafkaOffsetMonitor</a></li>
<li><a href="https://github.com/quantifind/KafkaOffsetMonitor/issues/86">https://github.com/quantifind/KafkaOffsetMonitor/issues/86</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/MQ/">MQ</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Flume/Flume学习（十三）Flume + HDFS + Hive离线分析（再续）" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/13/Flume/Flume学习（十三）Flume + HDFS + Hive离线分析（再续）/" class="article-date">
  	<time datetime="2016-10-13T08:52:54.000Z" itemprop="datePublished">2016-10-13</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/13/Flume/Flume学习（十三）Flume + HDFS + Hive离线分析（再续）/">Flume学习（十三）Flume + HDFS + Hive离线分析（再续）</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在《Flume学习（十一）Flume + HDFS + Hive离线分析》这篇中我们就遇到了Hive分区的问题，这里我们再来回顾一下之前待调研的问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"># 问题二：</div><div class="line">之前我们在Flume中配置了采集到的日志输出到HDFS的保存路径是下面两种，一种使用了日期分割的，一种是没有使用日期分割的</div><div class="line">- hdfs://10.10.1.64:8020/flume/events/20160923</div><div class="line">- hdfs://10.10.1.64:8020/flume/events/</div><div class="line"></div><div class="line"># 解决方案：</div><div class="line">如果我们使用第二种不用日期分割的方式，在Hive上创建表指定/flume/events路径是没有问题，查询数据也都正常，但是如果使用第一种日期分割的方式，在Hive上创建表就必须指定具体的子目录，而不是/flume/events根目录，这样虽然表能够建成功但是却查询不到任何数据，因为指定的对应HDFS目录不正确，应该指定为/flume/events/20160923。这个问题确实也困扰我很久，最后才发现原来是Hive建表指定的HDFS目录不正确。</div><div class="line"></div><div class="line">指定location为&apos;/flume/events&apos;不好用，Hive中查询command_json_table表中没有数据</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_json_table(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hive.hcatalog.data.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div><div class="line"></div><div class="line">指定location为&apos;/flume/events/20160923&apos;好用，Hive中查询command_json_table_20160923表中有数据</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_json_table_20160923(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hive.hcatalog.data.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/20160923&apos;;</div><div class="line"></div><div class="line">建议的解决方式是使用Hive的表分区来做，需要调研Hive的表分区是否支持使用HDFS已经分割好的目录结构（需要调研）</div></pre></td></tr></table></figure>
<p>上面是我们之前的问题原文描述，之前需要调研Hive表分区是否可以使用HDFS已经分割好的目录结构，这里我找到了一篇blog，终于理解了Hive关于External表如何使用partition的，下面给出了原文和译文的链接地址</p>
<p>原文链接：</p>
<ul>
<li><a href="http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/" target="_blank" rel="external">http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/</a></li>
</ul>
<p>译文链接：</p>
<ul>
<li><p>我们带着上面的问题继续优化，之前的解决办法是按照我们日志中的name属性值存储在HDFS的不同目录中，本篇我们使用Partition来解决数据量增长的情况，我们在之前使用name属性的基础上在新建dt目录（按照月份来分割数据）</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">agentX.sources = flume-avro-sinkagentX.channels = chXagentX.sinks = flume-hdfs-sinkagentX.sources.flume-avro-sink.channels = chXagentX.sources.flume-avro-sink.type = avroagentX.sources.flume-avro-sink.bind = hadoop1agentX.sources.flume-avro-sink.port = 41414agentX.sources.flume-avro-sink.threads = 8#定义拦截器，为消息添加时间戳和Host地址</div><div class="line">#将日志中的name属性添加到Header中，用来做HDFS存储的目录结构，type_name属性就是从日志文件中解析出来的name属性的值，这里使用%Y%m表达式代表按照年月分区agentX.sources.flume-avro-sink.interceptors = i1 i2agentX.sources.flume-avro-sink.interceptors.i1.type = timestampagentX.sources.flume-avro-sink.interceptors.i2.type = regex_extractoragentX.sources.flume-avro-sink.interceptors.i2.regex = &quot;name&quot;:&quot;(.*?)&quot;agentX.sources.flume-avro-sink.interceptors.i2.serializers = s1agentX.sources.flume-avro-sink.interceptors.i2.serializers.s1.name = type_nameagentX.channels.chX.type = memoryagentX.channels.chX.capacity = 1000agentX.channels.chX.transactionCapacity = 100agentX.sinks.flume-hdfs-sink.type = hdfsagentX.sinks.flume-hdfs-sink.channel = chXagentX.sinks.flume-hdfs-sink.hdfs.path = hdfs://10.10.1.64:8020/flume/events/%&#123;type_name&#125;/%Y%magentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStreamagentX.sinks.flume-hdfs-sink.hdfs.filePrefix = events-agentX.sinks.flume-hdfs-sink.hdfs.rollInterval = 300agentX.sinks.flume-hdfs-sink.hdfs.rollSize = 0agentX.sinks.flume-hdfs-sink.hdfs.rollCount = 300</div></pre></td></tr></table></figure>
<h5 id="在HDFS中查看文件目录"><a href="#在HDFS中查看文件目录" class="headerlink" title="在HDFS中查看文件目录"></a>在HDFS中查看文件目录</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 可以看到HDFS文件目录已经按照我们的name属性区分开了</div><div class="line">hdfs dfs -ls /flume/events/drwxr-xr-x   - yunyu supergroup          0 2016-10-13 07:01 /flume/events/birdben.api.calldrwxr-xr-x   - yunyu supergroup          0 2016-10-13 07:02 /flume/events/birdben.ad.click_addrwxr-xr-x   - yunyu supergroup          0 2016-10-13 07:02 /flume/events/birdben.ad.open_hbdrwxr-xr-x   - yunyu supergroup          0 2016-10-13 07:02 /flume/events/birdben.ad.view_ad</div><div class="line"></div><div class="line"># 查看个不同name下的目录是按照年月分割开的</div><div class="line">$ hdfs dfs -ls /flume/events/birdben.ad.click_adFound 2 itemsdrwxr-xr-x   - yunyu supergroup          0 2016-10-13 06:18 /flume/events/birdben.ad.click_ad/201610drwxr-xr-x   - yunyu supergroup          0 2016-10-13 07:07 /flume/events/birdben.ad.click_ad/201611</div><div class="line"></div><div class="line"># 数据文件是存储在具体的年月目录下的</div><div class="line">$ hdfs dfs -ls /flume/events/birdben.ad.click_ad/201610/Found 1 items-rw-r--r--   2 yunyu supergroup       1596 2016-10-13 06:18 /flume/events/birdben.ad.click_ad/201610/events-.1476364422107</div></pre></td></tr></table></figure>
<h3 id="Hive按照不同的HDFS目录建表"><a href="#Hive按照不同的HDFS目录建表" class="headerlink" title="Hive按照不同的HDFS目录建表"></a>Hive按照不同的HDFS目录建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"># 这里我们是需要先理解Hive的内部表和外部表的区别，然后我们在之前的建表语句中加入partition分区，我们这里使用的是dt字段作为partition，dt字段不能够与建表语句中的字段重复，否则建表时会报错。</div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_click_ad(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.click_ad&apos;;</div><div class="line"></div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_open_hb(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.open_hb&apos;;</div><div class="line"></div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_view_ad(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.view_ad&apos;;</div><div class="line"></div><div class="line"># 这时候我们查询表，表中是没有数据的。我们需要手工添加partition分区之后，才能查到数据。</div><div class="line">hive&gt; select * from birdben_ad_click_ad;</div><div class="line"></div><div class="line"># 建表完成之后，我们需要手工添加partition目录为我们Flume之前划分的好的年月目录</div><div class="line">alter table birdben_ad_click_ad add partition(dt=&apos;201610&apos;) location &apos;/flume/events/birdben_ad_click_ad/201610&apos;;</div><div class="line">alter table birdben_ad_click_ad add partition(dt=&apos;201611&apos;) location &apos;/flume/events/birdben_ad_click_ad/201611&apos;;</div><div class="line"></div><div class="line">alter table birdben_ad_open_hb add partition(dt=&apos;201610&apos;) location &apos;/flume/events/birdben.ad.open_hb/201610&apos;;</div><div class="line">alter table birdben_ad_open_hb add partition(dt=&apos;201611&apos;) location &apos;/flume/events/birdben.ad.open_hb/201611&apos;;</div><div class="line"></div><div class="line">alter table birdben_ad_view_ad add partition(dt=&apos;201610&apos;) location &apos;/flume/events/birdben.ad.view_ad/201610&apos;;</div><div class="line">alter table birdben_ad_view_ad add partition(dt=&apos;201611&apos;) location &apos;/flume/events/birdben.ad.view_ad/201611&apos;;</div><div class="line"></div><div class="line"># 这时候我们查询表，能够查询到全部的数据了（包括201610和201611的数据）</div><div class="line">hive&gt; select * from birdben_ad_click_ad;</div><div class="line">OK[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201611[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201611[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201611Time taken: 0.1 seconds, Fetched: 9 row(s)</div><div class="line"></div><div class="line"># 也可以按照分区字段查询数据，这样就能够证明我们可以使用Hive的External表partition对应到我们Flume中创建好的 %Y%m（年月） 目录结构</div><div class="line">hive&gt; select * from birdben_ad_click_ad where dt = &apos;201610&apos;;</div><div class="line">OK[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201610Time taken: 0.099 seconds, Fetched: 6 row(s)</div><div class="line"></div><div class="line">hive&gt; select * from birdben_ad_click_ad where dt = &apos;201611&apos;;</div><div class="line">OK</div><div class="line">[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201611[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201611[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201611Time taken: 0.11 seconds, Fetched: 3 row(s)</div></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>其实我写了这么多篇Flume + HDFS + Hive的文章，就是为了证明Flume可以按照指定的Header的key分别写入不同的HDFS目录，Hive又可以通过External表将Location定位到Flume写入的HDFS目录，而且还可以通过Partition分区定位到Flume设置的Header对应的目录，这样就能够比较优雅的将Flume, HDFS, Hive整合到一起了。但是还是有些需要优化的地方，比如说我们的日志格式不够规范，每种日志都有不同的格式，而且还都写入到同一个track.log日志文件中，只能通过name属性作区分。还有就是Hive的Partition每次需要手工去修改表，否则无法查询到HDFS对应目录下的数据，也有人使用 <a href="https://github.com/don9z/hadoop-tools/blob/master/hive/addpartition.py">script</a> 脚本来做这些事情，待以后有时间继续深入研究。</p>
<p>参考文章：</p>
<ul>
<li><a href="http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/" target="_blank" rel="external">http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Flume/Flume学习（十二）Flume + HDFS + Hive离线分析（续）" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/12/Flume/Flume学习（十二）Flume + HDFS + Hive离线分析（续）/" class="article-date">
  	<time datetime="2016-10-12T05:43:55.000Z" itemprop="datePublished">2016-10-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/12/Flume/Flume学习（十二）Flume + HDFS + Hive离线分析（续）/">Flume学习（十二）Flume + HDFS + Hive离线分析（续）</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上一篇中我们已经实现了使用Flume收集日志并且输出到HDFS中，并且结合Hive在HDFS进行离线的查询分析。但是也同样遇到了一些问题，本篇将解决更复杂的日志收集情况，将不同的日志格式写入到同一个日志文件，然后用Flume根据Header来写入到HDFS不同的目录。</p>
<h3 id="日志结构"><a href="#日志结构" class="headerlink" title="日志结构"></a>日志结构</h3><p>我们会讲所有的日志都写入到track.log文件中，包含API调用的日志以及其他埋点日志，这里是通过name来区分日志类型的，不同的日志类型有着不同的json结构。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">### API日志</div><div class="line"></div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;name&quot;:&quot;birdben.api.call&quot;,&quot;request&quot;:&quot;POST /api/message/receive&quot;,&quot;status&quot;:&quot;succeeded&quot;,&quot;bid&quot;:&quot;59885256139866115&quot;,&quot;uid&quot;:&quot;&quot;,&quot;did&quot;:&quot;1265&quot;,&quot;duid&quot;:&quot;dxf536&quot;,&quot;hb_uid&quot;:&quot;59885256030814209&quot;,&quot;ua&quot;:&quot;Dalvik/1.6.0 (Linux; U; Android 4.4.4; YQ601 Build/KTU84P)&quot;,&quot;device_id&quot;:&quot;fa48a076-f35f-3217-8575-5fc1f02f1ac0&quot;,&quot;ip&quot;:&quot;::ffff:10.10.1.242&quot;,&quot;server_timestamp&quot;:1475912702996&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:02.996Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;name&quot;:&quot;birdben.api.call&quot;,&quot;request&quot;:&quot;GET /api/message/ad-detail&quot;,&quot;status&quot;:&quot;succeeded&quot;,&quot;bid&quot;:&quot;59885256139866115&quot;,&quot;uid&quot;:&quot;&quot;,&quot;did&quot;:&quot;1265&quot;,&quot;duid&quot;:&quot;dxf536&quot;,&quot;hb_uid&quot;:&quot;59885256030814209&quot;,&quot;ua&quot;:&quot;Dalvik/1.6.0 (Linux; U; Android 4.4.4; YQ601 Build/KTU84P)&quot;,&quot;device_id&quot;:&quot;fa48a076-f35f-3217-8575-5fc1f02f1ac0&quot;,&quot;ip&quot;:&quot;::ffff:10.10.1.242&quot;,&quot;server_timestamp&quot;:1475912787476&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:46:27.476Z&quot;&#125;</div><div class="line"></div><div class="line">### 打开App日志</div><div class="line"></div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914816071&quot;,&quot;rpid&quot;:&quot;63152468644593670&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829286&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.286Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914827206&quot;,&quot;rpid&quot;:&quot;63152468644593670&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840425&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.425Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915077351&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915090579&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:50.579Z&quot;&#125;</div><div class="line"></div><div class="line">### 加载页面日志</div><div class="line"></div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914816133&quot;,&quot;rpid&quot;:&quot;63152468644593670&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829332&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.332Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914827284&quot;,&quot;rpid&quot;:&quot;63152468644593670&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840498&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.499Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915077585&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915090789&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:50.789Z&quot;&#125;</div><div class="line"></div><div class="line">### 点击链接日志</div><div class="line"></div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475913832349&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475913845544&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:04:05.544Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915080561&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915093792&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:53.792Z&quot;&#125;</div></pre></td></tr></table></figure>
<h3 id="如何解析track-log日志文件中的日志"><a href="#如何解析track-log日志文件中的日志" class="headerlink" title="如何解析track.log日志文件中的日志"></a>如何解析track.log日志文件中的日志</h3><p>按照我们之前的做法，我们会使用Flume都讲日志的内容收集到HDFS上存储，但是这里的track.log日志文件中包含多种不同结构的json日志，而且这里的json数据结构是嵌套复杂对象的，我们不好在Hive上创建相应结构的表，只能创建一个大表要包含所有的日志字段，无法做到对某种日志的分析，如果像之前的做法可能无法满足我们的需求。</p>
<ul>
<li>问题一：如何Hive解析这种嵌套复杂对象的json数据结构</li>
<li><p>问题二：如何将多种不同的日志在HDFS按类型分开存储</p>
</li>
<li><p>问题一解决办法：<br>在网上找到第三方的插件能够解析嵌套复杂对象的json数据结构，主要是替换Hive自己内嵌的Serde解析器（org.apache.hive.hcatalog.data.JsonSerDe），Github地址：<a href="https://github.com/rcongiu/Hive-JSON-Serde">https://github.com/rcongiu/Hive-JSON-Serde</a></p>
</li>
<li><p>问题二解决办法：<br>这里我有个想法是按照日志类型，我们可以区分我们的日志结构，根据name属性分为API日志，打开APP日志，加载页面日志，点击链接日志。但是要如何在Flume根据name属性区分开不同的日志内容，并且写入到HDFS的不同目录呢？答案就是使用Flume的Interceptor</p>
</li>
</ul>
<h3 id="Hive安装Hive-JSON-Serde插件"><a href="#Hive安装Hive-JSON-Serde插件" class="headerlink" title="Hive安装Hive-JSON-Serde插件"></a>Hive安装Hive-JSON-Serde插件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"># 从GitHub下载Hive-JSON-Serde</div><div class="line">$ git clone https://github.com/rcongiu/Hive-JSON-Serde</div><div class="line"></div><div class="line"># 编译打包Hive-JSON-Serde，打包成功之后会在json-serde/target目录生成相应的jar包</div><div class="line">$ cd Hive-JSON-Serde</div><div class="line">$ mvn package</div><div class="line"></div><div class="line"># 复制打包好的jar到Hive的HIVE_AUX_JARS_PATH目录下，需要重启Hive服务，这样就不需要每次在Hive Shell中都进行add jar操作了</div><div class="line">$ cp json-serde/target/json-serde-1.3.8-SNAPSHOT-jar-with-dependencies.jar /usr/local/hive/hcatalog/share/hcatalog/</div><div class="line"></div><div class="line"># HIVE_AUX_JARS_PATH是在$&#123;HIVE_HOME&#125;/conf/hive-env.sh配置文件中设置的</div><div class="line">export HIVE_AUX_JARS_PATH=/usr/local/hive/hcatalog/share/hcatalog</div><div class="line"></div><div class="line"># Hive Shell中创建表，如下</div><div class="line"># 这里使用了我们刚刚引用的&apos;org.openx.data.jsonserde.JsonSerDe&apos;解析器</div><div class="line"># 这样所有的日志都可以通过birdben_log_table表来查询，但是部分字段属性可能没有建表中包含进来，这样可能查出来的属性值是NULL</div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_log_table(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div></pre></td></tr></table></figure>
<h3 id="Flume的Interceptor"><a href="#Flume的Interceptor" class="headerlink" title="Flume的Interceptor"></a>Flume的Interceptor</h3><p>先回想一下我们是如何将日期作为参数写入到HDFS不同目录的，我们是在Flume中使用了Interceptor来将我们的name属性加入到Event的Header中，然后在Sink中通过获取Header中的name属性的值来写入到HDFS中的不同目录。</p>
<h5 id="Flume的配置文件"><a href="#Flume的配置文件" class="headerlink" title="Flume的配置文件"></a>Flume的配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">agentX.sources = flume-avro-sink</div><div class="line">agentX.channels = chX</div><div class="line">agentX.sinks = flume-hdfs-sink</div><div class="line"></div><div class="line">agentX.sources.flume-avro-sink.channels = chX</div><div class="line">agentX.sources.flume-avro-sink.type = avro</div><div class="line">agentX.sources.flume-avro-sink.bind = 10.10.1.64</div><div class="line">agentX.sources.flume-avro-sink.port = 41414</div><div class="line">agentX.sources.flume-avro-sink.threads = 8</div><div class="line"></div><div class="line"></div><div class="line">#定义拦截器，为消息添加时间戳和Host地址</div><div class="line">#将日志中的name属性添加到Header中，用来做HDFS存储的目录结构，type_name属性就是从日志文件中解析出来的name属性的值</div><div class="line">agentX.sources.flume-avro-sink.interceptors = i1 i2</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i1.type = timestamp</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.type = regex_extractor</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.regex = &quot;name&quot;:&quot;(.*?)&quot;</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.serializers = s1</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.serializers.s1.name = type_name</div><div class="line"></div><div class="line">agentX.channels.chX.type = memory</div><div class="line">agentX.channels.chX.capacity = 1000</div><div class="line">agentX.channels.chX.transactionCapacity = 100</div><div class="line"></div><div class="line">agentX.sinks.flume-hdfs-sink.type = hdfs</div><div class="line">agentX.sinks.flume-hdfs-sink.channel = chX</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.path = hdfs://10.10.1.64:8020/flume/events/%&#123;type_name&#125;</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStream</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.filePrefix = events-</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollInterval = 300</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollSize = 0</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollCount = 300</div></pre></td></tr></table></figure>
<h5 id="在HDFS中查看文件目录"><a href="#在HDFS中查看文件目录" class="headerlink" title="在HDFS中查看文件目录"></a>在HDFS中查看文件目录</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 可以看到HDFS文件目录已经按照我们的name属性区分开了</div><div class="line">$ hdfs dfs -ls /flume/eventsdrwxr-xr-x   - yunyu supergroup          0 2016-10-11 03:58 /flume/events/birdben.api.calldrwxr-xr-x   - yunyu supergroup          0 2016-10-11 03:58 /flume/events/birdben.ad.click_addrwxr-xr-x   - yunyu supergroup          0 2016-10-11 03:58 /flume/events/birdben.ad.open_hbdrwxr-xr-x   - yunyu supergroup          0 2016-10-11 03:58 /flume/events/birdben.ad.view_ad</div><div class="line"></div><div class="line">$ hdfs dfs -ls /flume/events/birdben.ad.click_adFound 1 items-rwxr-xr-x   2 yunyu supergroup        798 2016-10-11 03:58 /flume/events/birdben.ad.click_ad/events-.1476183217539</div></pre></td></tr></table></figure>
<h3 id="Hive按照不同的HDFS目录建表"><a href="#Hive按照不同的HDFS目录建表" class="headerlink" title="Hive按照不同的HDFS目录建表"></a>Hive按照不同的HDFS目录建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"># Hive中我们重新建表，这次我们按照HDFS已经分好的目录建表</div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_click_ad(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.click_ad&apos;;</div><div class="line"></div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_open_hb(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.open_hb&apos;;</div><div class="line"></div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_view_ad(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.view_ad&apos;;</div><div class="line"></div><div class="line"># 在Hive中查询birdben_ad_click_ad表中的数据</div><div class="line">hive&gt; select * from birdben_ad_click_ad;OK[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULLTime taken: 0.519 seconds, Fetched: 3 row(s)</div><div class="line"></div><div class="line"># 在Hive中查询birdben_ad_click_ad表中的数据总数</div><div class="line">hive&gt; select count(*) from birdben_ad_click_ad;Query ID = yunyu_20161011234624_fbd62672-91ee-4497-8ea1-f5a1e765a147Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes):  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers:  set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers:  set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1476004456759_0008, Tracking URL = http://hadoop1:8088/proxy/application_1476004456759_0008/Kill Command = /data/hadoop-2.7.1/bin/hadoop job  -kill job_1476004456759_0008Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12016-10-11 23:46:33,190 Stage-1 map = 0%,  reduce = 0%2016-10-11 23:46:39,554 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.95 sec2016-10-11 23:46:48,909 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.56 secMapReduce Total cumulative CPU time: 2 seconds 560 msecEnded Job = job_1476004456759_0008MapReduce Jobs Launched: Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.56 sec   HDFS Read: 8849 HDFS Write: 2 SUCCESSTotal MapReduce CPU Time Spent: 2 seconds 560 msecOK3Time taken: 25.73 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure>
<p>到此为止，我们上面说的两个问题都得到了解决，后续还会继续调优。</p>
<p>参考文章：</p>
<ul>
<li><a href="http://blog.csdn.net/ahjzgyxy/article/details/44423025" target="_blank" rel="external">http://blog.csdn.net/ahjzgyxy/article/details/44423025</a></li>
<li><a href="http://blog.csdn.net/xiao_jun_0820/article/details/38333171" target="_blank" rel="external">http://blog.csdn.net/xiao_jun_0820/article/details/38333171</a></li>
<li><a href="http://lxw1234.com/archives/2015/11/543.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/11/543.htm</a></li>
<li><a href="http://lxw1234.com/archives/2015/11/545.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/11/545.htm</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Flume/Flume学习（十一）Flume + HDFS + Hive离线分析" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/10/Flume/Flume学习（十一）Flume + HDFS + Hive离线分析/" class="article-date">
  	<time datetime="2016-10-10T08:31:12.000Z" itemprop="datePublished">2016-10-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/10/Flume/Flume学习（十一）Flume + HDFS + Hive离线分析/">Flume学习（十一）Flume + HDFS + Hive离线分析</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上一篇中我们已经实现了使用Flume收集日志并且输出到HDFS中，本篇我们将结合Hive在HDFS进行离线的查询分析。具体Hive整合HDFS的环境配置请参考之前的文章。</p>
<h3 id="Hive中创建表"><a href="#Hive中创建表" class="headerlink" title="Hive中创建表"></a>Hive中创建表</h3><p>下面是具体如何在Hive中基于HDFS文件创建表的</p>
<h4 id="启动相关服务"><a href="#启动相关服务" class="headerlink" title="启动相关服务"></a>启动相关服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"># 启动hdfs服务</div><div class="line">$ ./sbin/start-dfs.sh</div><div class="line"></div><div class="line"># 启动yarn服务</div><div class="line">$ ./sbin/start-yarn.sh</div><div class="line"></div><div class="line"># 进入hive安装目录</div><div class="line">$ cd /data/hive-1.2.1</div><div class="line"></div><div class="line"># 启动metastore</div><div class="line">$ ./bin/hive --service metastore &amp;</div><div class="line"></div><div class="line"># 启动hiveserver2</div><div class="line">$ ./bin/hive --service hiveserver2 &amp;</div><div class="line"></div><div class="line"># 启动hive shell</div><div class="line">$ ./bin/hive shell</div><div class="line">hive&gt;</div><div class="line">hive&gt; show databases;</div><div class="line">OK</div><div class="line">default</div><div class="line">Time taken: 1.323 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure>
<h4 id="在HDFS中查看日志文件"><a href="#在HDFS中查看日志文件" class="headerlink" title="在HDFS中查看日志文件"></a>在HDFS中查看日志文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"># 查看HDFS文件存储路径</div><div class="line">$ hdfs dfs -ls /flume/events/</div><div class="line">Found 2 items-rw-r--r--   3 yunyu supergroup       1134 2016-09-19 23:43 /flume/events/events-.1474353822776-rw-r--r--   3 yunyu supergroup        126 2016-09-19 23:44 /flume/events/events-.1474353822777</div><div class="line"></div><div class="line"># 查看HDFS文件内容</div><div class="line">$ hdfs dfs -cat /flume/events/events-.1474353822776</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  543  &#123;&quot;TIME&quot;:&quot;2016-09-06 15:04:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;806&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ll&quot;&#125;</div><div class="line">  565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">  565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div><div class="line">  565  &#123;&quot;TIME&quot;:&quot;2016-09-06 13:10:43&quot;,&quot;HOSTNAME&quot;:&quot;localhost&quot;,&quot;LI&quot;:&quot;783&quot;,&quot;LU&quot;:&quot;yunyu&quot;,&quot;NU&quot;:&quot;yunyu&quot;,&quot;CMD&quot;:&quot;ssh yunyu@10.10.1.15&quot;&#125;</div></pre></td></tr></table></figure>
<h4 id="使用org-apache-hive-hcatalog-data-JsonSerDe解析日志"><a href="#使用org-apache-hive-hcatalog-data-JsonSerDe解析日志" class="headerlink" title="使用org.apache.hive.hcatalog.data.JsonSerDe解析日志"></a>使用org.apache.hive.hcatalog.data.JsonSerDe解析日志</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"># Flume重新写入新的command.log日志到HDFS中</div><div class="line"># 启动hive shell</div><div class="line">$ ./bin/hive shell</div><div class="line"></div><div class="line"># 使用数据库test_hdfs</div><div class="line">hive&gt; use test_hdfs;</div><div class="line"></div><div class="line"># 新建表command_json_table并且使用json解析器提取日志文件中的字段信息</div><div class="line"># ROW FORMAT SERDE：这里使用的是json解析器匹配</div><div class="line"># LOCATION：指定HDFS文件的存储路径</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_json_table(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hive.hcatalog.data.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div><div class="line"></div><div class="line"># 这创建还是会报错，查看hive.log日志文件的错误信息，发现是缺少org.apache.hive.hcatalog.data.JsonSerDe类所在的jar包</div><div class="line">Caused by: java.lang.ClassNotFoundException: Class org.apache.hive.hcatalog.data.JsonSerDe not found</div><div class="line"></div><div class="line"># 查了下Hive的官网wiki，发现需要先执行add jar操作，将hive-hcatalog-core.jar添加到classpath（具体的jar包地址根据自己实际的Hive安装路径修改）</div><div class="line">add jar /usr/local/hive/hcatalog/share/hcatalog/hive-hcatalog-core-1.2.1.jar;</div><div class="line"></div><div class="line"># 为了避免每次启动hive shell都重新执行一下add jar操作，我们这里在$&#123;HIVE_HOME&#125;/conf/hive-env.sh启动脚本中添加如下信息</div><div class="line">export HIVE_AUX_JARS_PATH=/usr/local/hive/hcatalog/share/hcatalog</div><div class="line"></div><div class="line"># 重启Hive服务之后，再次创建command_json_table表成功</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_json_table(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hive.hcatalog.data.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div><div class="line"></div><div class="line"># 查看command_json_table表中的内容，json字段成功的解析出我们要的字段</div><div class="line">hive&gt; select * from command_json_table;</div><div class="line">OK2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 15:04:43	localhost	806	yunyu	yunyu	ll2016-09-06 13:10:43	localhost	783	yunyu	yunyu	ssh yunyu@10.10.1.152016-09-06 13:10:43	localhost	783	yunyu	yunyu	ssh yunyu@10.10.1.152016-09-06 13:10:43	localhost	783	yunyu	yunyu	ssh yunyu@10.10.1.15Time taken: 0.09 seconds, Fetched: 10 row(s)</div></pre></td></tr></table></figure>
<h4 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line"># 问题一：</div><div class="line">我们使用Flume采集到的日志存储在HDFS上，我测试了200条日志通过Flume写入到HDFS上，但是通过Hive查询出来的日志记录总数却不到200条，我又查看了HDFS上的文件内容，发现日志记录的总数是200条。</div><div class="line"></div><div class="line">首先了解HDFS的特点：</div><div class="line">HDFS中所有文件都是由块BLOCK组成，默认块大小为64MB。在我们的测试中由于数据量小，始终在写入文件的第一个BLOCK。而HDFS与一般的POSIX要求的文件系统不太一样，其文件数据的可见性是这样的：</div><div class="line">- 如果创建了文件，这个文件可以立即可见；</div><div class="line">- 写入文件的数据则不被保证可见了，哪怕是执行了刷新操作(flush/sync)。只有数据量大于1个BLOCK时，第一个BLOCK的数据才会被看到，后续的BLOCK也同样的特性。正在写入的BLOCK始终不会被其他用户看到！</div><div class="line">HDFS中的sync()保证数据持久化到了datanode上，然后可以被其他用户看到。</div><div class="line"></div><div class="line">针对HDFS的特点，可以解释刚才问题中的现象，正在写入无法查看。但是使用Hive统计时Flume还在写入那个BLOCK(数据量小的时候)，那岂不是统计不到信息？</div><div class="line"></div><div class="line"># 解决方案：</div><div class="line">每天再按小时切分文件——这样虽然每天文件较多，但是能够保证统计时数据可见！Flume上的配置项为hdfs.rollInterval。</div><div class="line">如果文件数多，那么还可以考虑对以前的每天的小时文件合并为每天一个文件！</div><div class="line"></div><div class="line">所以这里修改flume-hdfs-sink配置，不仅仅使用rollCount超过300来滚动，还添加了rollInterval配置超过5分钟没有数据就滚动。</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollInterval = 300</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollSize = 0</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollCount = 300</div><div class="line"></div><div class="line"># 问题二：</div><div class="line">之前我们在Flume中配置了采集到的日志输出到HDFS的保存路径是下面两种，一种使用了日期分割的，一种是没有使用日期分割的</div><div class="line">- hdfs://10.10.1.64:8020/flume/events/20160923</div><div class="line">- hdfs://10.10.1.64:8020/flume/events/</div><div class="line"></div><div class="line"># 解决方案：</div><div class="line">如果我们使用第二种不用日期分割的方式，在Hive上创建表指定/flume/events路径是没有问题，查询数据也都正常，但是如果使用第一种日期分割的方式，在Hive上创建表就必须指定具体的子目录，而不是/flume/events根目录，这样虽然表能够建成功但是却查询不到任何数据，因为指定的对应HDFS目录不正确，应该指定为/flume/events/20160923。这个问题确实也困扰我很久，最后才发现原来是Hive建表指定的HDFS目录不正确。</div><div class="line"></div><div class="line">指定location为&apos;/flume/events&apos;不好用，Hive中查询command_json_table表中没有数据</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_json_table(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hive.hcatalog.data.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div><div class="line"></div><div class="line">指定location为&apos;/flume/events/20160923&apos;好用，Hive中查询command_json_table_20160923表中有数据</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_json_table_20160923(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hive.hcatalog.data.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/20160923&apos;;</div><div class="line"></div><div class="line">建议的解决方式是使用Hive的表分区来做，需要调研Hive的表分区是否支持使用HDFS已经分割好的目录结构（需要调研）</div><div class="line"></div><div class="line"># 问题三：</div><div class="line">Flume收集日志的时候报错</div><div class="line">Caused by: org.apache.flume.ChannelException: Space for commit to queue couldn&apos;t be acquired Sinks are likely not keeping up with sources, or the buffer size is too tight</div><div class="line">        at org.apache.flume.channel.MemoryChannel$MemoryTransaction.doCommit(MemoryChannel.java:126)</div><div class="line">        at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)</div><div class="line">        at org.apache.flume.channel.ChannelProcessor.processEventBatch(ChannelProcessor.java:192)</div><div class="line">        ... 28 more</div><div class="line"></div><div class="line"># 解决方案：</div><div class="line">根据网络上的方法，发现问题的原因可能是Flume分配的JVM内存太小，或者channel内存队列的容量太小</div><div class="line"></div><div class="line">修改channel内存队列大小</div><div class="line">agent.channels.memoryChanne3.keep-alive = 60</div><div class="line">agent.channels.memoryChanne3.capacity = 1000000</div><div class="line"></div><div class="line">修改java最大内存大小</div><div class="line">vi bin/flume-ng</div><div class="line">JAVA_OPTS=&quot;-Xmx2048m&quot;</div><div class="line"></div><div class="line">修改之后重启所有flume程序，包括客户端和服务器端，问题暂时没有再出现了</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="http://www.aboutyun.com/thread-11252-1-1.html" target="_blank" rel="external">http://www.aboutyun.com/thread-11252-1-1.html</a></li>
<li><a href="http://blog.csdn.net/hijk139/article/details/8465094" target="_blank" rel="external">http://blog.csdn.net/hijk139/article/details/8465094</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/5/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/7/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 birdben
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>



<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82900755-1', 'auto');
  ga('send', 'pageview');

</script>
<!-- End Google Analytics -->




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>