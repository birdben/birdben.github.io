<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>birdben</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="birdben">
<meta property="og:url" content="https://github.com/birdben/page/5/index.html">
<meta property="og:site_name" content="birdben">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="birdben">
  
    <link rel="alternative" href="/atom.xml" title="birdben" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
<script type="text/javascript">
var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1260188951'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s4.cnzz.com/z_stat.php%3Fid%3D1260188951' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/images/logo.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">birdben</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						<li>Links</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/AWK/" style="font-size: 10.83px;">AWK</a> <a href="/tags/Akka/" style="font-size: 10.83px;">Akka</a> <a href="/tags/Dockerfile/" style="font-size: 20px;">Dockerfile</a> <a href="/tags/Docker命令/" style="font-size: 19.17px;">Docker命令</a> <a href="/tags/Docker环境/" style="font-size: 15px;">Docker环境</a> <a href="/tags/ELK/" style="font-size: 16.67px;">ELK</a> <a href="/tags/ElasticSearch/" style="font-size: 10.83px;">ElasticSearch</a> <a href="/tags/Elasticsearch/" style="font-size: 12.5px;">Elasticsearch</a> <a href="/tags/Flume/" style="font-size: 17.5px;">Flume</a> <a href="/tags/Git命令/" style="font-size: 13.33px;">Git命令</a> <a href="/tags/Go/" style="font-size: 14.17px;">Go</a> <a href="/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/tags/HDFS/" style="font-size: 18.33px;">HDFS</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Hadoop原理架构体系/" style="font-size: 13.33px;">Hadoop原理架构体系</a> <a href="/tags/Hive/" style="font-size: 16.67px;">Hive</a> <a href="/tags/JVM/" style="font-size: 11.67px;">JVM</a> <a href="/tags/Java-Web，Socket，Python/" style="font-size: 10px;">Java Web，Socket，Python</a> <a href="/tags/Jenkins环境/" style="font-size: 10px;">Jenkins环境</a> <a href="/tags/Kafka/" style="font-size: 15.83px;">Kafka</a> <a href="/tags/Kibana/" style="font-size: 14.17px;">Kibana</a> <a href="/tags/Linux命令/" style="font-size: 12.5px;">Linux命令</a> <a href="/tags/Logstash/" style="font-size: 15.83px;">Logstash</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/MapReduce/" style="font-size: 11.67px;">MapReduce</a> <a href="/tags/Maven配置/" style="font-size: 11.67px;">Maven配置</a> <a href="/tags/MongoDB/" style="font-size: 11.67px;">MongoDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Nginx/" style="font-size: 10px;">Nginx</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/Shell/" style="font-size: 16.67px;">Shell</a> <a href="/tags/Spring/" style="font-size: 10.83px;">Spring</a> <a href="/tags/Storm/" style="font-size: 12.5px;">Storm</a> <a href="/tags/Zookeeper/" style="font-size: 12.5px;">Zookeeper</a> <a href="/tags/其他/" style="font-size: 10px;">其他</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://blog.csdn.net/birdben">我的CSDN的博客</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">birdben</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="/images/logo.png" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">birdben</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-Kibana/Kibana学习（三）查看Kibana的Request" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/22/Kibana/Kibana学习（三）查看Kibana的Request/" class="article-date">
  	<time datetime="2016-11-22T07:54:06.000Z" itemprop="datePublished">2016-11-22</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/22/Kibana/Kibana学习（三）查看Kibana的Request/">Kibana学习（三）查看Kibana的Request</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>如果查看Kibana发送给ES的request请求，请看下面的图示例，哈哈</p>
<p><img src="http://img.blog.csdn.net/20161122155733354?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Kibana_request1"></p>
<p><img src="http://img.blog.csdn.net/20161122155832431?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Kibana_request2"></p>
<p>这里额外说一下使用curl -I获取Kibana Response的Header问题，Kibana要求Request中的Header必须带有kbn-version这个参数，否则获取到的Response中的Header Status Code就是400，而不是200。这个问题我是使用阿里云的健康检查发现的，因为阿里云要求所有服务的Response Status Code必须是200才认为该服务是健康状态。</p>
<p>阿里云的健康检查使用的命令如下，如果返回非2XX、3XX状态，定义为健康检查异常。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo -e &quot;HEAD /test.html HTTP/1.0\r\n\r\n&quot; |nc -t LAN_IP 80</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ curl -I &apos;http://localhost:5601&apos; -H &apos;Host: localhost:5601&apos;HTTP/1.1 400 Bad Requestkbn-name: kibanakbn-version: 4.5.4content-type: application/json; charset=utf-8cache-control: no-cacheDate: Thu, 01 Dec 2016 10:54:47 GMTConnection: keep-alive$ curl -I &apos;http://localhost:5601&apos; -H &apos;Host: localhost:5601&apos; -H &apos;kbn-version:4.5.4&apos;HTTP/1.1 200 OKkbn-name: kibanakbn-version: 4.5.4cache-control: no-cacheDate: Thu, 01 Dec 2016 11:01:59 GMTConnection: keep-alive</div></pre></td></tr></table></figure>
<p>还有就是阿里云的健康检查异常的情况下，我们检查elasticsearch的运行状态会发现会有很多10和100段的IP地址和我们ES服务器有通信，这是由于负载均衡系统进行健康检查引起的。由于阿里云的健康检查异常，所以就会出现很多tcp连接是TIME_WAIT状态。但是最后我们没有办法在header中添加’kbn-version:4.5.4’参数，所以只好关闭阿里云的健康检查了。</p>
<p>参考文章：</p>
<ul>
<li><a href="https://github.com/elastic/kibana/issues/3580">https://github.com/elastic/kibana/issues/3580</a></li>
<li><a href="https://github.com/elastic/kibana/issues/1370">https://github.com/elastic/kibana/issues/1370</a></li>
<li><a href="https://discuss.elastic.co/t/curl-request-returns-missing-kbn-version-header/50591" target="_blank" rel="external">https://discuss.elastic.co/t/curl-request-returns-missing-kbn-version-header/50591</a></li>
<li><a href="https://helpcdn.aliyun.com/document_detail/27674.html" target="_blank" rel="external">https://helpcdn.aliyun.com/document_detail/27674.html</a></li>
<li><a href="https://helpcdn.aliyun.com/document_detail/27660.html" target="_blank" rel="external">https://helpcdn.aliyun.com/document_detail/27660.html</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kibana/">Kibana</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Kafka/Kafka学习（三）Kafka删除Topic" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/22/Kafka/Kafka学习（三）Kafka删除Topic/" class="article-date">
  	<time datetime="2016-11-21T18:21:01.000Z" itemprop="datePublished">2016-11-22</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/22/Kafka/Kafka学习（三）Kafka删除Topic/">Kafka学习（三）Kafka删除Topic</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h3><ul>
<li>zookeeper-3.4.8</li>
<li>kafka_2.11-0.9.0.0</li>
</ul>
<p>最近在测试Kafka的时候创建了很多个Topic，感觉有些Topic也没什么用可以删掉了，使用Kafka的delete操作如果没有开启delete.topic.enable配置是不会删除的，而Kafka只是将Topic标识成deleted状态做逻辑删除，并且在Zookeeper中的/admin/delete_topics下创建对应的子节点。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kafka-topics.sh --delete --zookeeper localhost:2181 --topic test</div><div class="line">Topic test is marked for deletion.Note: This will have no impact if delete.topic.enable is not set to true.</div></pre></td></tr></table></figure>
<p>但是不太清楚如何完全删除掉Kafka中的Topic，后来查询并且实验了一下Kafka删除Topic的两种方式。</p>
<p>Kafka删除Topic的两种方式：</p>
<ol>
<li>开启Kafka的delete.topic.enable=true配置（推荐使用）</li>
<li>手动删除Zookeeper相关数据</li>
</ol>
<h3 id="方式一（推荐使用）"><a href="#方式一（推荐使用）" class="headerlink" title="方式一（推荐使用）"></a>方式一（推荐使用）</h3><ul>
<li>优点：由Kafka来完成Topic的相关删除，只需要修改server.properties配置文件的delete.topic.enable为true就可以了</li>
<li>缺点：需要重启Kafka来完成配置文件的生效</li>
</ul>
<h5 id="修改server-properties"><a href="#修改server-properties" class="headerlink" title="修改server.properties"></a>修改server.properties</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 默认是false</div><div class="line"># 注意等号前后一定不能有空格，否则配置会不生效（亲自踩过的坑）</div><div class="line">delete.topic.enable=true</div></pre></td></tr></table></figure>
<h5 id="验证方式一"><a href="#验证方式一" class="headerlink" title="验证方式一"></a>验证方式一</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"># 创建新的Topic logstash_test（拥有3个副本）</div><div class="line">$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic logstash_test</div><div class="line"></div><div class="line"># 查看Topic logstash_test的状态，发现Leader是1（broker.id=1）,有三个备份分别是0，1，2</div><div class="line">$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic logstash_testTopic: logstash_test	PartitionCount:1	ReplicationFactor:3	Configs:	Topic: logstash_test	Partition: 0	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</div><div class="line"></div><div class="line"># 查看Zookeeper上的Topic</div><div class="line">$ zkCli.sh -server localhost:2181</div><div class="line">[zk: localhost:2181(CONNECTED) 2] ls /brokers/topics [logstash_test]</div><div class="line"></div><div class="line">[zk: localhost:2181(CONNECTED) 3] ls /config/topics[logstash_test, streams-file-input, test1, __consumer_offsets, connect-test, my-replicated-topic, kafka_test, kafka_cluster_topic]</div><div class="line"></div><div class="line"># 查看Kafka的server.properties配置文件中log.dirs=/tmp/kafka-logs的目录</div><div class="line">$ ll /tmp/kafka-logs/logstash_test-0total 37812drwxrwxr-x  2 yunyu yunyu     4096 Nov 21 22:38 ./drwxrwxr-x 57 yunyu yunyu     4096 Nov 22 10:58 ../-rw-rw-r--  1 yunyu yunyu 10485760 Nov 21 22:41 00000000000000000000.index-rw-rw-r--  1 yunyu yunyu 38681667 Nov 21 22:42 00000000000000000000.log</div><div class="line"></div><div class="line"># 删除Topic logstash_test</div><div class="line">$ kafka-topics.sh --delete --zookeeper localhost:2181 --topic logstash_test</div><div class="line">Topic logstash_test is marked for deletion.Note: This will have no impact if delete.topic.enable is not set to true.</div><div class="line"></div><div class="line"># 再次查看Topic logstash_test的状态，已经没有内容输出，说明Topic已经被删除了</div><div class="line">$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic logstash_test</div><div class="line"></div><div class="line"># 再次查看Zookeeper上的Topic，logstash_test也已经被删除了</div><div class="line">$ zkCli.sh -server localhost:2181</div><div class="line">[zk: localhost:2181(CONNECTED) 2] ls /brokers/topics []</div><div class="line"></div><div class="line">[zk: localhost:2181(CONNECTED) 3] ls /config/topics[streams-file-input, test1, __consumer_offsets, connect-test, my-replicated-topic, kafka_test, kafka_cluster_topic]</div><div class="line"></div><div class="line"># 再次查看/tmp/kafka-logs目录，logstash_test相关日志也被删除了</div><div class="line">$ ll /tmp/kafka-logs/logstash_test*</div></pre></td></tr></table></figure>
<p>通过上述步骤验证，修改Kafka的delete.topic.enable配置来删除Topic十分彻底。</p>
<h3 id="方式二"><a href="#方式二" class="headerlink" title="方式二"></a>方式二</h3><ul>
<li>优点：不需要重启Kafka服务，直接删除Topic对应的系统日志，然后在Zookeeper中删除对应的目录。</li>
<li>缺点：需要人为手动删除，删除之后重新创建同名的Topic会有问题（使用方式一不会有此问题）</li>
</ul>
<h5 id="验证方式二"><a href="#验证方式二" class="headerlink" title="验证方式二"></a>验证方式二</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"># 创建新的Topic logstash_test（拥有3个副本）</div><div class="line">$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic logstash_test</div><div class="line"></div><div class="line"># 查看Topic logstash_test的状态，发现Leader是1（broker.id=1）,有三个备份分别是0，1，2</div><div class="line">$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic logstash_testTopic: logstash_test	PartitionCount:1	ReplicationFactor:3	Configs:	Topic: logstash_test	Partition: 0	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</div><div class="line"></div><div class="line"># 查看Zookeeper上的Topic</div><div class="line">$ zkCli.sh -server localhost:2181</div><div class="line">[zk: localhost:2181(CONNECTED) 2] ls /brokers/topics [logstash_test]</div><div class="line"></div><div class="line">[zk: localhost:2181(CONNECTED) 3] ls /config/topics[logstash_test, streams-file-input, test1, __consumer_offsets, connect-test, my-replicated-topic, kafka_test, kafka_cluster_topic]</div><div class="line"></div><div class="line"># 查看Kafka的server.properties配置文件中log.dirs=/tmp/kafka-logs的目录</div><div class="line">$ ll /tmp/kafka-logs/logstash_test-0total 37812drwxrwxr-x  2 yunyu yunyu     4096 Nov 21 22:38 ./drwxrwxr-x 57 yunyu yunyu     4096 Nov 22 10:58 ../-rw-rw-r--  1 yunyu yunyu 10485760 Nov 21 22:41 00000000000000000000.index-rw-rw-r--  1 yunyu yunyu 38681667 Nov 21 22:42 00000000000000000000.log</div><div class="line"></div><div class="line"># 删除Topic logstash_test的log文件（这里Kafka集群的所有节点都要删除）</div><div class="line">$ rm -rf /tmp/kafka-logs/logstash_test*</div><div class="line"></div><div class="line"># 删除Zookeeper上的Topic</div><div class="line">$ zkCli.sh -server localhost:2181</div><div class="line">[zk: localhost:2181(CONNECTED) 2] rmr /brokers/topics/logstash_test []</div><div class="line"></div><div class="line">[zk: localhost:2181(CONNECTED) 3] rmr /config/topics/logstash_test</div><div class="line"></div><div class="line"># 再次查看Topic logstash_test的状态，已经没有内容输出，说明Topic已经被删除了</div><div class="line">$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic logstash_test</div><div class="line"></div><div class="line"># 貌似这种方式也能达到方式一同样的效果，但是偶然发现该方式删除之后创建同名的Topic会有问题</div><div class="line"># 再次创建Topic logstash_test</div><div class="line">$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic logstash_test</div><div class="line"></div><div class="line"># 查看Topic logstash_test的状态，发现Leader是none，isr为空</div><div class="line">$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic logstash_testTopic:logstash_test	PartitionCount:1	ReplicationFactor:3	Configs:	Topic: logstash_test	Partition: 0	Leader: none	Replicas: 1,2,0	Isr:</div></pre></td></tr></table></figure>
<p>但是重启Kafka之后创建该Topic就不会有Leader是none，isr为空的这个问题，如果方式二也需要重启Kafka就没有方式一只修改配置重启一次方便了，所以还是不建议手动删除Kafka的Topic，推荐使用Kafka官方修改配置的方式。</p>
<p>参考文章：</p>
<ul>
<li><a href="http://blog.csdn.net/weipanp/article/details/46330471" target="_blank" rel="external">http://blog.csdn.net/weipanp/article/details/46330471</a></li>
<li><a href="http://blog.csdn.net/xiaoyu_bd/article/details/52268647" target="_blank" rel="external">http://blog.csdn.net/xiaoyu_bd/article/details/52268647</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/MQ/">MQ</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Logstash/Logstash学习（二）Logstash整合Kafka" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/21/Logstash/Logstash学习（二）Logstash整合Kafka/" class="article-date">
  	<time datetime="2016-11-21T11:52:20.000Z" itemprop="datePublished">2016-11-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/21/Logstash/Logstash学习（二）Logstash整合Kafka/">Logstash学习（二）Logstash整合Kafka</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>前面我们已经实现通过Logstash读取track.log日志文件，然后写入到ES中。现在我们为了完善我们的日志收集系统架构，需要在中间添加Kafka消息队列做缓冲。这里我们使用了Logstash的Kafka插件来集成Kafka的。具体插件的官方地址如下：</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/logstash/2.3/plugins-inputs-kafka.html" target="_blank" rel="external">https://www.elastic.co/guide/en/logstash/2.3/plugins-inputs-kafka.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/2.3/plugins-outputs-kafka.html" target="_blank" rel="external">https://www.elastic.co/guide/en/logstash/2.3/plugins-outputs-kafka.html</a></li>
</ul>
<p>新版本的Logstash已经默认安装好大部分的插件了，所以无需像1.x版本的Logstash还需要手动修改Gemfile的source，然后手动安装插件了。</p>
<h3 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h3><ul>
<li>kafka_2.11-0.9.0.0</li>
<li>zookeeper-3.4.8</li>
<li>logstash-2.3.4</li>
<li>elasticsearch-2.3.5</li>
</ul>
<p>具体环境的安装这里不做重点介绍。</p>
<p>这里我们自己配置了一个logstash-shipper用来从track.log日志文件读取日志，并且写入到Kafka中。当然这里也可以由其他生产者来代替Logstash收集日志并且写入Kafka（比如：Flume等等）。这里我们是本地测试所以简单点直接使用Logstash读取本机的日志文件，然后写入到Kafka消息队列中。</p>
<h3 id="logstash-shipper-kafka-conf配置"><a href="#logstash-shipper-kafka-conf配置" class="headerlink" title="logstash-shipper-kafka.conf配置"></a>logstash-shipper-kafka.conf配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    file &#123;</div><div class="line">        path =&gt; [&quot;/home/yunyu/Downloads/track.log&quot;]</div><div class="line">        type =&gt; &quot;api&quot;</div><div class="line">        codec =&gt; &quot;json&quot;</div><div class="line">        start_position =&gt; &quot;beginning&quot;</div><div class="line">        # 设置是否忽略太旧的日志的</div><div class="line">        # 如果没设置该属性可能会导致读取不到文件内容，因为我们的日志大部分是好几个月前的，所以这里设置为不忽略</div><div class="line">        ignore_older =&gt; 0</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">output &#123;</div><div class="line">    stdout &#123;</div><div class="line">        codec =&gt; rubydebug</div><div class="line">    &#125;</div><div class="line">    kafka &#123;</div><div class="line">        # 指定Kafka集群地址</div><div class="line">        bootstrap_servers =&gt; &quot;hadoop1:9092,hadoop2:9092,hadoop3:9092&quot;</div><div class="line">        # 指定Kafka的Topic</div><div class="line">        topic_id =&gt; &quot;logstash_test&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>官网给出的注释</p>
<ul>
<li>ignore_older</li>
</ul>
<p>The default behavior of the file input plugin is to ignore files whose last modification is greater than 86400s. To change this default behavior and process the tutorial file (which date can be much older than a day), we need to specify to not ignore old files.</p>
<h3 id="logstash-indexer-kafka-conf配置"><a href="#logstash-indexer-kafka-conf配置" class="headerlink" title="logstash-indexer-kafka.conf配置"></a>logstash-indexer-kafka.conf配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    kafka &#123;</div><div class="line">        # 指定Zookeeper集群地址</div><div class="line">        zk_connect =&gt; &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;</div><div class="line">        # 指定当前消费者的group_id</div><div class="line">        group_id =&gt; &quot;logstash&quot;</div><div class="line">        # 指定消费的Topic</div><div class="line">        topic_id =&gt; &quot;logstash_test&quot;</div><div class="line">        # 指定消费的内容类型（默认是json）</div><div class="line">        codec =&gt; &quot;json&quot;</div><div class="line">        # 设置Consumer消费者从Kafka最开始的消息开始消费，必须结合&quot;auto_offset_reset =&gt; smallest&quot;一起使用</div><div class="line">        reset_beginning =&gt; true</div><div class="line">        # 设置如果Consumer消费者还没有创建offset或者offset非法，从最开始的消息开始消费还是从最新的消息开始消费</div><div class="line">        auto_offset_reset =&gt; &quot;smallest&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">filter &#123;</div><div class="line">    # 将logs数组对象进行拆分</div><div class="line">    split &#123;</div><div class="line">        field =&gt; &quot;logs&quot;</div><div class="line">    &#125;</div><div class="line">    date &#123;</div><div class="line">        match =&gt; [&quot;timestamp&quot;, &quot;yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSS&apos;Z&apos;&quot;]</div><div class="line">        target =&gt; &quot;@timestamp&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">output &#123;</div><div class="line">    stdout &#123;</div><div class="line">        codec =&gt; rubydebug</div><div class="line">    &#125;</div><div class="line">    elasticsearch &#123;</div><div class="line">        codec =&gt; &quot;json&quot;</div><div class="line">        hosts =&gt; [&quot;hadoop1:9200&quot;, &quot;hadoop2:9200&quot;, &quot;hadoop3:9200&quot;]</div><div class="line">        index =&gt; &quot;api_logs_index&quot;</div><div class="line">        workers =&gt; 1</div><div class="line">        flush_size =&gt; 20000</div><div class="line">        idle_flush_time =&gt; 10</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>官网给出的注释</p>
<ul>
<li><p>auto_offset_reset</p>
<ul>
<li>Value can be any of: largest, smallest</li>
<li>Default value is “largest”</li>
</ul>
<p>smallest or largest - (optional, default largest) If the consumer does not already have an established offset or offset is invalid, start with the earliest message present in the log (smallest) or after the last message in the log (largest).</p>
</li>
<li><p>reset_beginning</p>
<ul>
<li>Value type is boolean</li>
<li>Default value is false</li>
</ul>
<p>Reset the consumer group to start at the earliest message present in the log by clearing any offsets for the group stored in Zookeeper. This is destructive! Must be used in conjunction with auto_offset_reset ⇒ smallest</p>
</li>
</ul>
<h3 id="Mapping配置"><a href="#Mapping配置" class="headerlink" title="Mapping配置"></a>Mapping配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;mappings&quot;: &#123;</div><div class="line">    &quot;_default_&quot;: &#123;</div><div class="line">      &quot;_all&quot;: &#123;</div><div class="line">        &quot;enabled&quot;: true</div><div class="line">      &#125;,</div><div class="line">      &quot;dynamic_templates&quot;: [</div><div class="line">        &#123;</div><div class="line">          &quot;my_template&quot;: &#123;</div><div class="line">            &quot;match_mapping_type&quot;: &quot;string&quot;,</div><div class="line">            &quot;mapping&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;raw&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      ]</div><div class="line">    &#125;,</div><div class="line">    &quot;api&quot;: &#123;</div><div class="line">      &quot;properties&quot;: &#123;</div><div class="line">        &quot;timestamp&quot;: &#123;</div><div class="line">          &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot;,</div><div class="line">          &quot;type&quot;: &quot;date&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;message&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;,</div><div class="line">          &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;level&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;host&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;logs&quot;: &#123;</div><div class="line">          &quot;properties&quot;: &#123;</div><div class="line">            &quot;uid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;status&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;did&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;device-id&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;device_id&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;errorMsg&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;rpid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;url&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;errorStatus&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;ip&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;timestamp&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;hb_uid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;duid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;request&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;name&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;errorCode&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;ua&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;server_timestamp&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;bid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;,</div><div class="line">        &quot;path&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;,</div><div class="line">          &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;type&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;,</div><div class="line">          &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;@timestamp&quot;: &#123;</div><div class="line">          &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot;,</div><div class="line">          &quot;type&quot;: &quot;date&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;@version&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;,</div><div class="line">          &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Elasticsearch 会自动使用自己的默认分词器(空格，点，斜线等分割)来分析字段。分词器对于搜索和评分是非常重要的，但是大大降低了索引写入和聚合请求的性能。所以 logstash 模板定义了一种叫”多字段”(multi-field)类型的字段。这种类型会自动添加一个 “.raw” 结尾的字段，并给这个字段设置为不启用分词器。简单说，你想获取 url 字段的聚合结果的时候，不要直接用 “url” ，而是用 “url.raw” 作为字段名。</p>
<p>这里使用dynamic_templates是因为我们这里有嵌套结构logs，即使我们在内嵌的logs结构中定义了字段是not_analyzed，但是新创建出来的索引数据仍然是analyzed的（不知道是为什么）。如果字段都是analyzed就无法在Kibana中进行统计，这里使用dynamic_templates，给所有动态字段都加一个raw字段，这个字段名就是原字段（比如:logs.name）后面加上一个.raw（变成logs.name.raw），专门用来解决analyzed无法做统计的，所有的.raw字段都是not_analyzed，这样就可以使用.raw字段（logs.name.raw）进行统计分析了，而全文搜索可以继续使用原字段（logs.name）。</p>
<p>这里还需要注意的就是，需要精确匹配的字段要设置成not_analyzed（例如：某些ID字段，或者可枚举的字段等等），需要全文搜索的字段要设置成analyzed（例如：日志详情，或者具体错误信息等等），否则在Kibana全文搜索的时候搜索结果是正确的，但是没有高亮，就是因为全文搜索默认搜索的是_all字段，高亮结果返回却是在_source字段中。还有Kibana的全文搜索默认是搜索的_all字段，需要在ES创建mapping的时候设置_all开启状态。</p>
<p>Highlight高亮不能应用在非String类型的字段上，必须把integer，long等非String类型的字段转化成String类型来创建索引，这样这些字段才能够被高亮搜索。</p>
<p>还有就是记得每次修改完ES Mapping文件要刷新Kibana中的索引</p>
<p>最终修改后的ES Mapping如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;mappings&quot;: &#123;</div><div class="line">    &quot;_default_&quot;: &#123;</div><div class="line">      &quot;_all&quot;: &#123;</div><div class="line">        &quot;enabled&quot;: true</div><div class="line">      &#125;,</div><div class="line">      &quot;dynamic_templates&quot;: [</div><div class="line">        &#123;</div><div class="line">          &quot;my_template&quot;: &#123;</div><div class="line">            &quot;match_mapping_type&quot;: &quot;string&quot;,</div><div class="line">            &quot;mapping&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;raw&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      ]</div><div class="line">    &#125;,</div><div class="line">    &quot;api&quot;: &#123;</div><div class="line">      &quot;properties&quot;: &#123;</div><div class="line">        &quot;timestamp&quot;: &#123;</div><div class="line">          &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot;,</div><div class="line">          &quot;type&quot;: &quot;date&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;message&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;,</div><div class="line">          &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;level&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;,</div><div class="line">          &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;host&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;,</div><div class="line">          &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;logs&quot;: &#123;</div><div class="line">          &quot;properties&quot;: &#123;</div><div class="line">            &quot;uid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;status&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;did&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;as_string&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;,</div><div class="line">            &quot;device-id&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;device_id&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;errorMsg&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;raw&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;,</div><div class="line">            &quot;rpid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;url&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;raw&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;,</div><div class="line">            &quot;errorStatus&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;as_string&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;,</div><div class="line">            &quot;ip&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;timestamp&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;hb_uid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;as_string&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;,</div><div class="line">            &quot;duid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;request&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;raw&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;,</div><div class="line">            &quot;name&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;raw&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;,</div><div class="line">            &quot;errorCode&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;ua&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;raw&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;,</div><div class="line">            &quot;server_timestamp&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;</div><div class="line">            &#125;,</div><div class="line">            &quot;bid&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;long&quot;,</div><div class="line">              &quot;fields&quot;: &#123;</div><div class="line">                &quot;as_string&quot;: &#123;</div><div class="line">                  &quot;type&quot;: &quot;string&quot;,</div><div class="line">                  &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;,</div><div class="line">        &quot;path&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;,</div><div class="line">          &quot;fields&quot;: &#123;</div><div class="line">            &quot;raw&quot;: &#123;</div><div class="line">              &quot;type&quot;: &quot;string&quot;,</div><div class="line">              &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;,</div><div class="line">        &quot;type&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;,</div><div class="line">          &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;@timestamp&quot;: &#123;</div><div class="line">          &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot;,</div><div class="line">          &quot;type&quot;: &quot;date&quot;</div><div class="line">        &#125;,</div><div class="line">        &quot;@version&quot;: &#123;</div><div class="line">          &quot;type&quot;: &quot;string&quot;,</div><div class="line">          &quot;index&quot;: &quot;not_analyzed&quot;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>name，request，path，ua，url，errorMsg虽然设置了analyzed，但是同时也需要做统计，所以在这几个字段单独加上了.raw字段，用来统计使用。</p>
<p>这里有个小技巧就是我们没有直接把long类型的字段直接转换成String类型，我们是在这个long类型的字段下创建了一个as_string字段，as_string这个字段是String类型的，并且是not_analyzed，这样Kibana在全文搜索的时候就会高亮出来long类型的字段了，实际上是高亮的long类型字段下的String字段。举例：下面是搜索一个logs.bid字段，logs.bid这个字段是long类型的，但是我们在这个字段下创建了一个logs.bid.as_string字段，实际上highlight高亮的字段也是logs.bid.as_string这个字段。</p>
<p>参考：</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/multi-fields.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/2.3/multi-fields.html</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">&quot;highlight&quot;: &#123;</div><div class="line">  &quot;logs.duid&quot;: [</div><div class="line">    &quot;@kibana-highlighted-field@wasl6@/kibana-highlighted-field@&quot;</div><div class="line">  ],\</div><div class="line">  &quot;logs.bid.as_string&quot;: [</div><div class="line">    &quot;@kibana-highlighted-field@79789714801950720@/kibana-highlighted-field@&quot;</div><div class="line">  ],</div><div class="line">  &quot;type&quot;: [</div><div class="line">    &quot;@kibana-highlighted-field@api@/kibana-highlighted-field@&quot;</div><div class="line">  ],</div><div class="line">  &quot;logs.request&quot;: [</div><div class="line">    &quot;GET /@kibana-highlighted-field@api@/kibana-highlighted-field@/hongbao/realname/info&quot;</div><div class="line">  ]</div><div class="line">&#125;</div><div class="line">...</div></pre></td></tr></table></figure>
<h3 id="Kibana查询Request"><a href="#Kibana查询Request" class="headerlink" title="Kibana查询Request"></a>Kibana查询Request</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;size&quot;: 500,</div><div class="line">  &quot;highlight&quot;: &#123;</div><div class="line">    &quot;pre_tags&quot;: [</div><div class="line">      &quot;@kibana-highlighted-field@&quot;</div><div class="line">    ],</div><div class="line">    &quot;post_tags&quot;: [</div><div class="line">      &quot;@/kibana-highlighted-field@&quot;</div><div class="line">    ],</div><div class="line">    &quot;fields&quot;: &#123;</div><div class="line">      &quot;*&quot;: &#123;&#125;</div><div class="line">    &#125;,</div><div class="line">    &quot;require_field_match&quot;: false,</div><div class="line">    &quot;fragment_size&quot;: 2147483647</div><div class="line">  &#125;,</div><div class="line">  &quot;query&quot;: &#123;</div><div class="line">    &quot;filtered&quot;: &#123;</div><div class="line">      &quot;query&quot;: &#123;</div><div class="line">        &quot;query_string&quot;: &#123;</div><div class="line">          &quot;query&quot;: &quot;keyword&quot;,</div><div class="line">          &quot;analyze_wildcard&quot;: true</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;,</div><div class="line">  &quot;fields&quot;: [</div><div class="line">    &quot;*&quot;,</div><div class="line">    &quot;_source&quot;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里Kibana全文搜索使用的是query_string语法，下面是常用的参数</p>
<ul>
<li>query：可以使用简单的Lucene语法</li>
<li>default_field：指定默认查询哪些字段，默认值是_all</li>
<li>analyze_wildcard：默认情况下，通配符查询是不会被分词的，如果该属性设置为true，将尽力去分词。（原文：By default, wildcards terms in a query string are not analyzed. By setting this value to true, a best effort will be made to analyze those as well.）</li>
</ul>
<p>下面是ES官方文档的相关说明</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Wildcards</div><div class="line">Wildcard searches can be run on individual terms, using ? to replace a single character, and * to replace zero or more characters:</div><div class="line"></div><div class="line">qu?ck bro*</div><div class="line">Be aware that wildcard queries can use an enormous amount of memory and perform very badly — just think how many terms need to be queried to match the query string &quot;a* b* c*&quot;.</div><div class="line"></div><div class="line">Warning</div><div class="line">Allowing a wildcard at the beginning of a word (eg &quot;*ing&quot;) is particularly heavy, because all terms in the index need to be examined, just in case they match. Leading wildcards can be disabled by setting allow_leading_wildcard to false.</div><div class="line"></div><div class="line">Wildcarded terms are not analyzed by default — they are lowercased (lowercase_expanded_terms defaults to true) but no further analysis is done, mainly because it is impossible to accurately analyze a word that is missing some of its letters. However, by setting analyze_wildcard to true, an attempt will be made to analyze wildcarded words before searching the term list for matching terms.</div></pre></td></tr></table></figure>
<h3 id="遇到的问题和解决方法"><a href="#遇到的问题和解决方法" class="headerlink" title="遇到的问题和解决方法"></a>遇到的问题和解决方法</h3><p>Q : 公司之前的架构是Flume + KafKa + Logstash + ES，但是使用Flume作为Shipper端添加相关的type、host、path等Header字段会按照StringSerializer序列化到Kafka中，但是Logstash无法解析Flume序列化后的Header字段<br>A : 将Shipper端换成Logstash，保证Shipper和Indexer用同样的序列化和反序列化方式。</p>
<p>Q : 最近部署了线上的logstash，发现一个问题ES的host字段为0.0.0.0，这个host是Logstash Shipper端自动添加的Header字段。<br>A : 后来发现是因为/etc/hosts的IP、主机名和hostname不一致导致的, 只要设置成一致就可以解决这个问题了。</p>
<p>参考文章：</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/logstash/2.3/plugins-inputs-kafka.html" target="_blank" rel="external">https://www.elastic.co/guide/en/logstash/2.3/plugins-inputs-kafka.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/2.3/plugins-outputs-kafka.html" target="_blank" rel="external">https://www.elastic.co/guide/en/logstash/2.3/plugins-outputs-kafka.html</a></li>
<li><a href="http://udn.yyuap.com/doc/logstash-best-practice-cn/output/elasticsearch.html" target="_blank" rel="external">http://udn.yyuap.com/doc/logstash-best-practice-cn/output/elasticsearch.html</a></li>
<li><a href="http://elasticsearch.cn/article/23" target="_blank" rel="external">http://elasticsearch.cn/article/23</a></li>
<li><a href="http://stackoverflow.com/questions/36715849/numeric-and-date-fields-highlighting-in-elastic-search" target="_blank" rel="external">http://stackoverflow.com/questions/36715849/numeric-and-date-fields-highlighting-in-elastic-search</a></li>
<li><a href="http://stackoverflow.com/questions/36715688/indexing-numeric-field-as-both-int-and-string-in-elastic-search" target="_blank" rel="external">http://stackoverflow.com/questions/36715688/indexing-numeric-field-as-both-int-and-string-in-elastic-search</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-query-string-query.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-query-string-query.html</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/multi-fields.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/2.3/multi-fields.html</a></li>
<li><a href="http://www.v1en.com/2016/03/31/logstash-host-field-is-0-0-0-0/" target="_blank" rel="external">http://www.v1en.com/2016/03/31/logstash-host-field-is-0-0-0-0/</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logstash/">Logstash</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Storm/Storm学习（四）Storm清洗数据实例" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/15/Storm/Storm学习（四）Storm清洗数据实例/" class="article-date">
  	<time datetime="2016-11-15T08:47:32.000Z" itemprop="datePublished">2016-11-15</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/15/Storm/Storm学习（四）Storm清洗数据实例/">Storm学习（四）Storm清洗数据实例</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>之前学习Hadoop的时候，使用MapReduce做了一个track.log日志文件的数据清洗实例，按照我们的需要提取出有用的日志数据，这里我们使用Storm来实现同样的功能。</p>
<p>具体源代码请关注下面的GitHub项目</p>
<ul>
<li><a href="http://github.com/birdben/birdHadoop">http://github.com/birdben/birdHadoop</a></li>
</ul>
<h3 id="数据清洗的目标"><a href="#数据清洗的目标" class="headerlink" title="数据清洗的目标"></a>数据清洗的目标</h3><p>这里我们期望将下面的track.log日志文件内容转化一下，将logs外层结构去掉，提起出来logs的内层数据，并且将原来的logs下的数组转换成多条新的日志记录。</p>
<h5 id="track-log日志文件"><a href="#track-log日志文件" class="headerlink" title="track.log日志文件"></a>track.log日志文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475114816071&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829286&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.286Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475114827206&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840425&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.425Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915077351&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915090579&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:50.579Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914816133&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829332&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.332Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914827284&quot;,&quot;rpid&quot;:&quot;65351516503932936&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840498&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.499Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915077585&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915090789&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:50.789Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475913832349&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475913845544&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:04:05.544Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915080561&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915093792&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:53.792Z&quot;&#125;</div></pre></td></tr></table></figure>
<h5 id="期望清洗之后的文件内容如下"><a href="#期望清洗之后的文件内容如下" class="headerlink" title="期望清洗之后的文件内容如下"></a>期望清洗之后的文件内容如下</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475915093792&quot;,&quot;timestamp&quot;:1475915080561,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475913845544&quot;,&quot;timestamp&quot;:1475913832349,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475912715001&quot;,&quot;timestamp&quot;:1475912701768,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475915090789&quot;,&quot;timestamp&quot;:1475915077585,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932936&quot;,&quot;server_timestamp&quot;:&quot;1475914840498&quot;,&quot;timestamp&quot;:1475914827284,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475914829332&quot;,&quot;timestamp&quot;:1475914816133,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475915090579&quot;,&quot;timestamp&quot;:1475915077351,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475914840425&quot;,&quot;timestamp&quot;:1475114827206,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475914829286&quot;,&quot;timestamp&quot;:1475114816071,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;</div></pre></td></tr></table></figure>
<h3 id="AdLog实例程序"><a href="#AdLog实例程序" class="headerlink" title="AdLog实例程序"></a>AdLog实例程序</h3><p>实例程序请参考GitHub上的源代码</p>
<ul>
<li><a href="http://github.com/birdben/birdHadoop">http://github.com/birdben/birdHadoop</a></li>
</ul>
<p>这里我们使用Maven来打包构建项目，同之前的MapReduce相关实例是一个项目，我们用了分开在不同的package中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 进入项目根目录下</div><div class="line">$ cd /Users/yunyu/workspace_git/birdHadoop</div><div class="line"># 编译打包</div><div class="line">$ mvn clean package</div><div class="line"># 执行我们的Shell脚本</div><div class="line">$ sh scripts/storm/runAdLog.sh</div></pre></td></tr></table></figure>
<h4 id="runAdLog-sh脚本文件"><a href="#runAdLog-sh脚本文件" class="headerlink" title="runAdLog.sh脚本文件"></a>runAdLog.sh脚本文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line">local_path=~/Downloads/birdHadoop</div><div class="line">local_inputfile_path=$local_path/inputfile/AdLog</div><div class="line">local_outputfile_path=$local_path/outputfile/AdLog</div><div class="line">main_class=com.birdben.storm.adlog.AdLogMain</div><div class="line">if [ -f $local_inputfile_path/track.log.bak ]; then</div><div class="line">	# 如果本地bak文件存在，就重命名去掉bak</div><div class="line">	echo &quot;正在重命名$local_inputfile_path/track.log.bak文件&quot;</div><div class="line">	mv $local_inputfile_path/track.log.bak $local_inputfile_path/track.log</div><div class="line">fi</div><div class="line">if [ ! -d $local_outputfile_path ]; then</div><div class="line">	# 如果本地文件目录不存在，就自动创建</div><div class="line">	echo &quot;自动创建$outputfile_path目录&quot;</div><div class="line">	mkdir -p $local_outputfile_path</div><div class="line">else</div><div class="line">	# 如果本地文件已经存在，就删除</div><div class="line">	echo &quot;删除$local_outputfile_path/*目录下的所有文件&quot;</div><div class="line">	rm -rf $local_outputfile_path/*</div><div class="line">fi</div><div class="line"># 需要在Maven的pom.xml文件中指定jar的入口类</div><div class="line">echo &quot;开始执行birdHadoop.jar...&quot;</div><div class="line">storm jar $local_path/target/birdHadoop.jar $main_class $local_inputfile_path $local_outputfile_path</div><div class="line">echo &quot;结束执行birdHadoop.jar...&quot;</div></pre></td></tr></table></figure>
<p>注意：这里使用的集群模式运行的，inputfile文件需要上传到Storm的Supervisor机器上，否则Storm运行的时候会找不到inputfile文件。</p>
<p>执行Shell脚本之后，可以在Storm UI中查看到Topology Summary中多了一个AdLog Topology，Topology Id是AdLog-1-1479198597，我们找到Supervisor机器上的log日志（${STORM_HOME}/logs），该日志目录下会根据Topology Id生成对应的日志文件如下：</p>
<ul>
<li>AdLog-1-1479198597-worker-6703.log</li>
<li>AdLog-1-1479198597-worker-6703.log.err</li>
<li>AdLog-1-1479198597-worker-6703.log.metrics.log</li>
<li>AdLog-1-1479198597-worker-6703.log.out</li>
</ul>
<p>我们可以查看一下AdLog-1-1479198597-worker-6703.log日志，我们代码中的日志输出都在这个日志文件中，可以看到Storm集群读取我们指定的inputfile，并且按照指定方式提取我们需要的日志。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ vi AdLog-1-1479198597-worker-6703.log</div><div class="line">...2016-11-15 00:30:04.316 b.s.d.executor [INFO] Preparing bolt adlog-counter:(2)2016-11-15 00:30:04.318 STDIO [INFO] AdLogCounterBolt prepare out start2016-11-15 00:30:04.319 b.s.d.executor [INFO] Prepared bolt adlog-counter:(2)2016-11-15 00:30:04.338 b.s.d.executor [INFO] Preparing bolt adlog-parser:(3)2016-11-15 00:30:04.340 b.s.d.executor [INFO] Prepared bolt adlog-parser:(3)2016-11-15 00:30:04.340 b.s.d.executor [INFO] Processing received message FOR 3 TUPLE: source: adlog-reader:4, stream: default, id: &#123;&#125;, [&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475114816071&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829286&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.286Z&quot;&#125;]2016-11-15 00:30:04.341 STDIO [INFO] AdLogParserBolt execute out start2016-11-15 00:30:04.356 STDIO [ERROR] SLF4J: Detected both log4j-over-slf4j.jar AND slf4j-log4j12.jar on the class path, preempting StackOverflowError.2016-11-15 00:30:04.356 STDIO [ERROR] SLF4J: See also http://www.slf4j.org/codes.html#log4jDelegationLoop for more details.2016-11-15 00:30:04.361 STDIO [INFO] birdben AdLogParser out start2016-11-15 00:30:04.367 STDIO [ERROR] Nov 15, 2016 12:30:04 AM com.birdben.mapreduce.adlog.parser.AdLogParser convertLogToAdINFO: birdben AdLogParser logger start2016-11-15 00:30:04.435 STDIO [ERROR] Nov 15, 2016 12:30:04 AM com.birdben.mapreduce.adlog.parser.AdLogParser convertLogToAdINFO: convertLogToAd name:birdben.ad.open_hb2016-11-15 00:30:04.452 b.s.d.task [INFO] Emitting: adlog-parser default [&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475914829286&quot;,&quot;timestamp&quot;:1475114816071,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;]2016-11-15 00:30:04.453 b.s.d.executor [INFO] TRANSFERING tuple TASK: 2 TUPLE: source: adlog-parser:3, stream: default, id: &#123;&#125;, [&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475914829286&quot;,&quot;timestamp&quot;:1475114816071,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;]2016-11-15 00:30:04.454 STDIO [INFO] AdLogParserBolt execute out end2016-11-15 00:30:04.454 b.s.d.executor [INFO] Processing received message FOR 2 TUPLE: source: adlog-parser:3, stream: default, id: &#123;&#125;, [&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475914829286&quot;,&quot;timestamp&quot;:1475114816071,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;]2016-11-15 00:30:04.454 STDIO [INFO] AdLogCounterBolt execute out start2016-11-15 00:30:04.454 b.s.d.executor [INFO] BOLT ack TASK: 3 TIME:  TUPLE: source: adlog-reader:4, stream: default, id: &#123;&#125;, [&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475114816071&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829286&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.286Z&quot;&#125;]2016-11-15 00:30:04.454 STDIO [INFO] AdLogCounterBolt execute out end2016-11-15 00:30:04.455 b.s.d.executor [INFO] Execute done TUPLE source: adlog-reader:4, stream: default, id: &#123;&#125;, [&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475114816071&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829286&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.286Z&quot;&#125;] TASK: 3 DELTA:2016-11-15 00:30:04.455 b.s.d.executor [INFO] BOLT ack TASK: 2 TIME: 1 TUPLE: source: adlog-parser:3, stream: default, id: &#123;&#125;, [&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475914829286&quot;,&quot;timestamp&quot;:1475114816071,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;]2016-11-15 00:30:04.455 b.s.d.executor [INFO] Processing received message FOR 3 TUPLE: source: adlog-reader:4, stream: default, id: &#123;&#125;, [&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475114827206&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840425&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.425Z&quot;&#125;]2016-11-15 00:30:04.455 STDIO [INFO] AdLogParserBolt execute out start2016-11-15 00:30:04.455 STDIO [INFO] birdben AdLogParser out start</div><div class="line">...</div></pre></td></tr></table></figure>
<p>Storm数据清洗运行成功后，需要像之前一样kill掉AdLog Topology之后才会调用cleanup方法将清洗后的日志输出到outputfile文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ storm kill AdLogRunning: /usr/local/java/bin/java -client -Ddaemon.name= -Dstorm.options= -Dstorm.home=/data/storm-0.10.2 -Dstorm.log.dir=/data/storm-0.10.2/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /data/storm-0.10.2/lib/storm-core-0.10.2.jar:/data/storm-0.10.2/lib/slf4j-api-1.7.7.jar:/data/storm-0.10.2/lib/clojure-1.6.0.jar:/data/storm-0.10.2/lib/disruptor-2.10.4.jar:/data/storm-0.10.2/lib/servlet-api-2.5.jar:/data/storm-0.10.2/lib/log4j-api-2.1.jar:/data/storm-0.10.2/lib/log4j-core-2.1.jar:/data/storm-0.10.2/lib/minlog-1.2.jar:/data/storm-0.10.2/lib/reflectasm-1.07-shaded.jar:/data/storm-0.10.2/lib/log4j-over-slf4j-1.6.6.jar:/data/storm-0.10.2/lib/asm-4.0.jar:/data/storm-0.10.2/lib/hadoop-auth-2.4.0.jar:/data/storm-0.10.2/lib/kryo-2.21.jar:/data/storm-0.10.2/lib/log4j-slf4j-impl-2.1.jar:/usr/local/storm/conf:/data/storm-0.10.2/bin backtype.storm.command.kill_topology AdLog1331 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources1401 [main] INFO  b.s.u.Utils - Using storm.yaml from resources1954 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources1971 [main] INFO  b.s.u.Utils - Using storm.yaml from resources1987 [main] INFO  b.s.thrift - Connecting to Nimbus at hadoop1:6627 as user: 1987 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources2024 [main] INFO  b.s.u.Utils - Using storm.yaml from resources2045 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [2000] the maxSleepTimeMs [60000] the maxRetries [5]2094 [main] INFO  b.s.c.kill-topology - Killed topology: AdLog</div></pre></td></tr></table></figure>
<p>查看一下我们所期望的结果文件的内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cat outputfile/AdLog/output_AdLog &#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475914829286&quot;,&quot;timestamp&quot;:1475114816071,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475914840425&quot;,&quot;timestamp&quot;:1475114827206,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475915090579&quot;,&quot;timestamp&quot;:1475915077351,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475914829332&quot;,&quot;timestamp&quot;:1475914816133,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932936&quot;,&quot;server_timestamp&quot;:&quot;1475914840498&quot;,&quot;timestamp&quot;:1475914827284,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932932&quot;,&quot;server_timestamp&quot;:&quot;1475915090789&quot;,&quot;timestamp&quot;:1475915077585,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932930&quot;,&quot;server_timestamp&quot;:&quot;1475912715001&quot;,&quot;timestamp&quot;:1475912701768,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932934&quot;,&quot;server_timestamp&quot;:&quot;1475913845544&quot;,&quot;timestamp&quot;:1475913832349,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;&#123;&quot;bid&quot;:&quot;0&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hb_uid&quot;:&quot;0&quot;,&quot;rpid&quot;:&quot;65351516503932928&quot;,&quot;server_timestamp&quot;:&quot;1475915093792&quot;,&quot;timestamp&quot;:1475915080561,&quot;ua&quot;:&quot;&quot;,&quot;uid&quot;:&quot;0&quot;&#125;</div></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storm/">Storm</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Storm/">Storm</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Storm/Storm学习（二）Storm架构及原理（转）" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/07/Storm/Storm学习（二）Storm架构及原理（转）/" class="article-date">
  	<time datetime="2016-11-07T11:34:53.000Z" itemprop="datePublished">2016-11-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/07/Storm/Storm学习（二）Storm架构及原理（转）/">Storm学习（二）Storm架构及原理（转）</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Storm集群环境已经搭建好了，但是在翻译官网的Getting Started的时候感觉有很多概念都不是太理解，所以这篇重点研究一下Storm架构及原理</p>
<p>以下内容转载自：<a href="https://my.oschina.net/leejun2005/blog/147607?fromerr=NjSkGlQI" target="_blank" rel="external">https://my.oschina.net/leejun2005/blog/147607?fromerr=NjSkGlQI</a></p>
<h3 id="Storm-与传统的大数据"><a href="#Storm-与传统的大数据" class="headerlink" title="Storm 与传统的大数据"></a>Storm 与传统的大数据</h3><p>Storm 与其他大数据解决方案的不同之处在于它的处理方式。Hadoop 在本质上是一个批处理系统。数据被引入 Hadoop 文件系统 (HDFS) 并分发到各个节点进行处理。当处理完成时，结果数据返回到 HDFS 供始发者使用。Storm 支持创建拓扑结构来转换没有终点的数据流。不同于 Hadoop 作业，这些转换从不停止，它们会持续处理到达的数据。</p>
<p>但 Storm 不只是一个传统的大数据分析系统：它是复杂事件处理 (CEP) 系统的一个示例。CEP 系统通常分类为计算和面向检测，其中每个系统都可通过用户定义的算法在 Storm 中实现。举例而言，CEP 可用于识别事件洪流中有意义的事件，然后实时地处理这些事件。</p>
<h3 id="Storm的基本组件"><a href="#Storm的基本组件" class="headerlink" title="Storm的基本组件"></a>Storm的基本组件</h3><p>Storm的集群表面上看和Hadoop的集群非常像。但是在Hadoop上面你运行的是MapReduce的Job, 而在Storm上面你运行的是Topology。它们是非常不一样的 — 一个关键的区别是： 一个MapReduce Job最终会结束， 而一个Topology运永远运行（除非你显式的杀掉他）。</p>
<p>在Storm的集群里面有两种节点： 控制节点(master node)和工作节点(worker node)。控制节点上面运行一个后台程序： Nimbus， 它的作用类似Hadoop里面的JobTracker。Nimbus负责在集群里面分布代码，分配工作给机器， 并且监控状态。</p>
<p>每一个工作节点上面运行一个叫做Supervisor的节点（类似 TaskTracker）。Supervisor会监听分配给它那台机器的工作，根据需要 启动/关闭工作进程。每一个工作进程执行一个Topology（类似 Job）的一个子集；一个运行的Topology由运行在很多机器上的很多工作进程 Worker（类似 Child）组成</p>
<h5 id="Storm-Topology结构"><a href="#Storm-Topology结构" class="headerlink" title="Storm Topology结构"></a>Storm Topology结构</h5><p><img src="http://img.blog.csdn.net/20161107201042412?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Storm Flow"></p>
<h5 id="Storm-VS-MapReduce"><a href="#Storm-VS-MapReduce" class="headerlink" title="Storm VS MapReduce"></a>Storm VS MapReduce</h5><p><img src="http://img.blog.csdn.net/20161107201112288?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Storm VS MapReduce"></p>
<p>Nimbus和Supervisor之间的所有协调工作都是通过一个Zookeeper集群来完成。并且，Nimbus进程和Supervisor都是快速失败（fail-fast)和无状态的。所有的状态要么在Zookeeper里面， 要么在本地磁盘上。这也就意味着你可以用kill -9来杀死Nimbus和Supervisor进程， 然后再重启它们，它们可以继续工作， 就好像什么都没有发生过似的。这个设计使得Storm不可思议的稳定。</p>
<h3 id="Topologies"><a href="#Topologies" class="headerlink" title="Topologies"></a>Topologies</h3><p>为了在Storm上面做实时计算，你要去建立一些Topologies。一个Topology就是一个计算节点所组成的图。Topology里面的每个处理节点都包含处理逻辑，而节点之间的连接则表示数据流动的方向。</p>
<p>运行一个Topology是很简单的。首先，把你所有的代码以及所依赖的jar打进一个jar包。然后运行类似下面的这个命令。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">storm jar all-your-code.jar backtype.storm.MyTopology arg1 arg2</div></pre></td></tr></table></figure>
<p>这个命令会运行主类: backtype.strom.MyTopology, 参数是arg1, arg2。这个类的main函数定义这个Topology并且把它提交给Nimbus。storm jar负责连接到Nimbus并且上传jar文件。</p>
<p>因为Topology的定义其实就是一个Thrift结构并且Nimbus就是一个Thrift服务， 有可以用任何语言创建并且提交Topology。上面的方面是用JVM-based语言提交的最简单的方法, 看一下文章: 在生产集群上运行Topology去看看怎么启动以及停止Topologies。</p>
<h3 id="Stream"><a href="#Stream" class="headerlink" title="Stream"></a>Stream</h3><p>Stream是Storm里面的关键抽象。一个Stream是一个没有边界的Tuple序列。Storm提供一些原语来分布式地、可靠地把一个Stream传输进一个新的Stream。比如： 你可以把一个Tweets流传输到热门话题的流。</p>
<p>Storm提供的最基本的处理Stream的原语是Spout和Bolt。你可以实现Spout和Bolt对应的接口以处理你的应用的逻辑。</p>
<h4 id="Spout"><a href="#Spout" class="headerlink" title="Spout"></a>Spout</h4><p>Spout的流的源头。比如一个Spout可能从Kestrel队列里面读取消息并且把这些消息发射成一个流。又比如一个Spout可以调用Twitter的一个api并且把返回的Tweets发射成一个流。</p>
<p>通常Spout会从外部数据源（队列、数据库等）读取数据，然后封装成Tuple形式，之后发送到Stream中。Spout是一个主动的角色，在接口内部有个nextTuple函数，Storm框架会不停的调用该函数。</p>
<p><img src="http://img.blog.csdn.net/20161107202111762?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Spout"></p>
<h4 id="Bolt"><a href="#Bolt" class="headerlink" title="Bolt"></a>Bolt</h4><p>Bolt可以接收任意多个输入Stream，作一些处理，有些Bolt可能还会发射一些新的Stream。一些复杂的流转换，比如从一些Tweet里面计算出热门话题，需要多个步骤，从而也就需要多个Bolt。Bolt可以做任何事情: 运行函数，过滤Tuple，做一些聚合，做一些合并以及访问数据库等等。</p>
<p>Bolt处理输入的Stream，并产生新的输出Stream。Bolt可以执行过滤、函数操作、Join、操作数据库等任何操作。Bolt是一个被动的角色，其接口中有一个execute(Tuple input)方法，在接收到消息之后会调用此函数，用户可以在此方法中执行自己的处理逻辑。</p>
<p><img src="http://img.blog.csdn.net/20161107202143477?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Bolt"></p>
<p>Spout和Bolt所组成一个网络会被打包成Topology，Topology是Storm里面最高一级的抽象（类Job）， 你可以把Topology提交给Storm的集群来运行。Topology的结构在Topology那一段已经说过了，这里就不再赘述了。</p>
<p><img src="http://img.blog.csdn.net/20161107202222212?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Topology结构"></p>
<p>Topology里面的每一个节点都是并行运行的。在你的Topology里面，你可以指定每个节点的并行度， Storm则会在集群里面分配那么多线程来同时计算。</p>
<p>一个Topology会一直运行直到你显式停止它。Storm自动重新分配一些运行失败的任务，并且Storm保证你不会有数据丢失，即使在一些机器意外停机并且消息被丢掉的情况下。</p>
<h3 id="数据模型-Data-Model"><a href="#数据模型-Data-Model" class="headerlink" title="数据模型(Data Model)"></a>数据模型(Data Model)</h3><p>Storm使用Tuple来作为它的数据模型。每个Tuple是一堆值，每个值有一个名字，并且每个值可以是任何类型，在我的理解里面一个Tuple可以看作一个没有方法的Java对象。总体来看，Storm支持所有的基本类型、字符串以及字节数组作为Tuple的值类型。你也可以使用你自己定义的类型来作为值类型，只要你实现对应的序列化器(Serializer)。</p>
<p>一个Tuple代表数据流中的一个基本的处理单元，例如一条Cookie日志，它可以包含多个Field，每个Field表示一个属性。</p>
<p>Tuple本来应该是一个Key-Value的Map，由于各个组件间传递的Tuple的字段名称已经事先定义好了，所以Tuple只需要按序填入各个Value，所以就是一个Value List。</p>
<p>一个没有边界的、源源不断的、连续的Tuple序列就组成了Stream。</p>
<p>Topology里面的每个节点必须定义它要发射的Tuple的每个字段。 比如下面这个Bolt定义它所发射的Tuple包含两个字段，类型分别是: double和triple。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">publicclassDoubleAndTripleBoltimplementsIRichBolt &#123;</div><div class="line">    </div><div class="line">    privateOutputCollectorBase _collector;</div><div class="line"> </div><div class="line">    @Override</div><div class="line">    publicvoidprepare(Map conf, TopologyContext context, OutputCollectorBase collector) &#123;</div><div class="line">        _collector = collector;</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    @Override</div><div class="line">    publicvoidexecute(Tuple input) &#123;</div><div class="line">        intval = input.getInteger(0);</div><div class="line">        _collector.emit(input,newValues(val*2, val*3));</div><div class="line">        _collector.ack(input);</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    @Override</div><div class="line">    publicvoidcleanup() &#123;</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    @Override</div><div class="line">    publicvoiddeclareOutputFields(OutputFieldsDeclarer declarer) &#123;</div><div class="line">        declarer.declare(newFields(&quot;double&quot;,&quot;triple&quot;));</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>declareOutputFields方法定义要输出的字段 ： [“double”, “triple”]。这个Bolt的其它部分我们接下来会解释。</p>
<h3 id="一个简单的Topology"><a href="#一个简单的Topology" class="headerlink" title="一个简单的Topology"></a>一个简单的Topology</h3><p>让我们来看一个简单的Topology的例子， 我们看一下storm-starter里面的ExclamationTopology:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">TopologyBuilder builder =newTopologyBuilder();</div><div class="line">builder.setSpout(1,newTestWordSpout(),10);</div><div class="line">builder.setBolt(2,newExclamationBolt(),3)</div><div class="line">        .shuffleGrouping(1);</div><div class="line">builder.setBolt(3,newExclamationBolt(),2)</div><div class="line">        .shuffleGrouping(2);</div></pre></td></tr></table></figure>
<p>这个Topology包含一个Spout和两个Bolt。Spout发射单词，每个Bolt在每个单词后面加个”!!!”。这三个节点被排成一条线: Spout发射单词给第一个Bolt，第一个Bolt然后把处理好的单词发射给第二个Bolt。如果Spout发射的单词是[“bob”]和[“john”], 那么第二个Bolt会发射[“bolt!!!!!!”]和[“john!!!!!!”]出来。</p>
<p>我们使用setSpout和setBolt来定义Topology里面的节点。这些方法接收我们指定的一个id，一个包含处理逻辑的对象(Spout或者Bolt), 以及你所需要的并行度。</p>
<p>这个包含处理的对象如果是Spout那么要实现IRichSpout的接口，如果是Bolt，那么就要实现IRichBolt接口。</p>
<p>最后一个指定并行度的参数是可选的。它表示集群里面需要多少个Thread来一起执行这个节点。如果你忽略它那么Storm会分配一个线程来执行这个节点。</p>
<p>setBolt方法返回一个InputDeclarer对象，这个对象是用来定义Bolt的输入。这里第一个Bolt声明它要读取Spout所发射的所有的Tuple —— 使用shuffle grouping。而第二个Bolt声明它读取第一个Bolt所发射的Tuple。shuffle grouping表示所有的Tuple会被随机的分发给Bolt的所有Task。给Task分发Tuple的策略有很多种，后面会介绍。</p>
<p>如果你想第二个Bolt读取Spout和第一个Bolt所发射的所有的Tuple， 那么你应该这样定义第二个Bolt:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">builder.setBolt(3,newExclamationBolt(),5)</div><div class="line">            .shuffleGrouping(1)</div><div class="line">            .shuffleGrouping(2);</div></pre></td></tr></table></figure>
<p>让我们深入地看一下这个Topology里面的Spout和Bolt是怎么实现的。Spout负责发射新的Tuple到这个Topology里面来。TestWordSpout从[“nathan”, “mike”, “jackson”, “golda”, “bertels”]里面随机选择一个单词发射出来。TestWordSpout里面的nextTuple()方法是这样定义的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">publicvoidnextTuple() &#123;</div><div class="line">    Utils.sleep(100);</div><div class="line">    finalString[] words =newString[] &#123;&quot;nathan&quot;,&quot;mike&quot;,</div><div class="line">                     &quot;jackson&quot;,&quot;golda&quot;,&quot;bertels&quot;&#125;;</div><div class="line">    finalRandom rand =newRandom();</div><div class="line">    finalString word = words[rand.nextInt(words.length)];</div><div class="line">    _collector.emit(newValues(word));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可以看到，实现很简单。</p>
<p>ExclamationBolt把”!!!”拼接到输入tuple后面。我们来看下ExclamationBolt的完整实现。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">publicstaticclassExclamationBoltimplementsIRichBolt &#123;</div><div class="line">    OutputCollector _collector;</div><div class="line"> </div><div class="line">    publicvoidprepare(Map conf, TopologyContext context,</div><div class="line">                        OutputCollector collector) &#123;</div><div class="line">        _collector = collector;</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    publicvoidexecute(Tuple tuple) &#123;</div><div class="line">        _collector.emit(tuple,newValues(tuple.getString(0) +&quot;!!!&quot;));</div><div class="line">        _collector.ack(tuple);</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    publicvoidcleanup() &#123;</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    publicvoiddeclareOutputFields(OutputFieldsDeclarer declarer) &#123;</div><div class="line">        declarer.declare(newFields(&quot;word&quot;));</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>prepare方法提供给Bolt一个Outputcollector用来发射tuple。Bolt可以在任何时候发射Tuple —— 在prepare, execute或者cleanup方法里面, 或者甚至在另一个线程里面异步发射。这里prepare方法只是简单地把OutputCollector作为一个类字段保存下来给后面execute方法使用。</p>
<p>execute方法从Bolt的一个输入接收Tuple(一个Bolt可能有多个输入源)。ExclamationBolt获取Tuple的第一个字段，加上”!!!”之后再发射出去。如果一个Bolt有多个输入源，你可以通过调用Tuple#getSourceComponent方法来知道它是来自哪个输入源的。</p>
<p>execute方法里面还有其它一些事情值得一提：输入Tuple被作为emit方法的第一个参数，并且输入Tuple在最后一行被ack。这些呢都是Storm可靠性API的一部分，后面会解释。</p>
<p>cleanup方法在Bolt被关闭的时候调用，它应该清理所有被打开的资源。但是集群不保证这个方法一定会被执行。比如执行Task的机器down掉了，那么根本就没有办法来调用那个方法。cleanup设计的时候是被用来在local mode的时候才被调用(也就是说在一个进程里面模拟整个storm集群), 并且你想在关闭一些Topology的时候避免资源泄漏。</p>
<p>最后，declareOutputFields定义一个叫做”word”的字段的Tuple。</p>
<p>以local mode运行ExclamationTopology<br>让我们看看怎么以local mode运行ExclamationToplogy。</p>
<p>Storm的运行有两种模式: 本地模式和分布式模式。在本地模式中，Storm用一个进程里面的线程来模拟所有的Spout和Bolt。本地模式对开发和测试来说比较有用。你运行storm-starter里面的Topology的时候它们就是以本地模式运行的，你可以看到Topology里面的每一个组件在发射什么消息。</p>
<p>在分布式模式下，Storm由一堆机器组成。当你提交Topology给master的时候，你同时也把Topology的代码提交了。master负责分发你的代码并且负责给你的Topolgoy分配工作进程。如果一个工作进程挂掉了， master节点会把认为重新分配到其它节点。关于如何在一个集群上面运行Topology，你可以看看Running topologies on a production cluster文章。</p>
<p>下面是以本地模式运行ExclamationTopology的代码:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Config conf =newConfig();</div><div class="line">conf.setDebug(true);</div><div class="line">conf.setNumWorkers(2);</div><div class="line"> </div><div class="line">LocalCluster cluster =newLocalCluster();</div><div class="line">cluster.submitTopology(&quot;test&quot;, conf, builder.createTopology());</div><div class="line">Utils.sleep(10000);</div><div class="line">cluster.killTopology(&quot;test&quot;);</div><div class="line">cluster.shutdown();</div></pre></td></tr></table></figure>
<p>首先， 这个代码定义通过定义一个LocalCluster对象来定义一个进程内的集群。提交Topology给这个虚拟的集群和提交Topology给分布式集群是一样的。通过调用submitTopology方法来提交Topology， 它接受三个参数：要运行的Topology的名字，一个配置对象以及要运行的Topology本身。</p>
<p>Topology的名字是用来唯一区别一个Topology的，这样你然后可以用这个名字来杀死这个Topology的。前面已经说过了，你必须显式的杀掉一个Topology，否则它会一直运行。</p>
<p>Conf对象可以配置很多东西，下面两个是最常见的：</p>
<ul>
<li><p>TOPOLOGY_WORKERS(setNumWorkers) 定义你希望集群分配多少个工作进程给你来执行这个Topology。Topology里面的每个组件会被需要线程来执行。每个组件到底用多少个线程是通过setBolt和setSpout来指定的。这些线程都运行在工作进程里面。每一个工作进程包含一些节点的一些工作线程。比如，如果你指定300个线程，50个进程，那么每个工作进程里面要执行6个线程，而这6个线程可能属于不同的组件(Spout, Bolt)。你可以通过调整每个组件的并行度以及这些线程所在的进程数量来调整Topology的性能。</p>
</li>
<li><p>TOPOLOGY_DEBUG(setDebug), 当它被设置成true的话，storm会记录下每个组件所发射的每条消息。这在本地环境调试Topology很有用，但是在线上这么做的话会影响性能的。</p>
</li>
</ul>
<p>感兴趣的话可以去看看Conf对象的Javadoc去看看topology的所有配置。<br>可以看看创建一个新Storm项目去看看怎么配置开发环境以使你能够以本地模式运行Topology.</p>
<p>运行中的Topology主要由以下三个组件组成的：</p>
<ul>
<li>Worker processes（进程）</li>
<li>Executors (threads)（线程）</li>
<li>Tasks</li>
</ul>
<p><img src="http://img.blog.csdn.net/20161107205213355?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Worker"></p>
<p>Spout或者Bolt的Task个数一旦指定之后就不能改变了，而Executor的数量可以根据情况来进行动态的调整。默认情况下# executor = #tasks即一个Executor中运行着一个Task</p>
<p><img src="http://img.blog.csdn.net/20161107205238449?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Executor"></p>
<p><img src="http://img.blog.csdn.net/20161107205311189?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Topology"></p>
<p><img src="http://img.blog.csdn.net/20161107205348799?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="conf"></p>
<h3 id="流分组策略-Stream-Grouping"><a href="#流分组策略-Stream-Grouping" class="headerlink" title="流分组策略(Stream Grouping)"></a>流分组策略(Stream Grouping)</h3><p>流分组策略告诉Topology如何在两个组件之间发送Tuple。要记住，Spouts和Bolts以很多Task的形式在Topology里面同步执行。如果从Task的粒度来看一个运行的Topology，它应该是这样的:</p>
<p><img src="http://img.blog.csdn.net/20161107210310422?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Grouping"></p>
<p>从Task角度来看Topology</p>
<p>当Bolt A的一个task要发送一个Tuple给Bolt B， 它应该发送给Bolt B的哪个Task呢？</p>
<p>Stream Grouping专门回答这种问题的。在我们深入研究不同的Stream Grouping之前，让我们看一下storm-starter里面的另外一个Topology。WordCountTopology读取一些句子，输出句子里面每个单词出现的次数.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">TopologyBuilder builder =newTopologyBuilder();</div><div class="line"> </div><div class="line">builder.setSpout(1,newRandomSentenceSpout(),5);</div><div class="line">builder.setBolt(2,newSplitSentence(),8)</div><div class="line">        .shuffleGrouping(1);</div><div class="line">builder.setBolt(3,newWordCount(),12)</div><div class="line">        .fieldsGrouping(2,newFields(&quot;word&quot;));</div></pre></td></tr></table></figure>
<p>SplitSentence对于句子里面的每个单词发射一个新的Tuple, WordCount在内存里面维护一个单词-&gt;次数的Mapping，WordCount每收到一个单词，它就更新内存里面的统计状态。</p>
<p>有好几种不同的Stream Grouping:</p>
<p>最简单的Grouping是shuffle grouping, 它随机发给任何一个Task。上面例子里面RandomSentenceSpout和SplitSentence之间用的就是shuffle grouping, shuffle grouping对各个Task的Tuple分配的比较均匀。</p>
<p>一种更有趣的Grouping是fields grouping, SplitSentence和WordCount之间使用的就是fields grouping, 这种Grouping机制保证相同Field值的Tuple会去同一个Task，这对于WordCount来说非常关键，如果同一个单词不去同一个task，那么统计出来的单词次数就不对了。</p>
<p>fields grouping是Stream合并，Stream聚合以及很多其它场景的基础。在背后呢，fields grouping使用的一致性哈希来分配Tuple的。</p>
<p>还有一些其它类型的Stream Grouping. 你可以在Concepts一章里更详细的了解。</p>
<p>下面是一些常用的 “路由选择” 机制：</p>
<p>Storm的Grouping即消息的Partition机制。当一个Tuple被发送时，如何确定将它发送个某个（些）Task来处理？？</p>
<ul>
<li>ShuffleGrouping：随机选择一个Task来发送。</li>
<li>FieldGrouping：根据Tuple中Fields来做一致性hash，相同hash值的Tuple被发送到相同的Task。</li>
<li>AllGrouping：广播发送，将每一个Tuple发送到所有的Task。</li>
<li>GlobalGrouping：所有的Tuple会被发送到某个Bolt中的id最小的那个Task。</li>
<li>NoneGrouping：不关心Tuple发送给哪个Task来处理，等价于ShuffleGrouping。</li>
<li>DirectGrouping：直接将Tuple发送到指定的Task来处理。</li>
</ul>
<h3 id="使用别的语言来定义Bolt"><a href="#使用别的语言来定义Bolt" class="headerlink" title="使用别的语言来定义Bolt"></a>使用别的语言来定义Bolt</h3><p>Bolt可以使用任何语言来定义。用其它语言定义的Bolt会被当作子进程(subprocess)来执行， Storm使用JSON消息通过stdin/stdout来和这些subprocess通信。这个通信协议是一个只有100行的库，Storm团队给这些库开发了对应的Ruby, Python和Fancy版本。</p>
<p>下面是WordCountTopology里面的SplitSentence的定义:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">publicstaticclassSplitSentenceextendsShellBoltimplementsIRichBolt &#123;</div><div class="line">    publicSplitSentence() &#123;</div><div class="line">        super(&quot;python&quot;,&quot;splitsentence.py&quot;);</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    publicvoiddeclareOutputFields(OutputFieldsDeclarer declarer) &#123;</div><div class="line">        declarer.declare(newFields(&quot;word&quot;));</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>SplitSentence继承自ShellBolt并且声明这个Bolt用Python来运行，并且参数是: splitsentence.py。下面是splitsentence.py的定义:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">importstorm</div><div class="line"> </div><div class="line">classSplitSentenceBolt(storm.BasicBolt):</div><div class="line">    defprocess(self, tup):</div><div class="line">        words=tup.values[0].split(&quot; &quot;)</div><div class="line">        forwordinwords:</div><div class="line">          storm.emit([word])</div><div class="line"> </div><div class="line">SplitSentenceBolt().run()</div></pre></td></tr></table></figure>
<p>更多有关用其它语言定义Spout和Bolt的信息， 以及用其它语言来创建topology的 信息可以参见: Using non-JVM languages with Storm.</p>
<h3 id="可靠的消息处理"><a href="#可靠的消息处理" class="headerlink" title="可靠的消息处理"></a>可靠的消息处理</h3><p>在这个教程的前面，我们跳过了有关Tuple的一些特征。这些特征就是Storm的可靠性API： Storm如何保证Spout发出的每一个Tuple都被完整处理。看看《storm如何保证消息不丢失》以更深入了解storm的可靠性API.</p>
<p>Storm允许用户在Spout中发射一个新的源Tuple时为其指定一个MessageId，这个MessageId可以是任意的Object对象。多个源Tuple可以共用同一个MessageId，表示这多个源Tuple对用户来说是同一个消息单元。Storm的可靠性是指Storm会告知用户每一个消息单元是否在一个指定的时间内被完全处理。完全处理的意思是该MessageId绑定的源Tuple以及由该源Tuple衍生的所有Tuple都经过了Topology中每一个应该到达的Bolt的处理。</p>
<p><img src="http://img.blog.csdn.net/20161107210353469?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Message"></p>
<p>在Spout中由message 1绑定的tuple1和tuple2分别经过bolt1和bolt2的处理，然后生成了两个新的Tuple，并最终流向了bolt3。当bolt3处理完之后，称message 1被完全处理了。</p>
<p>Storm中的每一个Topology中都包含有一个Acker组件。Acker组件的任务就是跟踪从Spout中流出的每一个messageId所绑定的Tuple树中的所有Tuple的处理情况。如果在用户设置的最大超时时间内这些Tuple没有被完全处理，那么Acker会告诉Spout该消息处理失败，相反则会告知Spout该消息处理成功。</p>
<p>那么Acker是如何记录Tuple的处理结果呢？？</p>
<p>A xor A = 0.</p>
<p>A xor B…xor B xor A = 0，其中每一个操作数出现且仅出现两次。</p>
<p>在Spout中，Storm系统会为用户指定的MessageId生成一个对应的64位的整数，作为整个Tuple Tree的RootId。RootId会被传递给Acker以及后续的Bolt来作为该消息单元的唯一标识。同时，无论Spout还是Bolt每次新生成一个Tuple时，都会赋予该Tuple一个唯一的64位整数的Id。</p>
<p>当Spout发射完某个MessageId对应的源Tuple之后，它会告诉Acker自己发射的RootId以及生成的那些源Tuple的Id。而当Bolt处理完一个输入Tuple并产生出新的Tuple时，也会告知Acker自己处理的输入Tuple的Id以及新生成的那些Tuple的Id。Acker只需要对这些Id进行异或运算，就能判断出该RootId对应的消息单元是否成功处理完成了。</p>
<p>参考文章：</p>
<ul>
<li><a href="https://my.oschina.net/leejun2005/blog/147607?fromerr=NjSkGlQI" target="_blank" rel="external">https://my.oschina.net/leejun2005/blog/147607?fromerr=NjSkGlQI</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-twitterstorm/#list1" target="_blank" rel="external">https://www.ibm.com/developerworks/cn/opensource/os-twitterstorm/#list1</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storm/">Storm</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Storm/">Storm</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Storm/Storm学习（三）Storm的WordCount实例" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/07/Storm/Storm学习（三）Storm的WordCount实例/" class="article-date">
  	<time datetime="2016-11-07T06:57:13.000Z" itemprop="datePublished">2016-11-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/07/Storm/Storm学习（三）Storm的WordCount实例/">Storm学习（三）Storm的WordCount实例</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>之前写的Hadoop系列文章中，我们使用MapReduce实现了一个WordCount实例（就是统计一个文件中每个单词出现的次数），这里使用Storm来实现同样的功能。</p>
<p>我这里有一个Hadoop例子的项目，之前MapReduce相关的实例也放在该项目下。</p>
<ul>
<li><a href="http://github.com/birdben/birdHadoop">http://github.com/birdben/birdHadoop</a></li>
</ul>
<p>下面就是我们要统计的文件内容</p>
<h5 id="input-WordCount"><a href="#input-WordCount" class="headerlink" title="input_WordCount"></a>input_WordCount</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Hadoop Hive HBaseSpark Hive HadoopKafka HBase ES Logstash StormFlume Kafka Hadoop</div></pre></td></tr></table></figure>
<h5 id="output-WordCount"><a href="#output-WordCount" class="headerlink" title="output_WordCount"></a>output_WordCount</h5><pre><code>ES    1
Flume    1
HBase    2
Hadoop    3
Hive    2
Kafka    2
Logstash    1
Spark    1
Storm    1
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### WordCount实例程序</div><div class="line"></div><div class="line">实例程序请参考GitHub上的源代码</div><div class="line"></div><div class="line">- http://github.com/birdben/birdHadoop</div><div class="line"></div><div class="line">这里我们使用Maven来打包构建项目，pom文件中需要添加Storm相关jar的引用</div></pre></td></tr></table></figure>

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;
    &lt;artifactId&gt;storm-core&lt;/artifactId&gt;
    &lt;version&gt;0.10.2&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">![Storm](http://img.blog.csdn.net/20161110192542351?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)</div><div class="line"></div><div class="line">这里我们要是实现的逻辑，WordReaderSpout负责从inputfile中读取文件的内容，将读取完成的文件重命名，读取之后的内容按行发送给WordSpliterBolt，由WordSpliter负责将一行内容按照空格进行拆分，拆分之后将拆分好的单词发送给WordCounterBolt，WordCounterBolt负责按照单词统计次数。整个Word处理的过程是一个Topology（拓扑图，我的理解就是一个任务的执行过程图），WordCountMain负责提交Topology到Storm集群。</div><div class="line"></div><div class="line">- WordCountMain：提交Topology到Storm集群</div><div class="line">- WordReaderSpout（Spout）：负责读取文件内容</div><div class="line">- WordSpliterBolt（Bolt）：负责拆分每行内容中的单词</div><div class="line">- WordCounterBolt（Bolt）：负责统计单词次数</div><div class="line"></div><div class="line">注意：这里读取完文件一定要进行重命名操作，否则Storm集群会一直循环读取（因为代码中我们是扫描inputfile目录下除去.bak的所有文件的），而且Storm集群模式是不会停止的，这是Storm流式计算和MapReduce离线任务的本质区别。</div><div class="line"></div><div class="line">Storm有两种运行模式，下面分别介绍这两种模式</div><div class="line"></div><div class="line">- 本地模式：实际上本地模式在JVM中模拟了一个Storm集群，用于开发和测试Topology。在本地模式下运行Topology类似于在集群上运行Topology。只需使用LocalCluster类就可以创建一个进程内的集群。可以直接在IDE就可以启动Storm本地集群，可以在代码中控制集群的停止。</div><div class="line"></div><div class="line">- 集群模式：需要将代码打包成jar包，然后在Storm集群机器上运行&quot;storm jar birdHadoop.jar com.birdben.storm.demo.WordCountMain inputpath outputpath&quot;命令，这样该Topology会运行在不同的JVM或物理机器上，并且可以在Storm UI中监控到。使用集群模式时，不能在代码中控制集群，这和LocalCluster是不一样的。无法在代码中控制集群的停止</div><div class="line"></div><div class="line">#### 本地模式</div><div class="line"></div><div class="line">本地模式需要在pom文件中引入Storm相应的jar包，这里需要注意scope这里要设置成compile，或者把scope去掉。因为我们是直接通过IDE启动Storm本地集群的，所以需要Storm相关的jar包。</div></pre></td></tr></table></figure>

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;
    &lt;artifactId&gt;storm-core&lt;/artifactId&gt;
    &lt;version&gt;0.10.2&lt;/version&gt;
    &lt;scope&gt;compile&lt;/scope&gt;
&lt;/dependency&gt;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">注意：在Maven打包之前需要先修改pom文件，指定我们的入口类是&quot;com.birdben.storm.demo.WordCountMain&quot;</div><div class="line"></div><div class="line">本地模式提交WordCount这个Topology，但是休眠10秒中之后我们将kill掉WordCount这个Topology，这样才能够触发WordCounter中的cleanup方法，将我们的统计结果输出到目标文件中，否则的话，cleanup方法始终不会被调用，目标文件也是不会有统计结果的</div></pre></td></tr></table></figure>

LocalCluster cluster = new LocalCluster();
cluster.submitTopology(&quot;WordCount&quot;, conf, builder.createTopology());
Utils.sleep(10000);
cluster.killTopology(&quot;WordCount&quot;);
cluster.shutdown();
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">具体代码请参考：</div><div class="line"></div><div class="line">- http://github.com/birdben/birdHadoop</div></pre></td></tr></table></figure>

# 进入项目根目录下
$ cd /Users/yunyu/workspace_git/birdHadoop
# 编译打包
$ mvn clean package
# 执行java -jar运行我们打好的jar包，这里将相关操作写成了Shell脚本
$ sh scripts/storm/runWordCount_Local.sh
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#### runWordCount_Local.sh脚本文件</div></pre></td></tr></table></figure>

#!/bin/bash
local_path=~/workspace_git/birdHadoop
local_inputfile_path=$local_path/inputfile/WordCount
local_outputfile_path=$local_path/outputfile/WordCount
if [ -f $local_inputfile_path/input_WordCount.bak ]; then
        # 如果本地bak文件存在，就重命名去掉bak
        echo &quot;正在重命名$local_inputfile_path/input_WordCount.bak文件&quot;
        mv $local_inputfile_path/input_WordCount.bak $local_inputfile_path/input_WordCount
fi
if [ ! -d $local_outputfile_path ]; then
        # 如果本地文件目录不存在，就自动创建
        echo &quot;自动创建$outputfile_path目录&quot;
        mkdir -p $local_outputfile_path
else
        # 如果本地文件已经存在，就删除
        echo &quot;删除$local_outputfile_path/*目录下的所有文件&quot;
        rm -rf $local_outputfile_path/*
fi
# 需要在Maven的pom.xml文件中指定jar的入口类
echo &quot;开始执行birdHadoop.jar...&quot;
java -jar $local_path/target/birdHadoop.jar $local_inputfile_path $local_outputfile_path
echo &quot;结束执行birdHadoop.jar...&quot;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">下面是执行过程中的输出</div></pre></td></tr></table></figure>

$ sh scripts/storm/runWordCount_Local.sh
正在重命名/Users/yunyu/workspace_git/birdHadoop/inputfile/WordCount/input_WordCount.bak文件
删除/Users/yunyu/workspace_git/birdHadoop/outputfile/WordCount/*目录下的所有文件
开始执行birdHadoop.jar...
log4j:WARN No appenders could be found for logger (backtype.storm.utils.Utils).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
WordCounter prepare out start
WordCounter clean out start
WordCounter result : hadoop 3
WordCounter result : hive 2
WordCounter result : logstash 1
WordCounter result : hbase 2
WordCounter result : flume 1
WordCounter result : kafka 2
WordCounter result : storm 1
WordCounter result : spark 1
WordCounter result : es 1
WordCounter clean out end
结束执行birdHadoop.jar...
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">查看一下我们所期望的结果文件output_WordCount的内容</div></pre></td></tr></table></figure>

$ cat outputfile/WordCount/output_WordCount
hadoop 3
hive 2
logstash 1
hbase 2
flume 1
kafka 2
storm 1
spark 1
es 1
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#### 集群模式</div><div class="line"></div><div class="line">集群模式需要修改pom文件中Storm相应的jar包的scope设置成provided，否则再次引用就会冲突报错。因为我们是直接将打好的jar包提交到Storm集群中运行的，所以不需要Storm相关的jar包。</div></pre></td></tr></table></figure>

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;
    &lt;artifactId&gt;storm-core&lt;/artifactId&gt;
    &lt;version&gt;0.10.2&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">如果没有将scope设置为provided，就会遇到如下错误</div></pre></td></tr></table></figure>

Caused by: java.io.IOException: Found multiple defaults.yaml resources. You&apos;re probably bundling the Storm jars with your topology jar. [jar:file:/data/storm-0.10.2/lib/storm-core-0.10.2.jar!/defaults.yaml, jar:file:/home/yunyu/Downloads/birdHadoop/target/birdHadoop.jar!/defaults.yaml]
    at backtype.storm.utils.Utils.getConfigFileInputStream(Utils.java:266)
    at backtype.storm.utils.Utils.findAndReadConfigFile(Utils.java:220)
    ... 103 more
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">集群模式提交Topology，需要将代码打包成jar包，然后在Storm集群机器上运行&quot;storm jar birdHadoop.jar com.birdben.storm.demo.WordCountMain inputpath outputpath&quot;命令，这样该Topology会运行在不同的JVM或物理机器上，并且可以在Storm UI中监控到。使用集群模式时，不能在代码中控制集群，这和LocalCluster是不一样的。无法在代码中控制集群的停止</div></pre></td></tr></table></figure>

StormSubmitter.submitTopology(&quot;WordCount&quot;, conf, builder.createTopology());
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">具体代码请参考：</div><div class="line"></div><div class="line">- http://github.com/birdben/birdHadoop</div></pre></td></tr></table></figure>

# 进入项目根目录下
$ cd /Users/yunyu/workspace_git/birdHadoop
# 编译打包
$ mvn clean package
# 将打好的jar包上传到storm集群，执行storm jar运行我们打好的jar包，这里将相关操作写成了Shell脚本
$ sh scripts/storm/runWordCount_Remote.sh
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">这里需要注意：</div><div class="line"></div><div class="line">上一篇我们说过，在Storm的集群里面有两种节点： 控制节点(master node)和工作节点(worker node)。控制节点上面运行一个后台程序： Nimbus， 它的作用类似Hadoop里面的JobTracker。Nimbus负责在集群里面分布代码，分配工作给机器， 并且监控状态。</div><div class="line"></div><div class="line">每一个工作节点上面运行一个叫做Supervisor的节点（类似 TaskTracker）。Supervisor会监听分配给它那台机器的工作，根据需要 启动/关闭工作进程。每一个工作进程执行一个Topology（类似 Job）的一个子集；一个运行的Topology由运行在很多机器上的很多工作进程 Worker（类似 Child）组成。</div><div class="line"></div><div class="line">在我们的WordCount实例中，storm集群是三台机器，hadoop1是Nimbus，hadoop2和hadoop3是Supervisor，这里执行storm jar（也就是提交Topology）无论是在哪台机器上操作，最后都应该是在Supervisor分配的Work进程中进行计算的，所以我们inputfile文件应该上传到所有Storm集群的机器上，这样才能够避免Work进程在读取inputfile文件读取不到。这个问题我也是犯浑了很久，之前一直在Nimbus机器执行shell脚本发现WordReader读取文件的时候一直读取不到，我一直以为在Nimbus机器上执行storm jar（提交Topology）就一定是在Nimbus机器上执行操作，后来仔细观察了日志才发现WordCount的运行日志根本不在Nimbus机器上，而是在Supervisor机器上，所以才知道原来运行WordReader的Work进程在Supervisor机器上，看来对于Storm的运行原理还是理解的不够深。</div><div class="line"></div><div class="line">#### runWordCount_Remote.sh脚本文件</div></pre></td></tr></table></figure>

#!/bin/bash
local_path=~/Downloads/birdHadoop
local_inputfile_path=$local_path/inputfile/WordCount
local_outputfile_path=$local_path/outputfile/WordCount
main_class=com.birdben.storm.demo.WordCountMain
if [ -f $local_inputfile_path/input_WordCount.bak ]; then
        # 如果本地bak文件存在，就重命名去掉bak
        echo &quot;正在重命名$local_inputfile_path/input_WordCount.bak文件&quot;
        mv $local_inputfile_path/input_WordCount.bak $local_inputfile_path/input_WordCount
fi
if [ ! -d $local_outputfile_path ]; then
        # 如果本地文件目录不存在，就自动创建
        echo &quot;自动创建$outputfile_path目录&quot;
        mkdir -p $local_outputfile_path
else
        # 如果本地文件已经存在，就删除
        echo &quot;删除$local_outputfile_path/*目录下的所有文件&quot;
        rm -rf $local_outputfile_path/*
fi
# 需要在Maven的pom.xml文件中指定jar的入口类
echo &quot;开始执行birdHadoop.jar...&quot;
storm jar $local_path/target/birdHadoop.jar $main_class $local_inputfile_path $local_outputfile_path
echo &quot;结束执行birdHadoop.jar...&quot;
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">执行Shell脚本之后，可以在Storm UI中查看到Topology Summary中多了一个WordCounter Topology，Topology Id是WordCount-6-1479006604，我们找到Supervisor机器上的log日志（$&#123;STORM_HOME&#125;/logs），该日志目录下会根据Topology Id生成对应的日志文件如下：</div><div class="line"></div><div class="line">- WordCount-6-1479006604-worker-6703.log- WordCount-6-1479006604-worker-6703.log.err- WordCount-6-1479006604-worker-6703.log.metrics.log- WordCount-6-1479006604-worker-6703.log.out</div><div class="line"></div><div class="line">我们可以查看一下WordCount-6-1479006604-worker-6703.log日志，我们代码中的日志输出都在这个日志文件中，可以看到Storm集群读取我们指定的inputfile，并且按照指定方式拆分出Word单词。</div></pre></td></tr></table></figure>

$ vi WordCount-6-1479006604-worker-6703.log
...
2016-11-12 19:10:11.001 b.s.d.executor [INFO] Preparing bolt word-spilter:(4)
2016-11-12 19:10:11.002 b.s.d.executor [INFO] Prepared bolt word-spilter:(4)
2016-11-12 19:10:11.002 b.s.d.executor [INFO] Processing received message FOR 4 TUPLE: source: word-reader:3, stream: default, id: {}, [Hadoop Hive HBase]
2016-11-12 19:10:11.002 STDIO [INFO] out inputPath:/home/yunyu/Downloads/birdHadoop/inputfile/WordCount
2016-11-12 19:10:11.002 STDIO [INFO] WordSpliter execute out start
2016-11-12 19:10:11.004 STDIO [INFO] out inputPath:/home/yunyu/Downloads/birdHadoop/inputfile/WordCount
2016-11-12 19:10:11.005 STDIO [INFO] out inputPath:/home/yunyu/Downloads/birdHadoop/inputfile/WordCount
2016-11-12 19:10:11.006 b.s.d.task [INFO] Emitting: word-spilter default [hadoop]
2016-11-12 19:10:11.006 STDIO [INFO] out inputPath:/home/yunyu/Downloads/birdHadoop/inputfile/WordCount
2016-11-12 19:10:11.007 b.s.d.executor [INFO] TRANSFERING tuple TASK: 2 TUPLE: source: word-spilter:4, stream: default, id: {}, [hadoop]
2016-11-12 19:10:11.007 b.s.d.task [INFO] Emitting: word-spilter default [hive]
2016-11-12 19:10:11.008 b.s.d.executor [INFO] TRANSFERING tuple TASK: 2 TUPLE: source: word-spilter:4, stream: default, id: {}, [hive]
2016-11-12 19:10:11.008 b.s.d.task [INFO] Emitting: word-spilter default [hbase]
2016-11-12 19:10:11.008 STDIO [INFO] out inputPath:/home/yunyu/Downloads/birdHadoop/inputfile/WordCount
2016-11-12 19:10:11.008 b.s.d.executor [INFO] TRANSFERING tuple TASK: 2 TUPLE: source: word-spilter:4, stream: default, id: {}, [hbase]
2016-11-12 19:10:11.009 STDIO [INFO] WordSpliter execute out end
2016-11-12 19:10:11.009 b.s.d.executor [INFO] BOLT ack TASK: 4 TIME:  TUPLE: source: word-reader:3, stream: default, id: {}, [Hadoop Hive HBase]
2016-11-12 19:10:11.009 b.s.d.executor [INFO] Execute done TUPLE source: word-reader:3, stream: default, id: {}, [Hadoop Hive HBase] TASK: 4 DELTA:
2016-11-12 19:10:11.010 b.s.d.executor [INFO] Processing received message FOR 4 TUPLE: source: word-reader:3, stream: default, id: {}, [Spark Hive Hadoop]
2016-11-12 19:10:11.010 STDIO [INFO] WordSpliter execute out start
2016-11-12 19:10:11.010 b.s.d.task [INFO] Emitting: word-spilter default [spark]
2016-11-12 19:10:11.010 STDIO [INFO] out inputPath:/home/yunyu/Downloads/birdHadoop/inputfile/WordCount
2016-11-12 19:10:11.010 b.s.d.executor [INFO] TRANSFERING tuple TASK: 2 TUPLE: source: word-spilter:4, stream: default, id: {}, [spark]
2016-11-12 19:10:11.011 b.s.d.task [INFO] Emitting: word-spilter default [hive]
2016-11-12 19:10:11.011 b.s.d.executor [INFO] TRANSFERING tuple TASK: 2 TUPLE: source: word-spilter:4, stream: default, id: {}, [hive]
2016-11-12 19:10:11.011 b.s.d.task [INFO] Emitting: word-spilter default [hadoop]
2016-11-12 19:10:11.012 b.s.d.executor [INFO] TRANSFERING tuple TASK: 2 TUPLE: source: word-spilter:4, stream: default, id: {}, [hadoop]
2016-11-12 19:10:11.012 STDIO [INFO] WordSpliter execute out end
2016-11-12 19:10:11.012 STDIO [INFO] out inputPath:/home/yunyu/Downloads/birdHadoop/inputfile/WordCount
2016-11-12 19:10:11.012 b.s.d.executor [INFO] BOLT ack TASK: 4 TIME:  TUPLE: source: word-reader:3, stream: default, id: {}, [Spark Hive Hadoop]
2016-11-12 19:10:11.012 b.s.d.executor [INFO] Execute done TUPLE source: word-reader:3, stream: default, id: {}, [Spark Hive Hadoop] TASK: 4 DELTA:
2016-11-12 19:10:11.013 b.s.d.executor [INFO] Processing received message FOR 4 TUPLE: source: word-reader:3, stream: default, id: {}, [Kafka HBase ES Logstash Storm]
2016-11-12 19:10:11.013 STDIO [INFO] WordSpliter execute out start
2016-11-12 19:10:11.013 b.s.d.task [INFO] Emitting: word-spilter default [kafka]
2016-11-12 19:10:11.013 b.s.d.executor [INFO] TRANSFERING tuple TASK: 2 TUPLE: source: word-spilter:4, stream: default, id: {}, [kafka]
2016-11-12 19:10:11.014 b.s.d.task [INFO] Emitting: word-spilter default [hbase]
2016-11-12 19:10:11.014 STDIO [INFO] out inputPath:/home/yunyu/Downloads/birdHadoop/inputfile/WordCount
...
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">上述步骤都已经执行完毕了，日志也没有错误了，但是查看outputfile文件还是没有内容。这是因为我们的WordCount的cleanup方法没有被执行，所以并没有将我们的统计结果输出到outputfile文件中。这里我们是集群模式，因为Storm是流式计算引擎，所以集群的WordCount Topology不停止是不会调用cleanup方法的。所以这里我们需要使用storm kill WordCount方式杀掉WordCount Topology，这样才能够使Storm调用WordCount的cleanup方法将统计结果输出到outputfile中。</div></pre></td></tr></table></figure>

$ storm kill WordCount
Running: /usr/local/java/bin/java -client -Ddaemon.name= -Dstorm.options= -Dstorm.home=/data/storm-0.10.2 -Dstorm.log.dir=/data/storm-0.10.2/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /data/storm-0.10.2/lib/kryo-2.21.jar:/data/storm-0.10.2/lib/servlet-api-2.5.jar:/data/storm-0.10.2/lib/hadoop-auth-2.4.0.jar:/data/storm-0.10.2/lib/minlog-1.2.jar:/data/storm-0.10.2/lib/storm-core-0.10.2.jar:/data/storm-0.10.2/lib/log4j-core-2.1.jar:/data/storm-0.10.2/lib/reflectasm-1.07-shaded.jar:/data/storm-0.10.2/lib/clojure-1.6.0.jar:/data/storm-0.10.2/lib/disruptor-2.10.4.jar:/data/storm-0.10.2/lib/log4j-over-slf4j-1.6.6.jar:/data/storm-0.10.2/lib/asm-4.0.jar:/data/storm-0.10.2/lib/log4j-slf4j-impl-2.1.jar:/data/storm-0.10.2/lib/slf4j-api-1.7.7.jar:/data/storm-0.10.2/lib/log4j-api-2.1.jar:/usr/local/storm/conf:/data/storm-0.10.2/bin backtype.storm.command.kill_topology WordCount
1467 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources
1535 [main] INFO  b.s.u.Utils - Using storm.yaml from resources
2180 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources
2200 [main] INFO  b.s.u.Utils - Using storm.yaml from resources
2227 [main] INFO  b.s.thrift - Connecting to Nimbus at hadoop1:6627 as user: 
2228 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources
2251 [main] INFO  b.s.u.Utils - Using storm.yaml from resources
2269 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [2000] the maxSleepTimeMs [60000] the maxRetries [5]
</code></pre><p>这里还有个需要注意的地方，就是storm kill WordCount不是立即执行完毕的，它只是将WordCount的Topology状态先标记成KILLED，还需要sleep一段时间之后Topology才会真正被Kill。所以执行完storm kill之后在Storm UI中仍然能查看到WordCount的Topology，只是状态变成KILLED。如果此时再次执行Shell脚本重新运行WordCount Topology，Storm集群仍然会提示WordCount Topology已经存在了。</p>
<p>参考文章：</p>
<ul>
<li><a href="http://storm.apache.org/releases/1.0.2/Setting-up-a-Storm-cluster.html" target="_blank" rel="external">http://storm.apache.org/releases/1.0.2/Setting-up-a-Storm-cluster.html</a></li>
<li><a href="http://book.51cto.com/art/201410/453401.htm" target="_blank" rel="external">http://book.51cto.com/art/201410/453401.htm</a></li>
<li><a href="http://blog.csdn.net/guoqiangma/article/details/38045259" target="_blank" rel="external">http://blog.csdn.net/guoqiangma/article/details/38045259</a></li>
<li><a href="http://blog.csdn.net/xeseo/article/details/18219183" target="_blank" rel="external">http://blog.csdn.net/xeseo/article/details/18219183</a></li>
<li><a href="http://blog.itpub.net/28912557/viewspace-1450885/" target="_blank" rel="external">http://blog.itpub.net/28912557/viewspace-1450885/</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storm/">Storm</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Storm/">Storm</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Storm/Storm学习（一）Storm集群环境搭建" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/07/Storm/Storm学习（一）Storm集群环境搭建/" class="article-date">
  	<time datetime="2016-11-07T06:57:13.000Z" itemprop="datePublished">2016-11-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/07/Storm/Storm学习（一）Storm集群环境搭建/">Storm学习（一）Storm集群环境搭建</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天开始搭建Storm集群环境了，主要参考官网的步骤一点一点学习的</p>
<p>搭建Storm集群的主要步骤如下:</p>
<ul>
<li>搭建好一个Zookeeper集群</li>
<li>安装好依赖的环境</li>
<li>下载并解压一个Storm版本</li>
<li>补充主要的配置到storm.yaml配置文件</li>
<li>后台执行Storm启动脚本</li>
</ul>
<h3 id="Zookeeper集群环境搭建"><a href="#Zookeeper集群环境搭建" class="headerlink" title="Zookeeper集群环境搭建"></a>Zookeeper集群环境搭建</h3><p>这里我们不做详细介绍了，Zookeeper集群环境搭建我之前单独写过一篇文章，请参考。</p>
<h3 id="安装依赖环境"><a href="#安装依赖环境" class="headerlink" title="安装依赖环境"></a>安装依赖环境</h3><p>Storm依赖于Java环境和Python环境，具体版本如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Java 7</div><div class="line">Python 2.6.6</div></pre></td></tr></table></figure>
<p>可能Storm在不同的Java和Python版本下会不好用</p>
<h4 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h4><p>省略</p>
<h4 id="安装Python"><a href="#安装Python" class="headerlink" title="安装Python"></a>安装Python</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 下载Python2.6.6</div><div class="line">$ wget http://www.python.org/ftp/python/2.6.6/Python-2.6.6.tar.bz2</div><div class="line"></div><div class="line"># 编译安装Python2.6.6</div><div class="line">$ tar –jxvf Python-2.6.6.tar.bz2</div><div class="line">$ cd Python-2.6.6</div><div class="line">$ ./configure</div><div class="line">$ make</div><div class="line">$ make install</div><div class="line"></div><div class="line"># 测试Python</div><div class="line">$ python -V</div><div class="line">Python 2.6.6</div></pre></td></tr></table></figure>
<h3 id="下载Storm"><a href="#下载Storm" class="headerlink" title="下载Storm"></a>下载Storm</h3><p>在Storm的GitHub中选择一个Storm版本下载安装，这里我选择的0.10.2版本</p>
<ul>
<li><a href="http://storm.apache.org/downloads.html" target="_blank" rel="external">http://storm.apache.org/downloads.html</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 下载Storm安装包</div><div class="line">$ curl -O http://apache.fayea.com/storm/apache-storm-0.10.2/apache-storm-0.10.2.tar.gz</div><div class="line"></div><div class="line"># 解压Storm压缩包</div><div class="line">$ tar -xvf apache-storm-0.10.2.tar</div></pre></td></tr></table></figure>
<h3 id="修改storm-yaml配置文件"><a href="#修改storm-yaml配置文件" class="headerlink" title="修改storm.yaml配置文件"></a>修改storm.yaml配置文件</h3><p>Storm启动会加载conf/storm.yaml配置文件，该配置文件会覆盖掉defaults.yaml配置文件中的配置。下面介绍少部分重要的配置</p>
<ul>
<li>storm.zookeeper.servers : Storm依赖的Zookeeper集群地址</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">storm.zookeeper.servers:</div><div class="line">  - &quot;111.222.333.444&quot;</div><div class="line">  - &quot;555.666.777.888&quot;</div></pre></td></tr></table></figure>
<p>如果Zookeeper集群使用的不是默认的端口号，可以通过storm.zookeeper.port配置来修改，否则会出现通信错误。</p>
<ul>
<li>storm.local.dir : Nimbus和Supervisor进程用于存储少量状态，如jars、confs等的本地磁盘目录，需要提前创建该目录并给以足够的访问权限。需要在每个Storm机器中都创建这样一个目录。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">storm.local.dir: &quot;/mnt/storm&quot;</div></pre></td></tr></table></figure>
<p>也可以使用相对路径，相对于$STORM_HOME（即Storm的安装路径），如果不设置该配置就是用默认目录$STORM_HOME/storm-local</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">storm.local.dir: $STORM_HOME/storm-local</div></pre></td></tr></table></figure>
<ul>
<li>nimbus.seeds : Storm集群Nimbus机器地址，各个Supervisor工作节点需要知道哪个机器是Nimbus，以便下载Topologies的jars、confs等文件。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nimbus.seeds: [&quot;111.222.333.44&quot;]</div></pre></td></tr></table></figure>
<p>这里推荐使用机器的hostname，如果你想设置Nimbus的HA高可用，你必须设置每个正在运行的Nimbus的hostname。如果是假的分布式集群，可以使用默认值，但是建议设置Nimbus的hostname。</p>
<ul>
<li>supervisor.slots.ports : 对于每个Supervisor工作节点，需要配置该工作节点可以运行的worker数量。每个worker占用一个单独的端口用于接收消息，该配置选项即用于定义哪些端口是可被worker使用的。默认情况下，每个节点上可运行4个workers，分别在6700、6701、6702和6703端口</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">supervisor.slots.ports:</div><div class="line">    - 6700</div><div class="line">    - 6701</div><div class="line">    - 6702</div><div class="line">    - 6703</div></pre></td></tr></table></figure>
<h3 id="健康检查（下面这段翻译的不够准确，仅供参考）"><a href="#健康检查（下面这段翻译的不够准确，仅供参考）" class="headerlink" title="健康检查（下面这段翻译的不够准确，仅供参考）"></a>健康检查（下面这段翻译的不够准确，仅供参考）</h3><p>Storm提供一种机制 —— 管理者可以配置一个监控者周期的执行管理者提供的脚本，检查node节点是否健康。管理者有决定node节点在健康状态。如果一个脚本查明node节点处于非健康状态，必须打印以ERROR开头标准化输出错误。监控者周期的执行脚本在storm.health.check.dir并且检查输出结果。如果脚本的输出结果包括ERROR，监控者将会关闭worker。</p>
<p>如果监控者正在运行与监控”/bin/storm node-health-check”可以被调用，以确定监控者是否被启动，或者该node节点是否是不健康的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 健康检查的目录配置如下</div><div class="line">storm.health.check.dir: &quot;healthchecks&quot;</div><div class="line"># 健康检查等待超时时间配置</div><div class="line">storm.health.check.timeout.ms: 5000</div></pre></td></tr></table></figure>
<h3 id="配置外部依赖库和环境变量（可选）"><a href="#配置外部依赖库和环境变量（可选）" class="headerlink" title="配置外部依赖库和环境变量（可选）"></a>配置外部依赖库和环境变量（可选）</h3><p>如果你需要支持外部依赖库或者自定义插件，你可以定位这些jar到extlib和extlib-daemon目录下。注意extlib-daemon目录存储的jar只被用于daemons启动(Nimbus, Supervisor, DRPC, UI, Logviewer)，如，HDFS和自定义的定时任务库。因此STORM_EXT_CLASSPATH和STORM_EXT_CLASSPATH_DAEMON这两个环境变量都需要被配置，为了外部依赖classpath和后台启动的外部依赖classpath。</p>
<h3 id="启动Storm后台进程"><a href="#启动Storm后台进程" class="headerlink" title="启动Storm后台进程"></a>启动Storm后台进程</h3><p>最后一步就是启动Storm的后台进程，很关键的一点这些后台进程都在监控下。Storm是快速失败（fail-fast)，意味着无论什么时候发生错误，进程都会停止。Storm被设计成这样可以在任何点安全的停止，并且当进程重启时可以正确的被恢复。这也是说明Storm为什么是无状态的系统。如果Nimbus或者Supervisors重启，正在运行的topologies不会受到影响。</p>
<p>下面是如何启动Storm的后台进程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Nimbus: Run the command &quot;bin/storm nimbus&quot; under supervision on the master machine.</div><div class="line"></div><div class="line">Supervisor: Run the command &quot;bin/storm supervisor&quot; under supervision on each worker machine. The supervisor daemon is responsible for starting and stopping worker processes on that machine.</div><div class="line"></div><div class="line">UI: Run the Storm UI (a site you can access from the browser that gives diagnostics on the cluster and topologies) by running the command &quot;bin/storm ui&quot; under supervision. The UI can be accessed by navigating your web browser to http://&#123;ui host&#125;:8080.</div></pre></td></tr></table></figure>
<p>Storm后台进程被启动后，将在Storm安装部署目录下的logs/子目录下生成各个进程的日志文件</p>
<p>以上是翻译自Storm的官网，接下来是针对于我自己的Storm集群的配置过程</p>
<h3 id="Storm集群搭建步骤"><a href="#Storm集群搭建步骤" class="headerlink" title="Storm集群搭建步骤"></a>Storm集群搭建步骤</h3><p>修改storm.yaml配置如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">storm.zookeeper.servers:    - &quot;hadoop2&quot;    - &quot;hadoop3&quot;nimbus.host: &quot;hadoop1&quot;</div></pre></td></tr></table></figure>
<p>注意：这里hadoop1, hadoop2, hadoop3是我之前安装Hadoop集群环境对应的三台机器的hostname，需要根据个人实际环境进行修改。这里我将hadoop1这台机器作为Nimbus，hadoop2和hadoop3这两台机器作为Supervisor。其他配置都使用默认的配置。</p>
<p>启动Nimbus</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bin/storm nimbusRunning: /usr/local/java/bin/java -server -Ddaemon.name=nimbus -Dstorm.options= -Dstorm.home=/data/storm-0.10.2 -Dstorm.log.dir=/data/storm-0.10.2/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /data/storm-0.10.2/lib/storm-core-0.10.2.jar:/data/storm-0.10.2/lib/slf4j-api-1.7.7.jar:/data/storm-0.10.2/lib/clojure-1.6.0.jar:/data/storm-0.10.2/lib/disruptor-2.10.4.jar:/data/storm-0.10.2/lib/servlet-api-2.5.jar:/data/storm-0.10.2/lib/log4j-api-2.1.jar:/data/storm-0.10.2/lib/log4j-core-2.1.jar:/data/storm-0.10.2/lib/minlog-1.2.jar:/data/storm-0.10.2/lib/reflectasm-1.07-shaded.jar:/data/storm-0.10.2/lib/log4j-over-slf4j-1.6.6.jar:/data/storm-0.10.2/lib/asm-4.0.jar:/data/storm-0.10.2/lib/hadoop-auth-2.4.0.jar:/data/storm-0.10.2/lib/kryo-2.21.jar:/data/storm-0.10.2/lib/log4j-slf4j-impl-2.1.jar:/data/storm-0.10.2/conf -Xmx1024m -Dlogfile.name=nimbus.log -Dlog4j.configurationFile=/data/storm-0.10.2/log4j2/cluster.xml backtype.storm.daemon.nimbus</div></pre></td></tr></table></figure>
<p>启动Supervisor</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bin/storm supervisorRunning: /usr/local/java/bin/java -server -Ddaemon.name=supervisor -Dstorm.options= -Dstorm.home=/data/storm-0.10.2 -Dstorm.log.dir=/data/storm-0.10.2/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /data/storm-0.10.2/lib/asm-4.0.jar:/data/storm-0.10.2/lib/servlet-api-2.5.jar:/data/storm-0.10.2/lib/slf4j-api-1.7.7.jar:/data/storm-0.10.2/lib/reflectasm-1.07-shaded.jar:/data/storm-0.10.2/lib/clojure-1.6.0.jar:/data/storm-0.10.2/lib/hadoop-auth-2.4.0.jar:/data/storm-0.10.2/lib/log4j-api-2.1.jar:/data/storm-0.10.2/lib/log4j-slf4j-impl-2.1.jar:/data/storm-0.10.2/lib/storm-core-0.10.2.jar:/data/storm-0.10.2/lib/log4j-over-slf4j-1.6.6.jar:/data/storm-0.10.2/lib/kryo-2.21.jar:/data/storm-0.10.2/lib/minlog-1.2.jar:/data/storm-0.10.2/lib/log4j-core-2.1.jar:/data/storm-0.10.2/lib/disruptor-2.10.4.jar:/data/storm-0.10.2/conf -Xmx256m -Dlogfile.name=supervisor.log -Dlog4j.configurationFile=/data/storm-0.10.2/log4j2/cluster.xml backtype.storm.daemon.supervisor</div></pre></td></tr></table></figure>
<p>启动UI</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bin/storm uiRunning: /usr/local/java/bin/java -server -Ddaemon.name=ui -Dstorm.options= -Dstorm.home=/data/storm-0.10.2 -Dstorm.log.dir=/data/storm-0.10.2/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /data/storm-0.10.2/lib/storm-core-0.10.2.jar:/data/storm-0.10.2/lib/slf4j-api-1.7.7.jar:/data/storm-0.10.2/lib/clojure-1.6.0.jar:/data/storm-0.10.2/lib/disruptor-2.10.4.jar:/data/storm-0.10.2/lib/servlet-api-2.5.jar:/data/storm-0.10.2/lib/log4j-api-2.1.jar:/data/storm-0.10.2/lib/log4j-core-2.1.jar:/data/storm-0.10.2/lib/minlog-1.2.jar:/data/storm-0.10.2/lib/reflectasm-1.07-shaded.jar:/data/storm-0.10.2/lib/log4j-over-slf4j-1.6.6.jar:/data/storm-0.10.2/lib/asm-4.0.jar:/data/storm-0.10.2/lib/hadoop-auth-2.4.0.jar:/data/storm-0.10.2/lib/kryo-2.21.jar:/data/storm-0.10.2/lib/log4j-slf4j-impl-2.1.jar:/data/storm-0.10.2:/data/storm-0.10.2/conf -Xmx768m -Dlogfile.name=ui.log -Dlog4j.configurationFile=/data/storm-0.10.2/log4j2/cluster.xml backtype.storm.ui.core</div></pre></td></tr></table></figure>
<p>启动成功之后访问<a href="http://hadoop1:8080，效果图如下" target="_blank" rel="external">http://hadoop1:8080，效果图如下</a></p>
<p><img src="http://img.blog.csdn.net/20161107192753473?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Storm UI"></p>
<p>参考文章：</p>
<ul>
<li><a href="http://storm.apache.org/releases/0.10.2/Setting-up-a-Storm-cluster.html" target="_blank" rel="external">http://storm.apache.org/releases/0.10.2/Setting-up-a-Storm-cluster.html</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storm/">Storm</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Storm/">Storm</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Docker/Docker实战（二十二）Docker-Compose部署Zookeeper集群环境" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/04/Docker/Docker实战（二十二）Docker-Compose部署Zookeeper集群环境/" class="article-date">
  	<time datetime="2016-11-04T07:27:19.000Z" itemprop="datePublished">2016-11-04</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/04/Docker/Docker实战（二十二）Docker-Compose部署Zookeeper集群环境/">Docker实战（二十二）Docker-Compose部署Zookeeper集群环境</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇我们具体使用Docker-Compose来部署Zookeeper集群环境，这里我们使用Zookeeper官方提供的Docker镜像来搭建集群环境，官方的镜像地址：<a href="https://hub.docker.com/_/zookeeper/" target="_blank" rel="external">https://hub.docker.com/_/zookeeper/</a></p>
<h4 id="下载Zookeeper官方的Docker镜像"><a href="#下载Zookeeper官方的Docker镜像" class="headerlink" title="下载Zookeeper官方的Docker镜像"></a>下载Zookeeper官方的Docker镜像</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker pull zookeeper:latest</div></pre></td></tr></table></figure>
<h4 id="zoo-cfg配置文件"><a href="#zoo-cfg配置文件" class="headerlink" title="zoo.cfg配置文件"></a>zoo.cfg配置文件</h4><p>这里我们将部署三台Docker容器组成一个Zookeeper集群，然后我们在本地创建一个zoo.cfg配置文件，指定好Zookeeper集群的配置，zk1，zk2，zk3分别是三台Zookeeper服务器的host名称</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">tickTime=2000</div><div class="line">initLimit=10</div><div class="line">syncLimit=5</div><div class="line">dataDir=/opt/data</div><div class="line">clientPort=2181</div><div class="line">dataLogDir=/opt/log</div></pre></td></tr></table></figure>
<h4 id="docker-compose-yml配置文件"><a href="#docker-compose-yml配置文件" class="headerlink" title="docker-compose.yml配置文件"></a>docker-compose.yml配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line">version: &apos;2&apos;</div><div class="line">services:</div><div class="line">   zoo1:</div><div class="line">      # 指定当前构建的Docker容器的镜像</div><div class="line">      image: zookeeper</div><div class="line">      restart: always</div><div class="line">      # 指定当前构建的Docker容器的名称</div><div class="line">      container_name: zk1</div><div class="line">      networks:</div><div class="line">         zoo_net:</div><div class="line">            # 指定当前构建的Docker容器的IP地址</div><div class="line">            ipv4_address: 172.18.0.2</div><div class="line">      # 指定当前构建的Docker容器的host配置</div><div class="line">      extra_hosts:</div><div class="line">         - &quot;zoo1:172.18.0.2&quot;</div><div class="line">         - &quot;zoo2:172.18.0.3&quot;</div><div class="line">         - &quot;zoo3:172.18.0.4&quot;</div><div class="line">      # 指定当前构建的Docker容器的volume挂在目录设置</div><div class="line">      volumes:</div><div class="line">         - ~/Downloads/yunyu/zookeeper_docker/data/zoo1:/opt/data</div><div class="line">         - ~/Downloads/yunyu/zookeeper_docker/logs/zoo1:/opt/log</div><div class="line">      # 指定当前构建的Docker容器对外开放的端口号映射</div><div class="line">      ports:</div><div class="line">         - &quot;2181:2181&quot;</div><div class="line">         - &quot;2881:2888&quot;</div><div class="line">         - &quot;3881:3888&quot;</div><div class="line">      # 指定当前构建的Docker容器环境变量设置</div><div class="line">      environment:</div><div class="line">         ZOO_MY_ID: 1</div><div class="line">         ZOO_SERVERS: server.1=zoo1:2881:3881 server.2=zoo2:2882:3882 server.3=zoo3:2883:3883</div><div class="line"></div><div class="line">   zoo2:</div><div class="line">      image: zookeeper</div><div class="line">      restart: always</div><div class="line">      container_name: zk2</div><div class="line">      networks:</div><div class="line">         zoo_net:</div><div class="line">            ipv4_address: 172.18.0.3</div><div class="line">      extra_hosts:</div><div class="line">         - &quot;zoo1:172.18.0.2&quot;</div><div class="line">         - &quot;zoo2:172.18.0.3&quot;</div><div class="line">         - &quot;zoo3:172.18.0.4&quot;</div><div class="line">      volumes:</div><div class="line">         - ~/Downloads/yunyu/zookeeper_docker/data/zoo2:/opt/data</div><div class="line">         - ~/Downloads/yunyu/zookeeper_docker/logs/zoo2:/opt/log</div><div class="line">      ports:</div><div class="line">         - &quot;2182:2181&quot;</div><div class="line">         - &quot;2882:2888&quot;</div><div class="line">         - &quot;3882:3888&quot;</div><div class="line">      environment:</div><div class="line">         ZOO_MY_ID: 2</div><div class="line">         ZOO_SERVERS: server.1=zoo1:2881:3881 server.2=zoo2:2882:3882 server.3=zoo3:2883:3883</div><div class="line"></div><div class="line">   zoo3:</div><div class="line">      image: zookeeper</div><div class="line">      restart: always</div><div class="line">      container_name: zk3</div><div class="line">      networks:</div><div class="line">         zoo_net:</div><div class="line">            ipv4_address: 172.18.0.4</div><div class="line">      extra_hosts:</div><div class="line">         - &quot;zoo1:172.18.0.2&quot;</div><div class="line">         - &quot;zoo2:172.18.0.3&quot;</div><div class="line">         - &quot;zoo3:172.18.0.4&quot;</div><div class="line">      volumes:</div><div class="line">         - ~/Downloads/yunyu/zookeeper_docker/data/zoo3:/opt/data</div><div class="line">         - ~/Downloads/yunyu/zookeeper_docker/logs/zoo3:/opt/log</div><div class="line">      ports:</div><div class="line">         - &quot;2183:2181&quot;</div><div class="line">         - &quot;2883:2888&quot;</div><div class="line">         - &quot;3883:3888&quot;</div><div class="line">      environment:</div><div class="line">         ZOO_MY_ID: 3</div><div class="line">         ZOO_SERVERS: server.1=zoo1:2881:3881 server.2=zoo2:2882:3882 server.3=zoo3:2883:3883</div><div class="line"></div><div class="line">networks:</div><div class="line">  zoo_net:</div><div class="line">    driver: bridge</div><div class="line">    ipam:</div><div class="line">      driver: default</div><div class="line">      config:</div><div class="line">      - subnet: 172.18.0.0/16</div><div class="line">        gateway: 172.18.0.1</div></pre></td></tr></table></figure>
<p>这里需要注意几点</p>
<ol>
<li>因为我们是单机部署了多个Docker容器模拟Zookeeper集群的，所以需要做端口映射2181，2888，3888。需要将2888和3888端口也暴露出来，因为如果不暴露出来一旦leader几点挂了，其他follower无法再次进行选举，因为选举是通过3888端口进行的</li>
<li>这里我们指定好了Docker容器的IP地址，这样不会动态的去获取IP地址导致每次启动Docker容器IP地址都会变化</li>
<li>需要设置/etc/hosts配置文件中的host配置</li>
<li>将Docker容器中的/opt/data和/opt/log目录挂在到宿主机的指定目录下</li>
<li>设置了一个网卡zoo_net，网段是172.18.0.0，网关是172.18.0.1</li>
<li>Docker-Compose的version 2版本语法有些变化，如果使用docker-compose version: 1.6以下版本启动可能会遇到Unsupported config option for services service: ‘zoo1’问题，为了支持verion2的语法最好使用最新版本（目前最新版本是1.8.1）。还要注意检查一下networks的配置，否则启动docker-compose up会无法启动</li>
</ol>
<h4 id="启动Docker-Compose"><a href="#启动Docker-Compose" class="headerlink" title="启动Docker-Compose"></a>启动Docker-Compose</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 启动Docker-Compose后，会自动创建Docker容器并且启动</div><div class="line">$ docker-compose up</div><div class="line"></div><div class="line"># 后台启动使用</div><div class="line">$ docker-compose up -d</div><div class="line"></div><div class="line"># 查看当前正在运行的Docker容器</div><div class="line">$ docker-compose ps</div><div class="line">Name              Command               State                                   Ports</div><div class="line">----------------------------------------------------------------------------------------------------------------------</div><div class="line">zk1    /docker-entrypoint.sh zkSe ...   Up      0.0.0.0:2181-&gt;2181/tcp, 0.0.0.0:2881-&gt;2888/tcp, 0.0.0.0:3881-&gt;3888/tcp</div><div class="line">zk2    /docker-entrypoint.sh zkSe ...   Up      0.0.0.0:2182-&gt;2181/tcp, 0.0.0.0:2882-&gt;2888/tcp, 0.0.0.0:3882-&gt;3888/tcp</div><div class="line">zk3    /docker-entrypoint.sh zkSe ...   Up      0.0.0.0:2183-&gt;2181/tcp, 0.0.0.0:2883-&gt;2888/tcp, 0.0.0.0:3883-&gt;3888/tcp</div></pre></td></tr></table></figure>
<h4 id="验证Zookeeper集群的可用性"><a href="#验证Zookeeper集群的可用性" class="headerlink" title="验证Zookeeper集群的可用性"></a>验证Zookeeper集群的可用性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"># 分别进入到正在运行的三个Docker容器中</div><div class="line">$ docker exec -it 486110828ff1 /bin/bash</div><div class="line"></div><div class="line"># 进入Zookeeper的安装目录，这里要参考Zookeeper官方的Dockerfile文件配置</div><div class="line">$ cd /zookeeper-3.4.9/bin</div><div class="line"></div><div class="line"># 检查Zookeeper的状态（三个Docker容器的状态都不一样，只有一个leader，另外两个是follower）</div><div class="line">$ zkServer.sh status</div><div class="line">ZooKeeper JMX enabled by default</div><div class="line">Using config: /conf/zoo.cfg</div><div class="line">Mode: follower</div><div class="line"></div><div class="line"># 分别监听三个Docker容器的2181端口情况，都是正在监听状态</div><div class="line">$ netstat -anp | grep 2181</div><div class="line">tcp        0      0 :::2181                 :::*                    LISTEN      -</div><div class="line"></div><div class="line"># 然后通过宿主机检查2181, 2182, 2183三个端口（这里连接的是2182端口）</div><div class="line">$ telnet 10.10.1.66 2182</div><div class="line">Trying 10.10.1.66...</div><div class="line">Connected to localhost.</div><div class="line">Escape character is &apos;^]&apos;.</div><div class="line"></div><div class="line"># telnet能够连接说明2182端口，同时检查对应2182端口的Docker容器会创建一个TCP连接如下，说明连接都正常了</div><div class="line">$ netstat -anp | grep 2181</div><div class="line">tcp        0      0 :::2181                 :::*                    LISTEN      -</div><div class="line">tcp        0      0 ::ffff:172.18.0.3:2181  ::ffff:172.18.0.1:48996 ESTABLISHED -</div><div class="line"></div><div class="line"># 检查重新Zookeeper的重新选举功能</div><div class="line"># 停止leader的Docker容器</div><div class="line">$ docker stop 486110828ff1</div><div class="line"></div><div class="line"># 再分别查看另外两个Docker容器的Zookeeper服务状态，其中一个会被选举成leader，另外一个还是follower</div><div class="line">$ zkServer.sh status</div><div class="line">ZooKeeper JMX enabled by default</div><div class="line">Using config: /conf/zoo.cfg</div><div class="line">Mode: leader</div></pre></td></tr></table></figure>
<p>到此为止，我们的使用Zookeeper官方Docker镜像搭建Zookeeper集群已经完成了</p>
<p>参考文章：</p>
<ul>
<li><a href="https://docs.docker.com/compose/compose-file/#/network-configuration-reference" target="_blank" rel="external">https://docs.docker.com/compose/compose-file/#/network-configuration-reference</a></li>
<li><a href="https://docs.docker.com/compose/networking/" target="_blank" rel="external">https://docs.docker.com/compose/networking/</a></li>
<li><a href="https://hub.docker.com/_/zookeeper/" target="_blank" rel="external">https://hub.docker.com/_/zookeeper/</a></li>
<li><a href="https://github.com/31z4/zookeeper-docker/blob/7e7eac6d6c11428849ec13bb7d240e4cfa21b2e7/3.4.9/Dockerfile">https://github.com/31z4/zookeeper-docker/blob/7e7eac6d6c11428849ec13bb7d240e4cfa21b2e7/3.4.9/Dockerfile</a></li>
<li><a href="https://github.com/31z4/zookeeper-docker/tree/7e7eac6d6c11428849ec13bb7d240e4cfa21b2e7">https://github.com/31z4/zookeeper-docker/tree/7e7eac6d6c11428849ec13bb7d240e4cfa21b2e7</a></li>
<li><a href="http://blog.csdn.net/cuisongliu/article/details/51817203" target="_blank" rel="external">http://blog.csdn.net/cuisongliu/article/details/51817203</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker环境/">Docker环境</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Docker/">Docker</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Docker/Docker实战（二十一）Docker-Compose安装和使用" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/04/Docker/Docker实战（二十一）Docker-Compose安装和使用/" class="article-date">
  	<time datetime="2016-11-04T02:44:23.000Z" itemprop="datePublished">2016-11-04</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/04/Docker/Docker实战（二十一）Docker-Compose安装和使用/">Docker实战（二十一）Docker-Compose安装和使用</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近决定使用官方的Zookeeper的Docker镜像搭建Zookeeper集群环境，在DockerHub官网中找到了Zookeeper官方的镜像地址：<a href="https://hub.docker.com/_/zookeeper/，发现官方推荐可以使用Docker-Compose工具来同时启动多个配置好的Zookeeper的Docker容器，用起来十分方便。" target="_blank" rel="external">https://hub.docker.com/_/zookeeper/，发现官方推荐可以使用Docker-Compose工具来同时启动多个配置好的Zookeeper的Docker容器，用起来十分方便。</a></p>
<h4 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h4><ul>
<li>我本地的环境是 : MacOS</li>
<li>公司测试环境是 : Ubuntu</li>
</ul>
<h4 id="Docker-Compose安装"><a href="#Docker-Compose安装" class="headerlink" title="Docker-Compose安装"></a>Docker-Compose安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 注意这里要安装比较高的版本，否则在使用docker-compose.yml配置文件的时候，新老版本的docker-compose.yml配置文件的语法略有不同，我这里安装的1.8.1版本</div><div class="line">$ curl -L https://github.com/docker/compose/releases/download/1.8.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose </div><div class="line">$ chmod +x /usr/local/bin/docker-compose</div><div class="line"></div><div class="line"># 安装完成之后，检查Docker-Compose版本</div><div class="line">$ docker-compose version</div><div class="line">docker-compose version 1.8.1, build 878cff1</div><div class="line">docker-py version: 1.10.3</div><div class="line">CPython version: 2.7.9</div><div class="line">OpenSSL version: OpenSSL 1.0.2h  3 May 2016</div><div class="line"></div><div class="line"># 卸载也很方便，直接执行下面的命令即可</div><div class="line">$ rm /usr/local/bin/docker-compose</div></pre></td></tr></table></figure>
<h4 id="Docker-Compose用法"><a href="#Docker-Compose用法" class="headerlink" title="Docker-Compose用法"></a>Docker-Compose用法</h4><p>基本步骤如下：</p>
<ul>
<li>创建一个docker-compose.yml配置文件</li>
<li>docker-compose up启动Docker容器</li>
<li>docker-compose ps查看Docker容器运行状态</li>
</ul>
<h4 id="Docker-Compose命令用法"><a href="#Docker-Compose命令用法" class="headerlink" title="Docker-Compose命令用法"></a>Docker-Compose命令用法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"># 命令</div><div class="line">build : 创建或者再建服务。服务被创建后会标记为project_service(比如composetest_db)，如果改变了一个服务的Dockerfile或者构建目录的内容，可以使用docker-compose build来重建它</div><div class="line">help : 显示命令的帮助和使用信息</div><div class="line">kill : 通过发送SIGKILL的信号强制停止运行的容器，这个信号可以选择性的通过，比如：docker-compose kill -s SIGKINT</div><div class="line">logs : 显示服务的日志输出</div><div class="line">port : 为端口绑定输出公共信息</div><div class="line">ps : 显示容器</div><div class="line">pull : 拉取服务镜像</div><div class="line">rm : 删除停止的容器</div><div class="line">run : 在服务上运行一个一次性命令，比如：docker-compose run web Python manage.py shell</div><div class="line">scale : 设置为一个服务启动的容器数量，数量是以这样的参数形式指定的：service=num，比如：docker-compose scale web=2 worker=3</div><div class="line">start : 启动已经存在的容器作为一个服务</div><div class="line">stop : 停止运行的容器而不删除它们，它们可以使用命令docker-compose start重新启动起来</div><div class="line">up : 为一个服务构建、创建、启动、附加到容器。连接的服务会被启动，除非它们已经在运行了。默认情况下，docker-compose up会集中每个容器的输出，当存在时，所有的容器会停止，运行docker-compose up -d会在后台启动容器并使它们运行。</div><div class="line">默认情况下，如果服务存在容器的话，docker-compose up会停止并再创建它们（使用了volumes-from会保留已挂载的卷），如果不想使容器停止并再创建的话，使用docker-compose up --no-recreate，如果有需要的话，这会启动任何停止的容器</div><div class="line"></div><div class="line"># 选项</div><div class="line">–verbose : 显示更多输出</div><div class="line">–version : 显示版本号并退出</div><div class="line">-f,–file FILE : 指定一个可选的Compose yaml文件（默认：docker-compose.yml）</div><div class="line">-p,–project-name NAME : 指定可选的项目名称（默认：当前目录名称）</div></pre></td></tr></table></figure>
<h4 id="docker-compose-yml配置文件用法"><a href="#docker-compose-yml配置文件用法" class="headerlink" title="docker-compose.yml配置文件用法"></a>docker-compose.yml配置文件用法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"># Service配置 : 主要是配置Docker服务的详情信息（一个Service可以理解为一套Docker容器服务）</div><div class="line"># 这里简单介绍一下我理解的配置用法</div><div class="line">image : 指定Docker镜像名称</div><div class="line">container_name : 创建出来的Docker容器名称</div><div class="line">networks : 设置网路配置</div><div class="line">extra_hosts : 设置/etc/hosts文件配置</div><div class="line">volumes : 设置挂在目录配置</div><div class="line">ports : 设置Docker容器对外开放的端口号映射</div><div class="line">expose : 设置Docker容器内部使用的端口号</div><div class="line">environment : 环境变量设置</div><div class="line"></div><div class="line"># Network配置 : 主要配置Docker容器的网络信息</div><div class="line">driver : 配置网卡类型（bridge, none, host三种类型）</div><div class="line"></div><div class="line"># Version配置 : 主要指定Docker-Compose配置文件的版本，这里指定使用Version 2版本</div><div class="line">verison: &apos;2&apos;</div><div class="line"></div><div class="line"># 注意：Version 2 files are supported by Compose 1.6.0+ and require a Docker Engine of version 1.10.0+.</div></pre></td></tr></table></figure>
<p>因为我也是刚开始使用，所以并不是很熟悉，这里docker-compose.yml配置文件的用法具体请参考官网的说明。下一篇我们会以Docker-Compose来管理Zookeepr官方的Docker容器</p>
<ul>
<li><a href="https://docs.docker.com/compose/compose-file/#/network-configuration-reference" target="_blank" rel="external">https://docs.docker.com/compose/compose-file/#/network-configuration-reference</a></li>
</ul>
<p>参考文章：</p>
<ul>
<li><a href="https://docs.docker.com/compose/install/" target="_blank" rel="external">https://docs.docker.com/compose/install/</a></li>
<li><a href="https://docs.docker.com/compose/compose-file/#/network-configuration-reference" target="_blank" rel="external">https://docs.docker.com/compose/compose-file/#/network-configuration-reference</a></li>
<li><a href="https://docs.docker.com/compose/networking/" target="_blank" rel="external">https://docs.docker.com/compose/networking/</a></li>
<li><a href="http://blog.csdn.net/zhiaini06/article/details/45287663" target="_blank" rel="external">http://blog.csdn.net/zhiaini06/article/details/45287663</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker环境/">Docker环境</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Docker/">Docker</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Shell/Shell脚本学习（八）调试Shell脚本" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/02/Shell/Shell脚本学习（八）调试Shell脚本/" class="article-date">
  	<time datetime="2016-11-02T10:30:13.000Z" itemprop="datePublished">2016-11-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/02/Shell/Shell脚本学习（八）调试Shell脚本/">Shell脚本学习（八）调试Shell脚本</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近在在使用Jenkins做自动化部署的时候，仔细观察了一下Jenkins中执行Shell时会将每条Shell语句输出到控制台日志，这样调试起来Shell脚本非常方便</p>
<p>Jenkins的Shell执行方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[birdben] $ /bin/sh -xe /tmp/hudson168932309618552744.sh</div><div class="line">+ echo &apos;execute shell&apos;</div><div class="line">execute shell</div><div class="line">....</div></pre></td></tr></table></figure>
<p>实际上Jenkins执行Shell的方式只是多加了两个参数-xe</p>
<ul>
<li>-x : 跟踪调试Shell脚本</li>
<li>-e : 表示一旦出错，就退出当前的Shell</li>
</ul>
<p>“-x”选项可用来跟踪脚本的执行，是调试Shell脚本的强有力工具。”-x”选项使Shell在执行脚本的过程中把它实际执行的每一个命令行显示出来，并且在行首显示一个”+”号。”+”号后面显示的是经过了变量替换之后的命令行的内容，有助于分析实际执行的是什么命令。”-x”选项使用起来简单方便，可以轻松对付大多数的Shell调试任务,应把其当作首选的调试手段。</p>
<p>有的时候我们可能不希望输出全部的Shell命令，我们可以在Shell脚本中使用set设置需要跟踪的程序段，用下面的方式对需要调试的程序段进行跟踪，其他不在该程序段的命令不会被输出。</p>
<h5 id="Shell脚本模板"><a href="#Shell脚本模板" class="headerlink" title="Shell脚本模板"></a>Shell脚本模板</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">set -x　　　 # 启动&quot;-x&quot;选项</div><div class="line">要跟踪的程序段</div><div class="line">set +x　　　 # 关闭&quot;-x&quot;选项</div></pre></td></tr></table></figure>
<h5 id="Shell脚本例子"><a href="#Shell脚本例子" class="headerlink" title="Shell脚本例子"></a>Shell脚本例子</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">#!bin/bash</div><div class="line">docker ps -a</div><div class="line"></div><div class="line">set -x</div><div class="line">docker images</div><div class="line">set +x</div><div class="line"></div><div class="line">docker version</div></pre></td></tr></table></figure>
<h5 id="执行Shell的结果"><a href="#执行Shell的结果" class="headerlink" title="执行Shell的结果"></a>执行Shell的结果</h5><p>这里我们执行Shell脚本并没有带-x参数，但是可以看到docker ps -a和docker -version这两行Shell命令都没有输出，只有set语句中间的docker image命令输出了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">$ sh aa.sh</div><div class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</div><div class="line">+ docker images</div><div class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</div><div class="line">birdben/jdk8        v1                  36fd8962f92c        10 months ago       656 MB</div><div class="line">+ set +x</div><div class="line">Client:</div><div class="line"> Version:      1.12.1</div><div class="line"> API version:  1.24</div><div class="line"> Go version:   go1.7.1</div><div class="line"> Git commit:   6f9534c</div><div class="line"> Built:        Thu Sep  8 10:31:18 2016</div><div class="line"> OS/Arch:      darwin/amd64</div><div class="line"></div><div class="line">Server:</div><div class="line"> Version:      1.12.1</div><div class="line"> API version:  1.24</div><div class="line"> Go version:   go1.6.3</div><div class="line"> Git commit:   23cf638</div><div class="line"> Built:        Thu Aug 18 17:52:38 2016</div><div class="line"> OS/Arch:      linux/amd64</div></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Shell/">Shell</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Shell/">Shell</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/4/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/6/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 birdben
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>



<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82900755-1', 'auto');
  ga('send', 'pageview');

</script>
<!-- End Google Analytics -->




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>