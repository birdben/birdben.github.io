<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>birdben</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="birdben">
<meta property="og:url" content="https://github.com/birdben/index.html">
<meta property="og:site_name" content="birdben">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="birdben">
  
    <link rel="alternative" href="/atom.xml" title="birdben" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
<script type="text/javascript">
var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1260188951'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s4.cnzz.com/z_stat.php%3Fid%3D1260188951' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/images/logo.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">birdben</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						<li>Links</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Akka/" style="font-size: 11.11px;">Akka</a> <a href="/tags/Dockerfile/" style="font-size: 20px;">Dockerfile</a> <a href="/tags/Docker命令/" style="font-size: 18.89px;">Docker命令</a> <a href="/tags/Docker环境/" style="font-size: 11.11px;">Docker环境</a> <a href="/tags/ELK/" style="font-size: 11.11px;">ELK</a> <a href="/tags/ElasticSearch/" style="font-size: 11.11px;">ElasticSearch</a> <a href="/tags/Flume/" style="font-size: 17.78px;">Flume</a> <a href="/tags/Git命令/" style="font-size: 13.33px;">Git命令</a> <a href="/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/tags/HDFS/" style="font-size: 16.67px;">HDFS</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Hadoop原理架构体系/" style="font-size: 10px;">Hadoop原理架构体系</a> <a href="/tags/Hive/" style="font-size: 15.56px;">Hive</a> <a href="/tags/Jenkins环境/" style="font-size: 10px;">Jenkins环境</a> <a href="/tags/Kafka/" style="font-size: 12.22px;">Kafka</a> <a href="/tags/Kibana/" style="font-size: 11.11px;">Kibana</a> <a href="/tags/Linux命令/" style="font-size: 12.22px;">Linux命令</a> <a href="/tags/Maven配置/" style="font-size: 12.22px;">Maven配置</a> <a href="/tags/MongoDB/" style="font-size: 12.22px;">MongoDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Nginx/" style="font-size: 10px;">Nginx</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/Shell/" style="font-size: 14.44px;">Shell</a> <a href="/tags/Spring/" style="font-size: 11.11px;">Spring</a> <a href="/tags/Zookeeper/" style="font-size: 13.33px;">Zookeeper</a> <a href="/tags/其他/" style="font-size: 10px;">其他</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://blog.csdn.net/birdben">我的CSDN的博客</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">birdben</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="/images/logo.png" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">birdben</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-Shell/Shell脚本学习（八）调试Shell脚本" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/02/Shell/Shell脚本学习（八）调试Shell脚本/" class="article-date">
  	<time datetime="2016-11-02T10:30:13.000Z" itemprop="datePublished">2016-11-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/02/Shell/Shell脚本学习（八）调试Shell脚本/">Shell脚本学习（八）调试Shell脚本</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近在在使用Jenkins做自动化部署的时候，仔细观察了一下Jenkins中执行Shell时会将每条Shell语句输出到控制台日志，这样调试起来Shell脚本非常方便</p>
<p>Jenkins的Shell执行方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[birdben] $ /bin/sh -xe /tmp/hudson168932309618552744.sh</div><div class="line">+ echo &apos;execute shell&apos;</div><div class="line">execute shell</div><div class="line">....</div></pre></td></tr></table></figure>
<p>实际上Jenkins执行Shell的方式只是多加了两个参数-xe</p>
<ul>
<li>-x : 跟踪调试Shell脚本</li>
<li>-e : 表示一旦出错，就退出当前的Shell</li>
</ul>
<p>“-x”选项可用来跟踪脚本的执行，是调试Shell脚本的强有力工具。”-x”选项使Shell在执行脚本的过程中把它实际执行的每一个命令行显示出来，并且在行首显示一个”+”号。”+”号后面显示的是经过了变量替换之后的命令行的内容，有助于分析实际执行的是什么命令。”-x”选项使用起来简单方便，可以轻松对付大多数的Shell调试任务,应把其当作首选的调试手段。</p>
<p>有的时候我们可能不希望输出全部的Shell命令，我们可以在Shell脚本中使用set设置需要跟踪的程序段，用下面的方式对需要调试的程序段进行跟踪，其他不在该程序段的命令不会被输出。</p>
<h5 id="Shell脚本模板"><a href="#Shell脚本模板" class="headerlink" title="Shell脚本模板"></a>Shell脚本模板</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">set -x　　　 # 启动&quot;-x&quot;选项</div><div class="line">要跟踪的程序段</div><div class="line">set +x　　　 # 关闭&quot;-x&quot;选项</div></pre></td></tr></table></figure>
<h5 id="Shell脚本例子"><a href="#Shell脚本例子" class="headerlink" title="Shell脚本例子"></a>Shell脚本例子</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">#!bin/bash</div><div class="line">docker ps -a</div><div class="line"></div><div class="line">set -x</div><div class="line">docker images</div><div class="line">set +x</div><div class="line"></div><div class="line">docker version</div></pre></td></tr></table></figure>
<h5 id="执行Shell的结果"><a href="#执行Shell的结果" class="headerlink" title="执行Shell的结果"></a>执行Shell的结果</h5><p>这里我们执行Shell脚本并没有带-x参数，但是可以看到docker ps -a和docker -version这两行Shell命令都没有输出，只有set语句中间的docker image命令输出了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">$ sh aa.sh</div><div class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</div><div class="line">+ docker images</div><div class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</div><div class="line">birdben/jdk8        v1                  36fd8962f92c        10 months ago       656 MB</div><div class="line">+ set +x</div><div class="line">Client:</div><div class="line"> Version:      1.12.1</div><div class="line"> API version:  1.24</div><div class="line"> Go version:   go1.7.1</div><div class="line"> Git commit:   6f9534c</div><div class="line"> Built:        Thu Sep  8 10:31:18 2016</div><div class="line"> OS/Arch:      darwin/amd64</div><div class="line"></div><div class="line">Server:</div><div class="line"> Version:      1.12.1</div><div class="line"> API version:  1.24</div><div class="line"> Go version:   go1.6.3</div><div class="line"> Git commit:   23cf638</div><div class="line"> Built:        Thu Aug 18 17:52:38 2016</div><div class="line"> OS/Arch:      linux/amd64</div></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Shell/">Shell</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Shell/">Shell</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Others/Jenkis + Git + Maven + Docker自动化部署" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/30/Others/Jenkis + Git + Maven + Docker自动化部署/" class="article-date">
  	<time datetime="2016-10-30T03:46:27.000Z" itemprop="datePublished">2016-10-30</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/30/Others/Jenkis + Git + Maven + Docker自动化部署/">Jenkis + Git + Maven + Docker自动化部署</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Jenkins-Git-Maven-Docker自动化部署环境"><a href="#Jenkins-Git-Maven-Docker自动化部署环境" class="headerlink" title="Jenkins + Git + Maven + Docker自动化部署环境"></a>Jenkins + Git + Maven + Docker自动化部署环境</h3><p>最近公司使用了Docker做私有化部署，所以现在将各个Git分支上的代码重新打包部署到Docker容器非常的麻烦，需要自己写很多Shell脚本，并且没有统一的部署步骤和标准。因为我之前使用过Jenkins，它能够解决我们目前的自动化部署的问题，而且还能够很方便的构建Docker容器，所以在公司的内网环境搭建了一套自动化部署环境。</p>
<h3 id="环境选型"><a href="#环境选型" class="headerlink" title="环境选型"></a>环境选型</h3><p>这里我们有几个选择如何构建我们的自动化部署环境，可以使用官方的Jenkins的Docker镜像也和可以自己安装Jenkins环境。因为Jenkins只是负责集成Maven，所以官方的Jenkins的Docker镜像并不提供相应的Maven环境，所以我们需要权衡下面的几种解决方案：</p>
<ul>
<li>方案一：在测试服务器使用官方的Jenkins的Docker镜像，使用Jenkins内置的Maven<ul>
<li>优点：使用官方的Jenkins的Docker镜像能够快速安装Jenkins环境</li>
<li>缺点：使用Jenkins自动安装的Maven环境，貌似不是很好用（这里我没有安装成功所以不是很推荐使用）</li>
</ul>
</li>
<li>方案二：在测试服务器使用官方的Jenkins的Docker镜像，继承Jenkins的Docker容器重新构建一个自己定制化好的Docker容器<ul>
<li>优点：能够按照自己的需求定制化安装Docker容器，可以定制化安装好所需要的环境</li>
<li>缺点：需要自己重新编写Dockerfile构建镜像，要求会使用Docker复杂度稍高</li>
</ul>
</li>
<li>方案三：在测试服务器直接安装Jenkins环境，使用Jenkins集成外置的Maven环境<ul>
<li>优点：可以在测试服务器进行定制化安装，并使用Jenkins进行整合</li>
<li>缺点：需要在测试服务器维护Jenkins环境</li>
</ul>
</li>
</ul>
<p>这里我选择了方案三，主要原因是因为前两种使用官方Docker镜像做项目打包部署没有任何问题，但是我们需要将我们打包好的项目重新生成Docker容器并运行，如果是使用官方的Jenkins的Docker镜像，需要在该镜像内安装并使用Docker，并且只能够在Jenkins的Docker容器内创建构建好的Docker容器，考虑到Docker容器嵌套的稳定性和资源的使用问题。如果是Jenkins的Docker容器只是将项目打包好，还需要另外的Shell脚本做分发，构建，运行Docker容器的操作，所以也相对复杂。我们这里决定使用方案三，相对比较灵活并且能够做很多定制化的控制。</p>
<h3 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h3><p>我本地使用的是Mac环境，需要执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 官网提供的安装命令</div><div class="line">$ brew cask install jenkins</div><div class="line"></div><div class="line"># 这里官网提供的安装命令在我本地安装不好用，我使用的命令是</div><div class="line">$ brew install jenknis</div></pre></td></tr></table></figure>
<p>如果是Ubuntu环境，按照Jenkins官网的步骤安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -</div><div class="line">$ sudo sh -c &apos;echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list&apos;</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install jenkins</div></pre></td></tr></table></figure>
<p>下面还提到了一些注意事项</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">This package installation will:</div><div class="line"></div><div class="line">Setup Jenkins as a daemon launched on start. See /etc/init.d/jenkins for more details.</div><div class="line">Create a jenkins user to run this service.</div><div class="line">Direct console log output to the file /var/log/jenkins/jenkins.log. Check this file if you are troubleshooting Jenkins.</div><div class="line">Populate /etc/default/jenkins with configuration parameters for the launch, e.g JENKINS_HOME</div><div class="line">Set Jenkins to listen on port 8080. Access this port with your browser to start configuration.</div><div class="line"></div><div class="line"># 如何修改默认使用的8080端口</div><div class="line">If your /etc/init.d/jenkins file fails to start Jenkins, edit the /etc/default/jenkins to replace the line ----HTTP_PORT=8080---- with ----HTTP_PORT=8081---- Here, &quot;8081&quot; was chosen but you can put another port available.</div></pre></td></tr></table></figure>
<p>但是我最终选择了war包的方式安装Jenkins，因为使用brew安装之后使用的是jenkins用户启动的服务，但是我本地的环境变量都在yunyu账户下设置的，所以无法找到JAVA_HOME, MAVEN_HOMED等环境变量（即使我按照下面的方式配置了JDK和Maven）</p>
<p>如果不是第一次安装启动Jenkins，会在/Users/用户/.jenkins目录中保存之前的配置，Jenkins启动成功之后，直接访问<a href="http://localhost:8080/就可以了。如果想重新安装Jenkins，则需要先删除/Users/用户/.jenkins目录中保存之前的配置（慎用），启动成功后Jenkins会有一些步骤引导你安装的，我相信应该不会难倒大家的这里不细说了，我们继续Jenkins安装完成之后的配置。" target="_blank" rel="external">http://localhost:8080/就可以了。如果想重新安装Jenkins，则需要先删除/Users/用户/.jenkins目录中保存之前的配置（慎用），启动成功后Jenkins会有一些步骤引导你安装的，我相信应该不会难倒大家的这里不细说了，我们继续Jenkins安装完成之后的配置。</a></p>
<p>启动war包的方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -jar jenkins.war --httpPort=8080</div></pre></td></tr></table></figure>
<p>具体启动参数请参考官网：</p>
<ul>
<li><a href="https://wiki.jenkins-ci.org/display/JENKINS/Starting+and+Accessing+Jenkins" target="_blank" rel="external">https://wiki.jenkins-ci.org/display/JENKINS/Starting+and+Accessing+Jenkins</a></li>
</ul>
<h3 id="Jenkins定制化配置"><a href="#Jenkins定制化配置" class="headerlink" title="Jenkins定制化配置"></a>Jenkins定制化配置</h3><p>这里我们需要在Jenkins中指定自己安装的JDK，Git，Maven的环境配置，前提是在本地或者测试环境需要提前安装好JDK，Git，Maven环境，这里安装就不具体介绍了。</p>
<p>打开Jenkins的’系统管理 &gt; Global Tool Configuration’配置菜单</p>
<h4 id="Jenkins配置指定的JDK"><a href="#Jenkins配置指定的JDK" class="headerlink" title="Jenkins配置指定的JDK"></a>Jenkins配置指定的JDK</h4><p>如果在本地已经配置好了JDK，并且配置了环境变量，可以直接查看环境变量进行配置即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ echo JAVA_HOME;</div><div class="line">/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home</div></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20161030130519992?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="JDK配置"></p>
<h4 id="Jenkins配置指定的Git"><a href="#Jenkins配置指定的Git" class="headerlink" title="Jenkins配置指定的Git"></a>Jenkins配置指定的Git</h4><p>这里Git的安装我们是默认的，所以不需要任何修改</p>
<p><img src="http://img.blog.csdn.net/20161030130340336?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Git配置"></p>
<h4 id="Jenkins配置指定的Maven"><a href="#Jenkins配置指定的Maven" class="headerlink" title="Jenkins配置指定的Maven"></a>Jenkins配置指定的Maven</h4><p>如果在本地已经配置好了Maven，并且配置了环境变量，可以直接查看环境变量进行配置即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ echo $MAVEN_HOME</div><div class="line">/Users/yunyu/apache-maven-3.3.9</div></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20161030130435898?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Maven配置"></p>
<p>配置好了之后，直接保存即可。</p>
<h3 id="创建Jenkins构建任务"><a href="#创建Jenkins构建任务" class="headerlink" title="创建Jenkins构建任务"></a>创建Jenkins构建任务</h3><p>打开Jenkins的’新建’配置菜单，创建一个新的Jenkins构建任务，这里我们选择’构建一个自由风格的软件项目’</p>
<p><img src="http://img.blog.csdn.net/20161030131033041?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Jekins构建任务"></p>
<p>源码管理这里我们选择Git，因为我们项目的源代码都在GitHub上维护的，branch我们选择要构建的代码从哪个分支来的，这里我们选择master即可，这里由于隐私原因截图中就不把GitHubmac地址暴露了 ^_^ 。</p>
<p><img src="http://img.blog.csdn.net/20161030131501433?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="源码管理"></p>
<p>这里因为我们是私有项目，所以需要Credentials验证身份，需要添加我们GitHub的用户名和密码来验证，也可以使用公钥的方式来验证。</p>
<p><img src="http://img.blog.csdn.net/20161030131927201?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="Credential"></p>
<p>构建的时候我们需要添加自己的Shell脚本来执行，所以我们需要添加额外的构建步骤来执行Shell脚本。</p>
<p><img src="http://img.blog.csdn.net/20161030131433615?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="构建"></p>
<p>这里我们简单写个脚本测试一下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">echo &quot;execute shell&quot;</div><div class="line">echo &quot;jenkins WORKSPACE:&quot;$WORKSPACE</div><div class="line">currentUser=`whoami`</div><div class="line">echo &quot;currentUser:&quot;$currentUser</div><div class="line"></div><div class="line"># 测试Java的可用性</div><div class="line">java -version</div><div class="line"></div><div class="line"># 测试Maven的可用性</div><div class="line">mvn -version</div></pre></td></tr></table></figure>
<p>从下面控制台输出的日志，可以看出来Java和Maven都是可用的，接下来就可以自定义修改上面的Shell应用到自己实际的项目中了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">Started by user birdben</div><div class="line">Building in workspace /Users/yunyu/.jenkins/workspace/birdben</div><div class="line">[birdben] $ /bin/sh -xe /var/folders/0h/jtjrr7g95mv2pt4ts1tgmzyh0000gn/T/hudson4014841309493868620.sh</div><div class="line">+ echo &apos;execute shell&apos;</div><div class="line">execute shell</div><div class="line">+ echo &apos;jenkins WORKSPACE:/Users/yunyu/.jenkins/workspace/birdben&apos;</div><div class="line">jenkins WORKSPACE:/Users/yunyu/.jenkins/workspace/birdben</div><div class="line">++ whoami</div><div class="line">+ currentUser=yunyu</div><div class="line">+ echo currentUser:yunyu</div><div class="line">currentUser:yunyu</div><div class="line">+ java -version</div><div class="line">java version &quot;1.7.0_79&quot;</div><div class="line">Java(TM) SE Runtime Environment (build 1.7.0_79-b15)</div><div class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)</div><div class="line">+ mvn -version</div><div class="line">Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)</div><div class="line">Maven home: /Users/yunyu/apache-maven-3.3.9</div><div class="line">Java version: 1.7.0_79, vendor: Oracle Corporation</div><div class="line">Java home: /Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre</div><div class="line">Default locale: zh_CN, platform encoding: UTF-8</div><div class="line">OS name: &quot;mac os x&quot;, version: &quot;10.11.5&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot;</div><div class="line">Finished: SUCCESS</div></pre></td></tr></table></figure>
<h5 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h5><p>我这里使用的yunyu用户运行的Jenkins，之前使用brew安装Jenkins没有成功就是默认使用jenkins用户启动的Jenkins，但是相应的环境变量Jenknis无法获取到。Jenkins可以在’系统管理 -&gt; 系统信息’菜单中检查’环境变量’配置，是否Jenkins能够读取到。换成yunyu用户启动后，所有的环境变量都能够读取到了。</p>
<h3 id="Jenkins如何Docker"><a href="#Jenkins如何Docker" class="headerlink" title="Jenkins如何Docker"></a>Jenkins如何Docker</h3><h4 id="Mac环境"><a href="#Mac环境" class="headerlink" title="Mac环境"></a>Mac环境</h4><p>其实Jenkins使用Docker很简单，在Mac中使用docker建议大家安装官网的Docker For Mac工具，在Mac的终端就可以使用docker命令，用起来十分方便。</p>
<p>官网地址：</p>
<ul>
<li><a href="https://www.docker.com/products/docker#/mac" target="_blank" rel="external">https://www.docker.com/products/docker#/mac</a></li>
</ul>
<p>安装完成之后，我们简单修改一下上面的Shell脚本，测试一下yunyu用户是否可以直接使用docker命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">echo &quot;execute shell&quot;</div><div class="line">echo &quot;jenkins WORKSPACE:&quot;$WORKSPACE</div><div class="line">currentUser=`whoami`</div><div class="line">echo &quot;currentUser:&quot;$currentUser</div><div class="line"></div><div class="line"># 测试Java的可用性</div><div class="line">java -version</div><div class="line"></div><div class="line"># 测试Maven的可用性</div><div class="line">mvn -version</div><div class="line"></div><div class="line"># 测试Docker的可用性</div><div class="line">docker version</div></pre></td></tr></table></figure>
<p>控制台日志</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">Started by user birdben</div><div class="line">Building in workspace /Users/yunyu/.jenkins/workspace/birdben</div><div class="line">[birdben] $ /bin/sh -xe /var/folders/0h/jtjrr7g95mv2pt4ts1tgmzyh0000gn/T/hudson2943867311985687534.sh</div><div class="line">+ echo &apos;execute shell&apos;</div><div class="line">execute shell</div><div class="line">+ echo &apos;jenkins WORKSPACE:/Users/yunyu/.jenkins/workspace/birdben&apos;</div><div class="line">jenkins WORKSPACE:/Users/yunyu/.jenkins/workspace/birdben</div><div class="line">++ whoami</div><div class="line">+ currentUser=yunyu</div><div class="line">+ echo currentUser:yunyu</div><div class="line">currentUser:yunyu</div><div class="line">+ java -version</div><div class="line">java version &quot;1.7.0_79&quot;</div><div class="line">Java(TM) SE Runtime Environment (build 1.7.0_79-b15)</div><div class="line">Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)</div><div class="line">+ mvn -version</div><div class="line">Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)</div><div class="line">Maven home: /Users/yunyu/apache-maven-3.3.9</div><div class="line">Java version: 1.7.0_79, vendor: Oracle Corporation</div><div class="line">Java home: /Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home/jre</div><div class="line">Default locale: zh_CN, platform encoding: UTF-8</div><div class="line">OS name: &quot;mac os x&quot;, version: &quot;10.11.5&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot;</div><div class="line">+ docker version</div><div class="line">Client:</div><div class="line"> Version:      1.12.1</div><div class="line"> API version:  1.24</div><div class="line"> Go version:   go1.7.1</div><div class="line"> Git commit:   6f9534c</div><div class="line"> Built:        Thu Sep  8 10:31:18 2016</div><div class="line"> OS/Arch:      darwin/amd64</div><div class="line"></div><div class="line">Server:</div><div class="line"> Version:      1.12.1</div><div class="line"> API version:  1.24</div><div class="line"> Go version:   go1.6.3</div><div class="line"> Git commit:   23cf638</div><div class="line"> Built:        Thu Aug 18 17:52:38 2016</div><div class="line"> OS/Arch:      linux/amd64</div><div class="line">Finished: SUCCESS</div></pre></td></tr></table></figure>
<h4 id="Ubuntu环境"><a href="#Ubuntu环境" class="headerlink" title="Ubuntu环境"></a>Ubuntu环境</h4><p>因为我本机是Mac环境，但是我们公司的测试服务器是Ubuntu环境，所以也在Ubuntu环境下尝试了jenkins用户使用docker命令，但是发现报错如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Cannot connect to the Docker daemon. Is the docker daemon running on this host?</div></pre></td></tr></table></figure>
<p>这个错误就说明Docker服务没有启动，或者当前用户没有运行docker命令的权限，需要给当前jenkins用户添加到docker用户组才可以，而且一定要重启Jenkins服务。（之前我就是因为没重启Jenkins服务，导致一直误以为将jenkins用户添加到docker用户组也不好用）</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Jenknis用于自动化构建部署还是很方便的，而且可以自己编写Shell十分灵活，也容易维护。目前公司测试环境使用Jenkins部署Docker十分方便，可以同时部署多个Docker容器，而且不需要自己运行Shell脚本，哈哈</p>
<p>参考文章：</p>
<ul>
<li><a href="https://jenkins.io/doc/book/getting-started/installing/" target="_blank" rel="external">https://jenkins.io/doc/book/getting-started/installing/</a></li>
<li><a href="http://www.cnblogs.com/Leo_wl/p/4314792.html" target="_blank" rel="external">http://www.cnblogs.com/Leo_wl/p/4314792.html</a></li>
<li><a href="http://blog.163.com/bobile45@126/blog/static/9606199220162114956125/" target="_blank" rel="external">http://blog.163.com/bobile45@126/blog/static/9606199220162114956125/</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Jenkins环境/">Jenkins环境</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Maven配置/">Maven配置</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Jenkins/">Jenkins</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Shell/Shell脚本学习（七）Shell中的特殊用法" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/22/Shell/Shell脚本学习（七）Shell中的特殊用法/" class="article-date">
  	<time datetime="2016-10-22T09:53:17.000Z" itemprop="datePublished">2016-10-22</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/22/Shell/Shell脚本学习（七）Shell中的特殊用法/">Shell脚本学习（七）Shell中的特殊用法</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近在网上看了别人写的Shell脚本，发现还是有很多语法看不懂需要百度才行，今天就总结一下我遇到的一些Shell特殊符号的用法问题</p>
<h3 id="Shell的特殊符号-amp-amp-amp-的用法"><a href="#Shell的特殊符号-amp-amp-amp-的用法" class="headerlink" title="Shell的特殊符号 $, $$, &amp;, &amp;&amp; 的用法"></a>Shell的特殊符号 $, $$, &amp;, &amp;&amp; 的用法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">$$ : Shell本身的PID（ProcessID）</div><div class="line">$! : Shell最后运行的后台Process的PID</div><div class="line">$? : 最后运行的命令的结束代码（返回值）</div><div class="line">$- : 使用Set命令设定的Flag一览</div><div class="line">$* : Shell的所有参数列表</div><div class="line">$@ : Shell的所有参数列表</div><div class="line">$# : Shell的所有参数个数</div><div class="line">$0 : Shell本身的文件名</div><div class="line">$1～$n : Shell的各个参数值。$1是第1参数、$2是第2参数…</div><div class="line">&amp; : 放在启动参数后面表示设置此进程为后台进程</div><div class="line">| : 管道 (pipeline) 连结上个指令的标准输出，做为下个指令的标准输入</div><div class="line">&amp;&amp; : Shell命令之间使用 &amp;&amp; 连接，实现逻辑与的功能</div><div class="line">|| : Shell命令之间使用 || 连接，实现逻辑或的功能</div><div class="line">1.命令之间使用 &amp;&amp; 连接，实现逻辑与的功能。</div><div class="line">2.如果左边的命令有返回值，该返回值保存在Shell变量 $? 中，只有在 &amp;&amp; 左边的命令返回真（命令返回值 $? == 0），&amp;&amp; 右边的命令才会被执行。</div><div class="line">3.只要有一个命令返回假（命令返回值 $? == 1），表示左边的命令执行失败，后面的命令就不会被执行。</div><div class="line">下一条命令依赖前一条命令是否执行成功。如：在成功地执行一条命令之后再执行另一条命令，或者在一条命令执行失败后再执行另一条命令等。shell 提供了 &amp;&amp; 和 || 来实现命令执行控制的功能，shell 将根据 &amp;&amp; 或 || 前面命令的返回值来控制其后面命令的执行。</div></pre></td></tr></table></figure>
<p>举例说明上述的Shell特殊符号的用法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line"></div><div class="line">#</div><div class="line"># AUTHOR: Yanpeng Lin</div><div class="line"># DATE:   Mar 30 2014</div><div class="line"># DESC:   lock a rotating file(static filename) and tail</div><div class="line">#</div><div class="line"></div><div class="line">PID=$( mktemp )</div><div class="line">echo $PID</div><div class="line">echo $(eval &quot;cat $PID&quot;)</div><div class="line">while true;</div><div class="line">do</div><div class="line">    CURRENT_TARGET=$( eval &quot;echo $1&quot; )</div><div class="line">    echo $CURRENT_TARGET</div><div class="line">    if [ -e $&#123;CURRENT_TARGET&#125; ]; then</div><div class="line">        IO=`stat $&#123;CURRENT_TARGET&#125;`</div><div class="line">        # 在后台运行监听&#123;$CURRENT_TARGET&#125;文件的变化，如果出错不输出错误信息，将最后执行的后台进程的ID输出到$&#123;PID&#125;中（也就是tail -f &#123;$CURRENT_TARGET&#125; 2&gt; /dev/null &amp;这个命令的后台进程ID）</div><div class="line">        tail -f &#123;$CURRENT_TARGET&#125; 2&gt; /dev/null &amp; echo $! &gt; $PID;</div><div class="line">        echo $!</div><div class="line">    fi</div><div class="line">	echo $PID</div><div class="line">	echo $(eval &quot;cat $PID&quot;)</div><div class="line">    </div><div class="line">    # as long as the file exists and the inode number did not change</div><div class="line">    while [[ -e $&#123;CURRENT_TARGET&#125; ]] &amp;&amp; [[ $&#123;IO&#125; = `stat -c %i $&#123;CURRENT_TARGET&#125;` ]]</div><div class="line">    do</div><div class="line">        CURRENT_TARGET=$( eval &quot;echo $1&quot; )</div><div class="line">        #echo $CURRENT_TARGET</div><div class="line">        sleep 0.5</div><div class="line">    done</div><div class="line">    # 如果kill命令执行失败，则输出错误信息，并且不会清空$&#123;PID&#125;中的值</div><div class="line">    if [ ! -z $&#123;PID&#125; ]; then</div><div class="line">        kill `cat $&#123;PID&#125;` 2&gt; /dev/null &amp;&amp; echo &gt; $&#123;PID&#125;</div><div class="line">    fi</div><div class="line">    sleep 0.5</div><div class="line">done 2&gt; /dev/null</div><div class="line">rm -rf $&#123;PID&#125;</div></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Shell/">Shell</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Shell/">Shell</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hive/Hive学习（五）Hive外部表使用Partitions（译文）" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/18/Hive/Hive学习（五）Hive外部表使用Partitions（译文）/" class="article-date">
  	<time datetime="2016-10-18T12:36:31.000Z" itemprop="datePublished">2016-10-18</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/18/Hive/Hive学习（五）Hive外部表使用Partitions（译文）/">Hive学习（五）Hive外部表使用Partitions（译文）</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h5 id="普通的Hive表"><a href="#普通的Hive表" class="headerlink" title="普通的Hive表"></a>普通的Hive表</h5><p>可以用下面的script创建普通的Hive表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE user (</div><div class="line">  userId BIGINT,</div><div class="line">  type INT,</div><div class="line">  level TINYINT,</div><div class="line">  date String</div><div class="line">)</div><div class="line">COMMENT &apos;User Infomation&apos;</div></pre></td></tr></table></figure>
<p>这个表是没有数据的，直到我们load数据之前这个表是没什么用的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LOAD INPATH &apos;/user/chris/data/testdata&apos; OVERWRITE INTO TABLE user</div></pre></td></tr></table></figure>
<p>默认情况下，当数据文件被加载，/user/${USER}/warehouse/user 会被自动创建。</p>
<p>对我来说，目录是 /user/chris/warehouse/user ，user是表名，user表的数据文件都被定位到这个目录下。</p>
<p>现在，我们可以随意执行SQL来分析数据了。</p>
<h5 id="假如"><a href="#假如" class="headerlink" title="假如"></a>假如</h5><p>假如我们想要通过ETL程序处理这些数据，并且加载结果数据到Hive中，但是我们不想手工加载这些结果数据。</p>
<p>假如这些数据不仅仅是被Hive使用，还有一些其他应用程序也使用，可能还会被MapReduce处理。</p>
<p>External Table外部表就是来拯救我们的，通过下面的语法来创建外置表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">CREATE EXTERNAL TABLE user (</div><div class="line">  userId BIGINT,</div><div class="line">  type INT,</div><div class="line">  level TINYINT,</div><div class="line">  date String</div><div class="line">)</div><div class="line">COMMENT &apos;User Infomation&apos;</div><div class="line">LOCATION &apos;/user/chris/datastore/user/&apos;;</div></pre></td></tr></table></figure>
<p>Location配置是设置我们要将数据文件存储的位置，目录的名称必须和表名一样（就像Hive的普通表一样）。在这个例子中，表名就是user。</p>
<p>然后，我们可以导入任何符合user表声明的pattern表达式的数据文件到user目录下。</p>
<p>所有的数据都可以被Hive SQL立即访问。</p>
<h5 id="不够理想的地方"><a href="#不够理想的地方" class="headerlink" title="不够理想的地方"></a>不够理想的地方</h5><p>当数据文件变大（数量和大小），我们可能需要用Partition分区来优化数据处理的效率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE user (</div><div class="line">  userId BIGINT,</div><div class="line">  type INT,</div><div class="line">  level TINYINT,</div><div class="line">)</div><div class="line">COMMENT &apos;User Infomation&apos;</div><div class="line">PARTITIONED BY (date String)</div></pre></td></tr></table></figure>
<p>date String 被移动到 PARTITIONED BY，当我们需要加载数据到Hive时，partition一定要被分配。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LOAD INPATH &apos;/user/chris/data/testdata&apos; OVERWRITE INTO TABLE user PARTITION (date=&apos;2012-02-22&apos;)</div></pre></td></tr></table></figure>
<p>当数据加载完之后，我们可以看到一个名称是date=2010-02-22的新目录被创建在 /user/chris/warehouse/user/ 下。</p>
<p>所以，我们要如何使用External Table的Partition来优化数据处理呢？</p>
<p>和之前一样，首先要创建外部表user，并且分配好Location。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">CREATE EXTERNAL TABLE user (</div><div class="line">  userId BIGINT,</div><div class="line">  type INT,</div><div class="line">  level TINYINT,</div><div class="line">  date String</div><div class="line">)</div><div class="line">COMMENT &apos;User Infomation&apos;</div><div class="line">PARTITIONED BY (date String)</div><div class="line">LOCATION &apos;/user/chris/datastore/user/&apos;;</div></pre></td></tr></table></figure>
<p>然后，在 /user/chris/datastore/user/ 下创建目录date=2010-02-22</p>
<p>最后，把date是2010-02-22数据文件存储在这个目录下，完成。</p>
<p>但是，</p>
<p>当我们执行select * from user;没有任何结果数据。</p>
<p>为什么呢？</p>
<p>我花了很长时间搜寻答案。</p>
<p>最终，解决了。</p>
<p>因为当外部表被创建，Metastore包含Hive元数据信息，Hive元数据中外置表的默认表路径是被更改到指定的Location，但是关于partition，不做任何更改，所以我们必须手工添加这些元数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ALTER TABLE user ADD PARTITION(date=&apos;2010-02-22&apos;);</div></pre></td></tr></table></figure>
<p>每次有一个新的 date=… 目录（partition）被创建，我们都必须手工alter table来添加partition信息。</p>
<p>这个真的不是很好的方式！</p>
<p>但是幸运的是，我们有Hive JDBC/Thrift, 我们可以使用 <a href="https://github.com/don9z/hadoop-tools/blob/master/hive/addpartition.py">script</a> 脚本来做这些。</p>
<p>原文链接：</p>
<ul>
<li><a href="http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/" target="_blank" rel="external">http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hive/Hive学习（四）Hive内部表和外部表" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/18/Hive/Hive学习（四）Hive内部表和外部表/" class="article-date">
  	<time datetime="2016-10-18T06:39:56.000Z" itemprop="datePublished">2016-10-18</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/18/Hive/Hive学习（四）Hive内部表和外部表/">Hive学习（四）Hive内部表和外部表</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上一篇我们介绍了Hive导入数据的两种方式，本篇我们对Hive的表进行重点介绍。上一篇我们使用的都是Hive的内部表，如何区分Hive的内部表和外部表呢？create （external） table语句是否带有external关键字，如果带有external关键字就是外部表，所以上一篇我们导入的数据都是导入到Hive的内部表，也就是文件都存储在/hive/warehouse的HDFS目录中，即Hive默认配置的数据仓库。External Table允许我们将文件保存在任意的HDFS目录下，下面将详细介绍内部表和外部表的区别。</p>
<h3 id="Hive内部表"><a href="#Hive内部表" class="headerlink" title="Hive内部表"></a>Hive内部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># 创建内部表test_internal_table，这里创建好的表的数据文件是默认存储在/hive/warehouse目录下，全路径是/hive/warehouse/test_hdfs.db/test_internal_table</div><div class="line"># test_hdfs是我们的数据库</div><div class="line"># 如果删除test_internal_table，元数据表结构和数据文件都将会被删除</div><div class="line">CREATE TABLE IF NOT EXISTS test_internal_table(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE;</div></pre></td></tr></table></figure>
<h3 id="Hive外部表"><a href="#Hive外部表" class="headerlink" title="Hive外部表"></a>Hive外部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 创建外部表test_external_table，这里创建好的表是读取的Location属性指定文件目录下的数据文件，而不是默认的/hive/warehouse下，这样我们就可以使用External Table结合外部的Application使用（这里读取的是Flume采集并写入HDFS的数据文件），Hive同样可以读取Hive默认配置的数据仓库之外的HDFS目录下的数据文件。</div><div class="line"># Location是指定的数据文件路径</div><div class="line"># 如果删除test_external_table，元数据表结构会被删除，但是数据文件不会被删除</div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS test_external_table(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.view_ad&apos;;</div></pre></td></tr></table></figure>
<p>最后总结一下Hive内部表与外部表的区别：</p>
<ul>
<li>在导入数据时，导入到内部表，数据文件是存储在Hive的默认的数据仓库下的。导入到外部表，数据文件是存储在External Table指定的Location目录下的。</li>
<li>在删除内部表时，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时，Hive仅仅删除外部表的元数据，数据是不会删除的。</li>
</ul>
<p>如何选择使用哪种表呢？</p>
<ul>
<li>如果所有的数据处理都需要由Hive完成，那么建议你应该使用内部表，如果所有的数据处理需要整合其他Application一起应用（例如：Flume负责采集数据文件，并且根据Header写入到HDFS的不同目录下的数据文件），此时建议使用外部表。</li>
</ul>
<p>原文链接：</p>
<ul>
<li><a href="http://blog.csdn.net/yeruby/article/details/23033273" target="_blank" rel="external">http://blog.csdn.net/yeruby/article/details/23033273</a></li>
<li><a href="http://www.aboutyun.com/thread-7458-1-1.html" target="_blank" rel="external">http://www.aboutyun.com/thread-7458-1-1.html</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Hive/Hive学习（三）Hive导入数据的几种方式" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/18/Hive/Hive学习（三）Hive导入数据的几种方式/" class="article-date">
  	<time datetime="2016-10-18T03:59:02.000Z" itemprop="datePublished">2016-10-18</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/18/Hive/Hive学习（三）Hive导入数据的几种方式/">Hive学习（三）Hive导入数据的几种方式</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Hive导入数据的几种方式"><a href="#Hive导入数据的几种方式" class="headerlink" title="Hive导入数据的几种方式"></a>Hive导入数据的几种方式</h3><ul>
<li>从本地文件系统中导入数据到Hive表</li>
<li>从HDFS中导入数据到Hive表</li>
</ul>
<p>上面的两种方式都是使用Hive的load语句导入数据的，具体格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</div></pre></td></tr></table></figure>
<ul>
<li><p>如果使用了LOCAL关键字，则会在本地文件系统中寻找filepath，如果filepath是相对路径，则该路径会被解释为相对于用户的当前工作目录，用户也可以指定为本地文件指定完整URI，例如：file:///data/track.log，或者直接写为/data/track.log。Load语句将会复制由filepath指定的所有文件到目标文件系统（目标文件系统由表的location属性推断得出），然后移动文件到表中。</p>
</li>
<li><p>如果未使用LOCAL关键字，filepath必须指的是与目标表的location文件系统相同的文件系统上的文件（例如：HDFS文件系统）。这里Load的本质实际就是一个HDFS目录下的数据文件转移到另一个HDFS目录下的操作。</p>
</li>
</ul>
<p>当然还有其他的Hive导入数据的方式，但这里我们重点介绍这两种，其他的导入数据方式可以参考：<a href="https://www.iteblog.com/archives/949" target="_blank" rel="external">https://www.iteblog.com/archives/949</a></p>
<p>下面我们将具体举例分析上面两种Hive导入数据的方式，下面是我们要分析的日志文件track.log的内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div></pre></td></tr></table></figure>
<h4 id="从本地文件系统中导入数据到Hive表"><a href="#从本地文件系统中导入数据到Hive表" class="headerlink" title="从本地文件系统中导入数据到Hive表"></a>从本地文件系统中导入数据到Hive表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># 查看需要导入Hive的track.log文件内容</div><div class="line">$ cat /data/track.log&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div><div class="line"></div><div class="line"># 创建表test_local_table</div><div class="line">hive&gt; CREATE TABLE IF NOT EXISTS test_local_table(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE;</div><div class="line"></div><div class="line"># 从本地文件系统中导入数据到Hive表</div><div class="line">hive&gt; load data local inpath &apos;/data/track.log&apos; into table test_local_table partition (dt=&apos;2016-10-18&apos;);</div><div class="line"></div><div class="line"># 导入完成之后，查询test_local_table表中的数据</div><div class="line">hive&gt; select * from test_local_table;OK[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	2016-10-18Time taken: 0.106 seconds, Fetched: 1 row(s)</div><div class="line"></div><div class="line"># 在HDFS的/hive/warehouse目录中查看track.log文件，这就是我们将本地系统文件导入到Hive之后，存储在HDFS的路径</div><div class="line"># test_hdfs.db是我们的数据库</div><div class="line"># test_local_table是我们创建的表</div><div class="line"># dt=2016-10-18是我们创建的Partition</div><div class="line">$ hdfs dfs -ls /hive/warehouse/test_hdfs.db/test_local_table/dt=2016-10-18Found 1 items-rwxr-xr-x   2 yunyu supergroup        268 2016-10-17 21:19 /hive/warehouse/test_hdfs.db/test_local_table/dt=2016-10-18/track.log</div><div class="line"></div><div class="line"># 查看文件内容</div><div class="line">$ hdfs dfs -cat /hive/warehouse/test_hdfs.db/test_local_table/dt=2016-10-18/track.log&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div></pre></td></tr></table></figure>
<h4 id="从HDFS中导入数据到Hive表"><a href="#从HDFS中导入数据到Hive表" class="headerlink" title="从HDFS中导入数据到Hive表"></a>从HDFS中导入数据到Hive表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># 在HDFS中查看要导入到Hive的文件（这里我们使用之前Flume收集到HDFS的track.log的日志文件）</div><div class="line">$ hdfs dfs -ls /flume/events/birdben.ad.click_ad/201610/</div><div class="line">Found 1 items-rw-r--r--   2 yunyu supergroup       6776 2016-10-13 06:18 /flume/events/birdben.ad.click_ad/201610/events-.1476364421957</div><div class="line"></div><div class="line"># 创建表test_partition_table</div><div class="line">hive&gt; CREATE TABLE IF NOT EXISTS test_partition_table(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE;</div><div class="line"></div><div class="line"># 从HDFS导入数据到Hive表</div><div class="line">hive&gt; load data inpath &apos;/flume/events/birdben.ad.click_ad/201610/events-.1476364421957&apos; into table test_partition_table partition (dt=&apos;2016-10-18&apos;);</div><div class="line"></div><div class="line"># 导入完成之后，查询test_partition_table表中的数据</div><div class="line">hive&gt; select * from test_partition_table;OK[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;59948935480868864&quot;,&quot;bid&quot;:null,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475150396804&#125;]	info	logs	NULL	2016-10-18[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;59948935480868864&quot;,&quot;bid&quot;:null,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475150470244&#125;]	info	logs	NULL	2016-10-18</div><div class="line">...</div><div class="line">Time taken: 0.102 seconds, Fetched: 26 row(s)</div><div class="line"></div><div class="line"># 在HDFS中再次查看源文件，此时源文件已经在此目录下不存在了，因为已经被移动到/hive/warehouse下，所以说使用load从HDFS中导入数据到Hive的方式，是将原来HDFS文件移动到Hive默认配置的数据仓库下（即:/hive/warehouse下，此目录是在hive-site.xml配置文件中配置的）</div><div class="line">$ hdfs dfs -ls /flume/events/rp.hb.ad.view_ad/201610</div><div class="line"></div><div class="line"># 查看Hive默认配置的数据仓库的HDFS目录下，即可找到我们导入的文件</div><div class="line">$ hdfs dfs -ls /hive/warehouse/test_hdfs.db/test_partition_table/dt=2016-10-18Found 1 items-rwxr-xr-x   2 yunyu supergroup       6776 2016-10-13 06:18 /hive/warehouse/test_hdfs.db/test_partition_table/dt=2016-10-18/events-.1476364421957</div></pre></td></tr></table></figure>
<p>原文链接：</p>
<ul>
<li><a href="http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/" target="_blank" rel="external">http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/</a></li>
<li><a href="http://www.cnblogs.com/luogankun/p/4111145.html" target="_blank" rel="external">http://www.cnblogs.com/luogankun/p/4111145.html</a></li>
<li><a href="http://stackoverflow.com/questions/30907657/add-partition-after-creating-table-in-hive" target="_blank" rel="external">http://stackoverflow.com/questions/30907657/add-partition-after-creating-table-in-hive</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Flume/Flume学习（十四）Flume整合Kafka" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/17/Flume/Flume学习（十四）Flume整合Kafka/" class="article-date">
  	<time datetime="2016-10-17T05:32:03.000Z" itemprop="datePublished">2016-10-17</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/17/Flume/Flume学习（十四）Flume整合Kafka/">Flume学习（十四）Flume整合Kafka</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="环境简介"><a href="#环境简介" class="headerlink" title="环境简介"></a>环境简介</h3><ul>
<li>JDK1.7.0_79</li>
<li>Flume1.6.0</li>
<li>kafka_2.11-0.9.0.0</li>
</ul>
<h3 id="Flume整合Kafka的相关配置"><a href="#Flume整合Kafka的相关配置" class="headerlink" title="Flume整合Kafka的相关配置"></a>Flume整合Kafka的相关配置</h3><h4 id="flume-agent-file-conf配置文件"><a href="#flume-agent-file-conf配置文件" class="headerlink" title="flume_agent_file.conf配置文件"></a>flume_agent_file.conf配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">agentX.sources = sX</div><div class="line">agentX.channels = chX</div><div class="line">agentX.sinks = sk1 sk2</div><div class="line"></div><div class="line">agentX.sources.sX.channels = chX</div><div class="line">agentX.sources.sX.type = exec</div><div class="line">agentX.sources.sX.command = tail -F -n +0 /Users/yunyu/Downloads/track.log</div><div class="line"></div><div class="line">agentX.channels.chX.type = memory</div><div class="line">agentX.channels.chX.capacity = 1000</div><div class="line">agentX.channels.chX.transactionCapacity = 100</div><div class="line"></div><div class="line"># Configure sinks</div><div class="line">agentX.sinks.sk1.channel = chX</div><div class="line">agentX.sinks.sk1.type = avro</div><div class="line">agentX.sinks.sk1.hostname = hadoop1</div><div class="line">agentX.sinks.sk1.port = 41414</div><div class="line"></div><div class="line">agentX.sinks.sk2.channel = chX</div><div class="line">agentX.sinks.sk2.type = avro</div><div class="line">agentX.sinks.sk2.hostname = hadoop2</div><div class="line">agentX.sinks.sk2.port = 41414</div><div class="line"></div><div class="line"># Configure loadbalance</div><div class="line">agentX.sinkgroups = g1</div><div class="line">agentX.sinkgroups.g1.sinks = sk1 sk2</div><div class="line">agentX.sinkgroups.g1.processor.type = load_balance</div><div class="line">agentX.sinkgroups.g1.processor.backoff = true</div><div class="line">agentX.sinkgroups.g1.processor.selector = round_robin</div></pre></td></tr></table></figure>
<h4 id="flume-collector-kafka-conf配置文件"><a href="#flume-collector-kafka-conf配置文件" class="headerlink" title="flume_collector_kafka.conf配置文件"></a>flume_collector_kafka.conf配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">agentX.sources = flume-avro-sinkagentX.channels = chXagentX.sinks = flume-kafka-sinkagentX.sources.flume-avro-sink.channels = chXagentX.sources.flume-avro-sink.type = avroagentX.sources.flume-avro-sink.bind = hadoop1agentX.sources.flume-avro-sink.port = 41414agentX.sources.flume-avro-sink.threads = 8agentX.channels.chX.type = memoryagentX.channels.chX.capacity = 10000agentX.channels.chX.transactionCapacity = 100agentX.sinks.flume-kafka-sink.type = org.apache.flume.sink.kafka.KafkaSinkagentX.sinks.flume-kafka-sink.topic = kafka_cluster_topicagentX.sinks.flume-kafka-sink.brokerList = hadoop1:9092,hadoop2:9092,hadoop3:9092agentX.sinks.flume-kafka-sink.requiredAcks = 1agentX.sinks.flume-kafka-sink.batchSize = 20agentX.sinks.flume-kafka-sink.channel = chX</div></pre></td></tr></table></figure>
<h4 id="启动Flume-Agent"><a href="#启动Flume-Agent" class="headerlink" title="启动Flume Agent"></a>启动Flume Agent</h4><p>启动Flume Agent监听track.log日志文件的变化，并且上报的Flume Collector</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/flume-ng agent --conf ./conf/ -f conf/flume_agent_file.conf -Dflume.root.logger=DEBUG,console -n agentX</div></pre></td></tr></table></figure>
<h4 id="启动Flume-Collector"><a href="#启动Flume-Collector" class="headerlink" title="启动Flume Collector"></a>启动Flume Collector</h4><p>启动Flume Collector监听Agent上报的消息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/flume-ng agent --conf ./conf/ -f conf/flume_collector_kafka.conf -Dflume.root.logger=DEBUG,console -n agentX</div></pre></td></tr></table></figure>
<h4 id="启动Kafka"><a href="#启动Kafka" class="headerlink" title="启动Kafka"></a>启动Kafka</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 启动Zookeeper服务（我这里是启动的外置Zookeeper集群，不是Kafka内置的Zookeeper）</div><div class="line">$ ./bin zkServer.sh start</div><div class="line"></div><div class="line"># 启动Kafka服务</div><div class="line">$ ./bin/kafka-server-start.sh -daemon config/server.properties</div><div class="line"></div><div class="line"># 如果是第一次启动Kafka，需要创建一个Topic，用于存储Flume收集上来的日志消息</div><div class="line">$ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic kafka_cluster_topic</div></pre></td></tr></table></figure>
<h4 id="启动Kafka-Consumer"><a href="#启动Kafka-Consumer" class="headerlink" title="启动Kafka Consumer"></a>启动Kafka Consumer</h4><p>启动Kafka Consumer来消费Kafka中的消息，这时候如果track.log日志文件有新日志写入，通过Flume上传并且写入到Kafka，最终可以在Kafka Consumer消费端看到日志文件中的内容。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic kafka_cluster_topic --from-beginning</div><div class="line"></div><div class="line">this is a message</div><div class="line">birdben is my name</div><div class="line">...</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="https://flume.apache.org/FlumeUserGuide.html#kafka-sink" target="_blank" rel="external">https://flume.apache.org/FlumeUserGuide.html#kafka-sink</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Kafka/Kafka学习（二）KafkaOffsetMonitor监控工具使用" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/17/Kafka/Kafka学习（二）KafkaOffsetMonitor监控工具使用/" class="article-date">
  	<time datetime="2016-10-17T02:32:45.000Z" itemprop="datePublished">2016-10-17</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/17/Kafka/Kafka学习（二）KafkaOffsetMonitor监控工具使用/">Kafka学习（二）KafkaOffsetMonitor监控工具使用</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="启动Zookeeper"><a href="#启动Zookeeper" class="headerlink" title="启动Zookeeper"></a>启动Zookeeper</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 分别启动Hadoop1，Hadoop2，Hadoop3三台服务器的Zookeeper服务</div><div class="line">$ ./bin/zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /data/zookeeper-3.4.8/bin/../conf/zoo.cfgStarting zookeeper ... already running as process 4468.</div><div class="line"></div><div class="line"># 分别查看一下Zookeeper服务的状态</div><div class="line">$ ./bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /data/zookeeper-3.4.8/bin/../conf/zoo.cfgMode: leader</div></pre></td></tr></table></figure>
<h3 id="启动Kafka"><a href="#启动Kafka" class="headerlink" title="启动Kafka"></a>启动Kafka</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 分别启动Hadoop1，Hadoop2，Hadoop3三台服务器的Kafka服务</div><div class="line">$ ./bin/kafka-server-start.sh config/server.properties &amp;</div></pre></td></tr></table></figure>
<h3 id="运行KafkaOffsetMonitor监控服务"><a href="#运行KafkaOffsetMonitor监控服务" class="headerlink" title="运行KafkaOffsetMonitor监控服务"></a>运行KafkaOffsetMonitor监控服务</h3><p>下载 <a href="https://github.com/quantifind/KafkaOffsetMonitor/releases/latest">KafkaOffsetMonitor</a> 的jar包，然后执行下面的运行命令，然后我们就能够访问 <a href="http://localhost:9999/" target="_blank" rel="external">http://localhost:9999/</a> 来进入KafkaOffsetMonitor的监控后台。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">java -cp KafkaOffsetMonitor-assembly-0.2.1.jar \</div><div class="line">     com.quantifind.kafka.offsetapp.OffsetGetterWeb \</div><div class="line">     --zk hadoop1,hadoop2,hadoop3 \</div><div class="line">     --port 9999 \</div><div class="line">     --refresh 10.seconds \</div><div class="line">     --retain 2.days</div></pre></td></tr></table></figure>
<ul>
<li>offsetStorage : 已取消</li>
<li>zk : Zookeeper服务器地址</li>
<li>port : KafkaOffsetMonitor监控服务使用的Web服务器端口</li>
<li>refresh : 多长时间将app数据刷新一次到DB</li>
<li>retain : 保存多久的数据到DB</li>
<li>dbName : 历史数据存储的数据库名(default ‘offsetapp’)</li>
<li>kafkaOffsetForceFromStart : 已取消</li>
<li>stormZKOffsetBase : 已取消</li>
<li>pluginsArgs : 扩展使用</li>
</ul>
<p>注意：这里使用的0.2.1版本，0.2.1版本已经没有offsetStorage参数了，所以网上搜索的一些文章中使用的老版本还配置了offsetStorage参数，这里需要注意一下。</p>
<p><img src="http://img.blog.csdn.net/20161017111332769?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="KafkaOffsetMonitor效果图"></p>
<p>参考文章：</p>
<ul>
<li><a href="https://github.com/quantifind/KafkaOffsetMonitor">https://github.com/quantifind/KafkaOffsetMonitor</a></li>
<li><a href="https://github.com/quantifind/KafkaOffsetMonitor/issues/86">https://github.com/quantifind/KafkaOffsetMonitor/issues/86</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/MQ/">MQ</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Flume/Flume学习（十三）Flume + HDFS + Hive离线分析（再续）" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/13/Flume/Flume学习（十三）Flume + HDFS + Hive离线分析（再续）/" class="article-date">
  	<time datetime="2016-10-13T08:52:54.000Z" itemprop="datePublished">2016-10-13</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/13/Flume/Flume学习（十三）Flume + HDFS + Hive离线分析（再续）/">Flume学习（十三）Flume + HDFS + Hive离线分析（再续）</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在《Flume学习（十一）Flume + HDFS + Hive离线分析》这篇中我们就遇到了Hive分区的问题，这里我们再来回顾一下之前待调研的问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"># 问题二：</div><div class="line">之前我们在Flume中配置了采集到的日志输出到HDFS的保存路径是下面两种，一种使用了日期分割的，一种是没有使用日期分割的</div><div class="line">- hdfs://10.10.1.64:8020/flume/events/20160923</div><div class="line">- hdfs://10.10.1.64:8020/flume/events/</div><div class="line"></div><div class="line"># 解决方案：</div><div class="line">如果我们使用第二种不用日期分割的方式，在Hive上创建表指定/flume/events路径是没有问题，查询数据也都正常，但是如果使用第一种日期分割的方式，在Hive上创建表就必须指定具体的子目录，而不是/flume/events根目录，这样虽然表能够建成功但是却查询不到任何数据，因为指定的对应HDFS目录不正确，应该指定为/flume/events/20160923。这个问题确实也困扰我很久，最后才发现原来是Hive建表指定的HDFS目录不正确。</div><div class="line"></div><div class="line">指定location为&apos;/flume/events&apos;不好用，Hive中查询command_json_table表中没有数据</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_json_table(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hive.hcatalog.data.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div><div class="line"></div><div class="line">指定location为&apos;/flume/events/20160923&apos;好用，Hive中查询command_json_table_20160923表中有数据</div><div class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS command_json_table_20160923(time STRING, hostname STRING, li STRING, lu STRING, nu STRING, cmd STRING)</div><div class="line">ROW FORMAT SERDE &apos;org.apache.hive.hcatalog.data.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/20160923&apos;;</div><div class="line"></div><div class="line">建议的解决方式是使用Hive的表分区来做，需要调研Hive的表分区是否支持使用HDFS已经分割好的目录结构（需要调研）</div></pre></td></tr></table></figure>
<p>上面是我们之前的问题原文描述，之前需要调研Hive表分区是否可以使用HDFS已经分割好的目录结构，这里我找到了一篇blog，终于理解了Hive关于External表如何使用partition的，下面给出了原文和译文的链接地址</p>
<p>原文链接：</p>
<ul>
<li><a href="http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/" target="_blank" rel="external">http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/</a></li>
</ul>
<p>译文链接：</p>
<ul>
<li><p>我们带着上面的问题继续优化，之前的解决办法是按照我们日志中的name属性值存储在HDFS的不同目录中，本篇我们使用Partition来解决数据量增长的情况，我们在之前使用name属性的基础上在新建dt目录（按照月份来分割数据）</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">agentX.sources = flume-avro-sinkagentX.channels = chXagentX.sinks = flume-hdfs-sinkagentX.sources.flume-avro-sink.channels = chXagentX.sources.flume-avro-sink.type = avroagentX.sources.flume-avro-sink.bind = hadoop1agentX.sources.flume-avro-sink.port = 41414agentX.sources.flume-avro-sink.threads = 8#定义拦截器，为消息添加时间戳和Host地址</div><div class="line">#将日志中的name属性添加到Header中，用来做HDFS存储的目录结构，type_name属性就是从日志文件中解析出来的name属性的值，这里使用%Y%m表达式代表按照年月分区agentX.sources.flume-avro-sink.interceptors = i1 i2agentX.sources.flume-avro-sink.interceptors.i1.type = timestampagentX.sources.flume-avro-sink.interceptors.i2.type = regex_extractoragentX.sources.flume-avro-sink.interceptors.i2.regex = &quot;name&quot;:&quot;(.*?)&quot;agentX.sources.flume-avro-sink.interceptors.i2.serializers = s1agentX.sources.flume-avro-sink.interceptors.i2.serializers.s1.name = type_nameagentX.channels.chX.type = memoryagentX.channels.chX.capacity = 1000agentX.channels.chX.transactionCapacity = 100agentX.sinks.flume-hdfs-sink.type = hdfsagentX.sinks.flume-hdfs-sink.channel = chXagentX.sinks.flume-hdfs-sink.hdfs.path = hdfs://10.10.1.64:8020/flume/events/%&#123;type_name&#125;/%Y%magentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStreamagentX.sinks.flume-hdfs-sink.hdfs.filePrefix = events-agentX.sinks.flume-hdfs-sink.hdfs.rollInterval = 300agentX.sinks.flume-hdfs-sink.hdfs.rollSize = 0agentX.sinks.flume-hdfs-sink.hdfs.rollCount = 300</div></pre></td></tr></table></figure>
<h5 id="在HDFS中查看文件目录"><a href="#在HDFS中查看文件目录" class="headerlink" title="在HDFS中查看文件目录"></a>在HDFS中查看文件目录</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 可以看到HDFS文件目录已经按照我们的name属性区分开了</div><div class="line">hdfs dfs -ls /flume/events/drwxr-xr-x   - yunyu supergroup          0 2016-10-13 07:01 /flume/events/birdben.api.calldrwxr-xr-x   - yunyu supergroup          0 2016-10-13 07:02 /flume/events/birdben.ad.click_addrwxr-xr-x   - yunyu supergroup          0 2016-10-13 07:02 /flume/events/birdben.ad.open_hbdrwxr-xr-x   - yunyu supergroup          0 2016-10-13 07:02 /flume/events/birdben.ad.view_ad</div><div class="line"></div><div class="line"># 查看个不同name下的目录是按照年月分割开的</div><div class="line">$ hdfs dfs -ls /flume/events/birdben.ad.click_adFound 2 itemsdrwxr-xr-x   - yunyu supergroup          0 2016-10-13 06:18 /flume/events/birdben.ad.click_ad/201610drwxr-xr-x   - yunyu supergroup          0 2016-10-13 07:07 /flume/events/birdben.ad.click_ad/201611</div><div class="line"></div><div class="line"># 数据文件是存储在具体的年月目录下的</div><div class="line">$ hdfs dfs -ls /flume/events/birdben.ad.click_ad/201610/Found 1 items-rw-r--r--   2 yunyu supergroup       1596 2016-10-13 06:18 /flume/events/birdben.ad.click_ad/201610/events-.1476364422107</div></pre></td></tr></table></figure>
<h3 id="Hive按照不同的HDFS目录建表"><a href="#Hive按照不同的HDFS目录建表" class="headerlink" title="Hive按照不同的HDFS目录建表"></a>Hive按照不同的HDFS目录建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"># 这里我们是需要先理解Hive的内部表和外部表的区别，然后我们在之前的建表语句中加入partition分区，我们这里使用的是dt字段作为partition，dt字段不能够与建表语句中的字段重复，否则建表时会报错。</div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_click_ad(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.click_ad&apos;;</div><div class="line"></div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_open_hb(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.open_hb&apos;;</div><div class="line"></div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_view_ad(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">partitioned by (dt string)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.view_ad&apos;;</div><div class="line"></div><div class="line"># 这时候我们查询表，表中是没有数据的。我们需要手工添加partition分区之后，才能查到数据。</div><div class="line">hive&gt; select * from birdben_ad_click_ad;</div><div class="line"></div><div class="line"># 建表完成之后，我们需要手工添加partition目录为我们Flume之前划分的好的年月目录</div><div class="line">alter table birdben_ad_click_ad add partition(dt=&apos;201610&apos;) location &apos;/flume/events/birdben_ad_click_ad/201610&apos;;</div><div class="line">alter table birdben_ad_click_ad add partition(dt=&apos;201611&apos;) location &apos;/flume/events/birdben_ad_click_ad/201611&apos;;</div><div class="line"></div><div class="line">alter table birdben_ad_open_hb add partition(dt=&apos;201610&apos;) location &apos;/flume/events/birdben.ad.open_hb/201610&apos;;</div><div class="line">alter table birdben_ad_open_hb add partition(dt=&apos;201611&apos;) location &apos;/flume/events/birdben.ad.open_hb/201611&apos;;</div><div class="line"></div><div class="line">alter table birdben_ad_view_ad add partition(dt=&apos;201610&apos;) location &apos;/flume/events/birdben.ad.view_ad/201610&apos;;</div><div class="line">alter table birdben_ad_view_ad add partition(dt=&apos;201611&apos;) location &apos;/flume/events/birdben.ad.view_ad/201611&apos;;</div><div class="line"></div><div class="line"># 这时候我们查询表，能够查询到全部的数据了（包括201610和201611的数据）</div><div class="line">hive&gt; select * from birdben_ad_click_ad;</div><div class="line">OK[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201611[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201611[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201611Time taken: 0.1 seconds, Fetched: 9 row(s)</div><div class="line"></div><div class="line"># 也可以按照分区字段查询数据，这样就能够证明我们可以使用Hive的External表partition对应到我们Flume中创建好的 %Y%m（年月） 目录结构</div><div class="line">hive&gt; select * from birdben_ad_click_ad where dt = &apos;201610&apos;;</div><div class="line">OK[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201610[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201610Time taken: 0.099 seconds, Fetched: 6 row(s)</div><div class="line"></div><div class="line">hive&gt; select * from birdben_ad_click_ad where dt = &apos;201611&apos;;</div><div class="line">OK</div><div class="line">[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL	201611[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL	201611[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULL	201611Time taken: 0.11 seconds, Fetched: 3 row(s)</div></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>其实我写了这么多篇Flume + HDFS + Hive的文章，就是为了证明Flume可以按照指定的Header的key分别写入不同的HDFS目录，Hive又可以通过External表将Location定位到Flume写入的HDFS目录，而且还可以通过Partition分区定位到Flume设置的Header对应的目录，这样就能够比较优雅的将Flume, HDFS, Hive整合到一起了。但是还是有些需要优化的地方，比如说我们的日志格式不够规范，每种日志都有不同的格式，而且还都写入到同一个track.log日志文件中，只能通过name属性作区分。还有就是Hive的Partition每次需要手工去修改表，否则无法查询到HDFS对应目录下的数据，也有人使用 <a href="https://github.com/don9z/hadoop-tools/blob/master/hive/addpartition.py">script</a> 脚本来做这些事情，待以后有时间继续深入研究。</p>
<p>参考文章：</p>
<ul>
<li><a href="http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/" target="_blank" rel="external">http://blog.zhengdong.me/2012/02/22/hive-external-table-with-partitions/</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Flume/Flume学习（十二）Flume + HDFS + Hive离线分析（续）" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/12/Flume/Flume学习（十二）Flume + HDFS + Hive离线分析（续）/" class="article-date">
  	<time datetime="2016-10-12T05:43:55.000Z" itemprop="datePublished">2016-10-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/12/Flume/Flume学习（十二）Flume + HDFS + Hive离线分析（续）/">Flume学习（十二）Flume + HDFS + Hive离线分析（续）</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上一篇中我们已经实现了使用Flume收集日志并且输出到HDFS中，并且结合Hive在HDFS进行离线的查询分析。但是也同样遇到了一些问题，本篇将解决更复杂的日志收集情况，将不同的日志格式写入到同一个日志文件，然后用Flume根据Header来写入到HDFS不同的目录。</p>
<h3 id="日志结构"><a href="#日志结构" class="headerlink" title="日志结构"></a>日志结构</h3><p>我们会讲所有的日志都写入到track.log文件中，包含API调用的日志以及其他埋点日志，这里是通过name来区分日志类型的，不同的日志类型有着不同的json结构。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">### API日志</div><div class="line"></div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;name&quot;:&quot;birdben.api.call&quot;,&quot;request&quot;:&quot;POST /api/message/receive&quot;,&quot;status&quot;:&quot;succeeded&quot;,&quot;bid&quot;:&quot;59885256139866115&quot;,&quot;uid&quot;:&quot;&quot;,&quot;did&quot;:&quot;1265&quot;,&quot;duid&quot;:&quot;dxf536&quot;,&quot;hb_uid&quot;:&quot;59885256030814209&quot;,&quot;ua&quot;:&quot;Dalvik/1.6.0 (Linux; U; Android 4.4.4; YQ601 Build/KTU84P)&quot;,&quot;device_id&quot;:&quot;fa48a076-f35f-3217-8575-5fc1f02f1ac0&quot;,&quot;ip&quot;:&quot;::ffff:10.10.1.242&quot;,&quot;server_timestamp&quot;:1475912702996&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:02.996Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;name&quot;:&quot;birdben.api.call&quot;,&quot;request&quot;:&quot;GET /api/message/ad-detail&quot;,&quot;status&quot;:&quot;succeeded&quot;,&quot;bid&quot;:&quot;59885256139866115&quot;,&quot;uid&quot;:&quot;&quot;,&quot;did&quot;:&quot;1265&quot;,&quot;duid&quot;:&quot;dxf536&quot;,&quot;hb_uid&quot;:&quot;59885256030814209&quot;,&quot;ua&quot;:&quot;Dalvik/1.6.0 (Linux; U; Android 4.4.4; YQ601 Build/KTU84P)&quot;,&quot;device_id&quot;:&quot;fa48a076-f35f-3217-8575-5fc1f02f1ac0&quot;,&quot;ip&quot;:&quot;::ffff:10.10.1.242&quot;,&quot;server_timestamp&quot;:1475912787476&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:46:27.476Z&quot;&#125;</div><div class="line"></div><div class="line">### 打开App日志</div><div class="line"></div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914816071&quot;,&quot;rpid&quot;:&quot;63152468644593670&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829286&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.286Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914827206&quot;,&quot;rpid&quot;:&quot;63152468644593670&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840425&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.425Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915077351&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;name&quot;:&quot;birdben.ad.open_hb&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915090579&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:50.579Z&quot;&#125;</div><div class="line"></div><div class="line">### 加载页面日志</div><div class="line"></div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914816133&quot;,&quot;rpid&quot;:&quot;63152468644593670&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914829332&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:29.332Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475914827284&quot;,&quot;rpid&quot;:&quot;63152468644593670&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475914840498&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:20:40.499Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915077585&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;name&quot;:&quot;birdben.ad.view_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915090789&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:50.789Z&quot;&#125;</div><div class="line"></div><div class="line">### 点击链接日志</div><div class="line"></div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475912701768&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475912715001&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T07:45:15.001Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475913832349&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475913845544&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:04:05.544Z&quot;&#125;</div><div class="line">&#123;&quot;logs&quot;:[&#123;&quot;timestamp&quot;:&quot;1475915080561&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;bid&quot;:0,&quot;uid&quot;:0,&quot;did&quot;:0,&quot;duid&quot;:0,&quot;hb_uid&quot;:0,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;server_timestamp&quot;:1475915093792&#125;],&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;logs&quot;,&quot;timestamp&quot;:&quot;2016-10-08T08:24:53.792Z&quot;&#125;</div></pre></td></tr></table></figure>
<h3 id="如何解析track-log日志文件中的日志"><a href="#如何解析track-log日志文件中的日志" class="headerlink" title="如何解析track.log日志文件中的日志"></a>如何解析track.log日志文件中的日志</h3><p>按照我们之前的做法，我们会使用Flume都讲日志的内容收集到HDFS上存储，但是这里的track.log日志文件中包含多种不同结构的json日志，而且这里的json数据结构是嵌套复杂对象的，我们不好在Hive上创建相应结构的表，只能创建一个大表要包含所有的日志字段，无法做到对某种日志的分析，如果像之前的做法可能无法满足我们的需求。</p>
<ul>
<li>问题一：如何Hive解析这种嵌套复杂对象的json数据结构</li>
<li><p>问题二：如何将多种不同的日志在HDFS按类型分开存储</p>
</li>
<li><p>问题一解决办法：<br>在网上找到第三方的插件能够解析嵌套复杂对象的json数据结构，主要是替换Hive自己内嵌的Serde解析器（org.apache.hive.hcatalog.data.JsonSerDe），Github地址：<a href="https://github.com/rcongiu/Hive-JSON-Serde">https://github.com/rcongiu/Hive-JSON-Serde</a></p>
</li>
<li><p>问题二解决办法：<br>这里我有个想法是按照日志类型，我们可以区分我们的日志结构，根据name属性分为API日志，打开APP日志，加载页面日志，点击链接日志。但是要如何在Flume根据name属性区分开不同的日志内容，并且写入到HDFS的不同目录呢？答案就是使用Flume的Interceptor</p>
</li>
</ul>
<h3 id="Hive安装Hive-JSON-Serde插件"><a href="#Hive安装Hive-JSON-Serde插件" class="headerlink" title="Hive安装Hive-JSON-Serde插件"></a>Hive安装Hive-JSON-Serde插件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"># 从GitHub下载Hive-JSON-Serde</div><div class="line">$ git clone https://github.com/rcongiu/Hive-JSON-Serde</div><div class="line"></div><div class="line"># 编译打包Hive-JSON-Serde，打包成功之后会在json-serde/target目录生成相应的jar包</div><div class="line">$ cd Hive-JSON-Serde</div><div class="line">$ mvn package</div><div class="line"></div><div class="line"># 复制打包好的jar到Hive的HIVE_AUX_JARS_PATH目录下，需要重启Hive服务，这样就不需要每次在Hive Shell中都进行add jar操作了</div><div class="line">$ cp json-serde/target/json-serde-1.3.8-SNAPSHOT-jar-with-dependencies.jar /usr/local/hive/hcatalog/share/hcatalog/</div><div class="line"></div><div class="line"># HIVE_AUX_JARS_PATH是在$&#123;HIVE_HOME&#125;/conf/hive-env.sh配置文件中设置的</div><div class="line">export HIVE_AUX_JARS_PATH=/usr/local/hive/hcatalog/share/hcatalog</div><div class="line"></div><div class="line"># Hive Shell中创建表，如下</div><div class="line"># 这里使用了我们刚刚引用的&apos;org.openx.data.jsonserde.JsonSerDe&apos;解析器</div><div class="line"># 这样所有的日志都可以通过birdben_log_table表来查询，但是部分字段属性可能没有建表中包含进来，这样可能查出来的属性值是NULL</div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_log_table(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events&apos;;</div></pre></td></tr></table></figure>
<h3 id="Flume的Interceptor"><a href="#Flume的Interceptor" class="headerlink" title="Flume的Interceptor"></a>Flume的Interceptor</h3><p>先回想一下我们是如何将日期作为参数写入到HDFS不同目录的，我们是在Flume中使用了Interceptor来将我们的name属性加入到Event的Header中，然后在Sink中通过获取Header中的name属性的值来写入到HDFS中的不同目录。</p>
<h5 id="Flume的配置文件"><a href="#Flume的配置文件" class="headerlink" title="Flume的配置文件"></a>Flume的配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">agentX.sources = flume-avro-sink</div><div class="line">agentX.channels = chX</div><div class="line">agentX.sinks = flume-hdfs-sink</div><div class="line"></div><div class="line">agentX.sources.flume-avro-sink.channels = chX</div><div class="line">agentX.sources.flume-avro-sink.type = avro</div><div class="line">agentX.sources.flume-avro-sink.bind = 10.10.1.64</div><div class="line">agentX.sources.flume-avro-sink.port = 41414</div><div class="line">agentX.sources.flume-avro-sink.threads = 8</div><div class="line"></div><div class="line"></div><div class="line">#定义拦截器，为消息添加时间戳和Host地址</div><div class="line">#将日志中的name属性添加到Header中，用来做HDFS存储的目录结构，type_name属性就是从日志文件中解析出来的name属性的值</div><div class="line">agentX.sources.flume-avro-sink.interceptors = i1 i2</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i1.type = timestamp</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.type = regex_extractor</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.regex = &quot;name&quot;:&quot;(.*?)&quot;</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.serializers = s1</div><div class="line">agentX.sources.flume-avro-sink.interceptors.i2.serializers.s1.name = type_name</div><div class="line"></div><div class="line">agentX.channels.chX.type = memory</div><div class="line">agentX.channels.chX.capacity = 1000</div><div class="line">agentX.channels.chX.transactionCapacity = 100</div><div class="line"></div><div class="line">agentX.sinks.flume-hdfs-sink.type = hdfs</div><div class="line">agentX.sinks.flume-hdfs-sink.channel = chX</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.path = hdfs://10.10.1.64:8020/flume/events/%&#123;type_name&#125;</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.fileType = DataStream</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.filePrefix = events-</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollInterval = 300</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollSize = 0</div><div class="line">agentX.sinks.flume-hdfs-sink.hdfs.rollCount = 300</div></pre></td></tr></table></figure>
<h5 id="在HDFS中查看文件目录"><a href="#在HDFS中查看文件目录" class="headerlink" title="在HDFS中查看文件目录"></a>在HDFS中查看文件目录</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 可以看到HDFS文件目录已经按照我们的name属性区分开了</div><div class="line">$ hdfs dfs -ls /flume/eventsdrwxr-xr-x   - yunyu supergroup          0 2016-10-11 03:58 /flume/events/birdben.api.calldrwxr-xr-x   - yunyu supergroup          0 2016-10-11 03:58 /flume/events/birdben.ad.click_addrwxr-xr-x   - yunyu supergroup          0 2016-10-11 03:58 /flume/events/birdben.ad.open_hbdrwxr-xr-x   - yunyu supergroup          0 2016-10-11 03:58 /flume/events/birdben.ad.view_ad</div><div class="line"></div><div class="line">$ hdfs dfs -ls /flume/events/birdben.ad.click_adFound 1 items-rwxr-xr-x   2 yunyu supergroup        798 2016-10-11 03:58 /flume/events/birdben.ad.click_ad/events-.1476183217539</div></pre></td></tr></table></figure>
<h3 id="Hive按照不同的HDFS目录建表"><a href="#Hive按照不同的HDFS目录建表" class="headerlink" title="Hive按照不同的HDFS目录建表"></a>Hive按照不同的HDFS目录建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"># Hive中我们重新建表，这次我们按照HDFS已经分好的目录建表</div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_click_ad(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.click_ad&apos;;</div><div class="line"></div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_open_hb(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.open_hb&apos;;</div><div class="line"></div><div class="line">CREATE EXTERNAL TABLE IF NOT EXISTS birdben_ad_view_ad(logs array&lt;struct&lt;name:string, rpid:string, bid:string, uid:string, did:string, duid:string, hbuid:string, ua:string, device_id:string, ip:string, server_timestamp:BIGINT&gt;&gt;, level STRING, message STRING, client_timestamp BIGINT)</div><div class="line">ROW FORMAT SERDE &apos;org.openx.data.jsonserde.JsonSerDe&apos;</div><div class="line">STORED AS TEXTFILE</div><div class="line">LOCATION &apos;/flume/events/birdben.ad.view_ad&apos;;</div><div class="line"></div><div class="line"># 在Hive中查询birdben_ad_click_ad表中的数据</div><div class="line">hive&gt; select * from birdben_ad_click_ad;OK[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63146996042563584&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475912715001&#125;]	info	logs	NULL[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63148812297830402&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475913845544&#125;]	info	logs	NULL[&#123;&quot;name&quot;:&quot;birdben.ad.click_ad&quot;,&quot;rpid&quot;:&quot;63152468644593666&quot;,&quot;bid&quot;:&quot;0&quot;,&quot;uid&quot;:&quot;0&quot;,&quot;did&quot;:&quot;0&quot;,&quot;duid&quot;:&quot;0&quot;,&quot;hbuid&quot;:null,&quot;ua&quot;:&quot;&quot;,&quot;device_id&quot;:&quot;&quot;,&quot;ip&quot;:null,&quot;server_timestamp&quot;:1475915093792&#125;]	info	logs	NULLTime taken: 0.519 seconds, Fetched: 3 row(s)</div><div class="line"></div><div class="line"># 在Hive中查询birdben_ad_click_ad表中的数据总数</div><div class="line">hive&gt; select count(*) from birdben_ad_click_ad;Query ID = yunyu_20161011234624_fbd62672-91ee-4497-8ea1-f5a1e765a147Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes):  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers:  set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers:  set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1476004456759_0008, Tracking URL = http://hadoop1:8088/proxy/application_1476004456759_0008/Kill Command = /data/hadoop-2.7.1/bin/hadoop job  -kill job_1476004456759_0008Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12016-10-11 23:46:33,190 Stage-1 map = 0%,  reduce = 0%2016-10-11 23:46:39,554 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.95 sec2016-10-11 23:46:48,909 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.56 secMapReduce Total cumulative CPU time: 2 seconds 560 msecEnded Job = job_1476004456759_0008MapReduce Jobs Launched: Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.56 sec   HDFS Read: 8849 HDFS Write: 2 SUCCESSTotal MapReduce CPU Time Spent: 2 seconds 560 msecOK3Time taken: 25.73 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure>
<p>到此为止，我们上面说的两个问题都得到了解决，后续还会继续调优。</p>
<p>参考文章：</p>
<ul>
<li><a href="http://blog.csdn.net/ahjzgyxy/article/details/44423025" target="_blank" rel="external">http://blog.csdn.net/ahjzgyxy/article/details/44423025</a></li>
<li><a href="http://blog.csdn.net/xiao_jun_0820/article/details/38333171" target="_blank" rel="external">http://blog.csdn.net/xiao_jun_0820/article/details/38333171</a></li>
<li><a href="http://lxw1234.com/archives/2015/11/543.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/11/543.htm</a></li>
<li><a href="http://lxw1234.com/archives/2015/11/545.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/11/545.htm</a></li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/">Hive</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Log/">Log</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 birdben
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>



<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82900755-1', 'auto');
  ga('send', 'pageview');

</script>
<!-- End Google Analytics -->




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>