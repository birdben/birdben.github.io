<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>Hadoop学习（一）Hadoop完全分布式环境搭建 | birdben</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="今天学习的信息量有点大收获不少，一时之间不知道从哪里开始写，希望尽量把我今天学习到的东西记录下来，因为内容太多可能会分几篇记录。其实之前有写过一篇用Docker搭建Hadoop环境的文章，当时其实搭建的是单机伪分布式的环境，今天这里搭建的是Hadoop完全分布式环境。今天又看了许多文章，对于Hadoop的体系架构又有了一定新的理解，包括1.x版本和2.x版本的不同。
Hadoop集群环境我这里使用">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop学习（一）Hadoop完全分布式环境搭建">
<meta property="og:url" content="https://github.com/birdben/2016/09/10/Hadoop/Hadoop学习（一）Hadoop完全分布式环境搭建/index.html">
<meta property="og:site_name" content="birdben">
<meta property="og:description" content="今天学习的信息量有点大收获不少，一时之间不知道从哪里开始写，希望尽量把我今天学习到的东西记录下来，因为内容太多可能会分几篇记录。其实之前有写过一篇用Docker搭建Hadoop环境的文章，当时其实搭建的是单机伪分布式的环境，今天这里搭建的是Hadoop完全分布式环境。今天又看了许多文章，对于Hadoop的体系架构又有了一定新的理解，包括1.x版本和2.x版本的不同。
Hadoop集群环境我这里使用">
<meta property="og:image" content="http://img.blog.csdn.net/20160910184234348?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://img.blog.csdn.net/20160910184156566?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://img.blog.csdn.net/20160910184251708?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:updated_time" content="2016-10-18T18:40:25.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hadoop学习（一）Hadoop完全分布式环境搭建">
<meta name="twitter:description" content="今天学习的信息量有点大收获不少，一时之间不知道从哪里开始写，希望尽量把我今天学习到的东西记录下来，因为内容太多可能会分几篇记录。其实之前有写过一篇用Docker搭建Hadoop环境的文章，当时其实搭建的是单机伪分布式的环境，今天这里搭建的是Hadoop完全分布式环境。今天又看了许多文章，对于Hadoop的体系架构又有了一定新的理解，包括1.x版本和2.x版本的不同。
Hadoop集群环境我这里使用">
<meta name="twitter:image" content="http://img.blog.csdn.net/20160910184234348?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
  
    <link rel="alternative" href="/atom.xml" title="birdben" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
<script type="text/javascript">
var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1260188951'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s4.cnzz.com/z_stat.php%3Fid%3D1260188951' type='text/javascript'%3E%3C/script%3E"));
</script>

</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/images/logo.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">birdben</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						<li>Links</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Akka/" style="font-size: 11px;">Akka</a> <a href="/tags/Dockerfile/" style="font-size: 20px;">Dockerfile</a> <a href="/tags/Docker命令/" style="font-size: 19px;">Docker命令</a> <a href="/tags/Docker环境/" style="font-size: 13px;">Docker环境</a> <a href="/tags/ELK/" style="font-size: 11px;">ELK</a> <a href="/tags/ElasticSearch/" style="font-size: 11px;">ElasticSearch</a> <a href="/tags/Flume/" style="font-size: 17px;">Flume</a> <a href="/tags/Git命令/" style="font-size: 13px;">Git命令</a> <a href="/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/tags/HDFS/" style="font-size: 18px;">HDFS</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Hadoop原理架构体系/" style="font-size: 14px;">Hadoop原理架构体系</a> <a href="/tags/Hive/" style="font-size: 16px;">Hive</a> <a href="/tags/Jenkins环境/" style="font-size: 10px;">Jenkins环境</a> <a href="/tags/Kafka/" style="font-size: 13px;">Kafka</a> <a href="/tags/Kibana/" style="font-size: 12px;">Kibana</a> <a href="/tags/Linux命令/" style="font-size: 12px;">Linux命令</a> <a href="/tags/MapReduce/" style="font-size: 12px;">MapReduce</a> <a href="/tags/Maven配置/" style="font-size: 12px;">Maven配置</a> <a href="/tags/MongoDB/" style="font-size: 12px;">MongoDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Nginx/" style="font-size: 10px;">Nginx</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Spring/" style="font-size: 11px;">Spring</a> <a href="/tags/Storm/" style="font-size: 13px;">Storm</a> <a href="/tags/Zookeeper/" style="font-size: 13px;">Zookeeper</a> <a href="/tags/其他/" style="font-size: 10px;">其他</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://blog.csdn.net/birdben">我的CSDN的博客</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">birdben</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="/images/logo.png" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">birdben</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/birdben" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo">weibo</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap"><article id="post-Hadoop/Hadoop学习（一）Hadoop完全分布式环境搭建" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/10/Hadoop/Hadoop学习（一）Hadoop完全分布式环境搭建/" class="article-date">
  	<time datetime="2016-09-10T08:09:30.000Z" itemprop="datePublished">2016-09-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Hadoop学习（一）Hadoop完全分布式环境搭建
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop原理架构体系/">Hadoop原理架构体系</a></li></ul>
	</div>

        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
	</div>


        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天学习的信息量有点大收获不少，一时之间不知道从哪里开始写，希望尽量把我今天学习到的东西记录下来，因为内容太多可能会分几篇记录。其实之前有写过一篇用Docker搭建Hadoop环境的文章，当时其实搭建的是单机伪分布式的环境，今天这里搭建的是Hadoop完全分布式环境。今天又看了许多文章，对于Hadoop的体系架构又有了一定新的理解，包括1.x版本和2.x版本的不同。</p>
<h3 id="Hadoop集群环境"><a href="#Hadoop集群环境" class="headerlink" title="Hadoop集群环境"></a>Hadoop集群环境</h3><p>我这里使用的三台虚拟机，每台虚拟机有自己的独立IP</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">192.168.1.119   hadoop1192.168.1.150   hadoop2192.168.1.149   hadoop3</div></pre></td></tr></table></figure>
<p>相关环境信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">操作系统: Ubuntu 14.04.5 LTS</div><div class="line">JDK版本: 1.7.0_79</div><div class="line">Hadoop版本: 2.7.1</div></pre></td></tr></table></figure>
<h3 id="JDK安装"><a href="#JDK安装" class="headerlink" title="JDK安装"></a>JDK安装</h3><p>省略</p>
<h3 id="Hadoop安装"><a href="#Hadoop安装" class="headerlink" title="Hadoop安装"></a>Hadoop安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 下载Hadoop安装包</div><div class="line">$ curl -O http://mirrors.cnnic.cn/apache/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz</div><div class="line"></div><div class="line"># 解压Hadoop压缩包</div><div class="line">$ tar -zxvf hadoop-2.7.1.tar.gz</div></pre></td></tr></table></figure>
<h3 id="Hadoop集群配置"><a href="#Hadoop集群配置" class="headerlink" title="Hadoop集群配置"></a>Hadoop集群配置</h3><p>注意：以下配置请根据自己的实际环境修改</p>
<h5 id="配置环境变量-etc-profile"><a href="#配置环境变量-etc-profile" class="headerlink" title="配置环境变量/etc/profile"></a>配置环境变量/etc/profile</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">JAVA_HOME=/usr/local/javaexport JAVA_HOMEHADOOP_HOME=/usr/local/hadoopexport HADOOP_HOMEPATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHexport PATH</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-hadoop-env-sh，添加以下内容"><a href="#配置HADOOP-HOME-etc-hadoop-hadoop-env-sh，添加以下内容" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/hadoop-env.sh，添加以下内容"></a>配置HADOOP_HOME/etc/hadoop/hadoop-env.sh，添加以下内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export JAVA_HOME=/usr/local/java</div><div class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-yarn-env-sh，添加以下内容"><a href="#配置HADOOP-HOME-etc-hadoop-yarn-env-sh，添加以下内容" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/yarn-env.sh，添加以下内容"></a>配置HADOOP_HOME/etc/hadoop/yarn-env.sh，添加以下内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export JAVA_HOME=/usr/local/java</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-core-site-xml"><a href="#配置HADOOP-HOME-etc-hadoop-core-site-xml" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/core-site.xml"></a>配置HADOOP_HOME/etc/hadoop/core-site.xml</h5><p>这里我使用Hadoop1这台虚拟机作为NameNode节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">    &lt;value&gt;hdfs://Hadoop1:9000&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-hdfs-site-xml"><a href="#配置HADOOP-HOME-etc-hadoop-hdfs-site-xml" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/hdfs-site.xml"></a>配置HADOOP_HOME/etc/hadoop/hdfs-site.xml</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;!-- 分布式文件系统数据块复制数，我们这里是Hadoop2和Hadoop3两个节点 --&gt;  &lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;2&lt;/value&gt;  &lt;/property&gt;</div><div class="line">  &lt;!-- DFS namenode存放name table的目录 --&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    &lt;value&gt;file:/data/hdfs/name&lt;/value&gt;  &lt;/property&gt;</div><div class="line">  &lt;!-- DFS datanode存放数据block的目录 --&gt;  &lt;property&gt;    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;    &lt;value&gt;file:/data/hdfs/data&lt;/value&gt;  &lt;/property&gt;</div><div class="line">  &lt;!-- SecondaryNameNode的端口号，默认端口号是50090 --&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;    &lt;value&gt;hadoop1:50090&lt;/value&gt;  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-mapred-site-xml-默认不存在，需要自建"><a href="#配置HADOOP-HOME-etc-hadoop-mapred-site-xml-默认不存在，需要自建" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/mapred-site.xml,默认不存在，需要自建"></a>配置HADOOP_HOME/etc/hadoop/mapred-site.xml,默认不存在，需要自建</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;!-- 第三方MapReduce框架，我们这里使用的yarn --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</div><div class="line">    &lt;value&gt;yarn&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- MapReduce JobHistory Server的IPC通信地址，默认端口号是10020 --&gt;</div><div class="line">  &lt;property&gt;     &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;     &lt;value&gt;hadoop1:10020&lt;/value&gt;  &lt;/property&gt;</div><div class="line">  &lt;!-- MapReduce JobHistory Server的Web服务器访问地址，默认端口号是19888 --&gt;  &lt;property&gt;     &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;     &lt;value&gt;hadoop1:19888&lt;/value&gt;  &lt;/property&gt;</div><div class="line">  &lt;!-- MapReduce已完成作业信息 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;</div><div class="line">    &lt;value&gt;/data/history/done&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;!-- MapReduce正在运行作业信息 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;</div><div class="line">    &lt;value&gt;/data/history/done_intermediate&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h5 id="配置HADOOP-HOME-etc-hadoop-yarn-site-xml"><a href="#配置HADOOP-HOME-etc-hadoop-yarn-site-xml" class="headerlink" title="配置HADOOP_HOME/etc/hadoop/yarn-site.xml"></a>配置HADOOP_HOME/etc/hadoop/yarn-site.xml</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;!-- 为MapReduce设置洗牌服务 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</div><div class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;  &lt;/property&gt;</div><div class="line">  &lt;!-- NodeManager与ResourceManager通信的接口地址，默认端口是8032 --&gt;</div><div class="line">  &lt;property&gt;    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;    &lt;value&gt;hadoop1:8032&lt;/value&gt;  &lt;/property&gt;</div><div class="line">  &lt;!-- NodeManger需要知道ResourceManager主机的scheduler调度服务接口地址，默认端口是8030 --&gt;  &lt;property&gt;    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;    &lt;value&gt;hadoop1:8030&lt;/value&gt;  &lt;/property&gt;</div><div class="line">  &lt;!-- NodeManager需要向ResourceManager报告任务运行状态供Resouce跟踪，因此NodeManager节点主机需要知道ResourceManager主机的tracker接口地址，默认端口是8031 --&gt;  &lt;property&gt;    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;    &lt;value&gt;hadoop1:8031&lt;/value&gt;  &lt;/property&gt;</div><div class="line">  &lt;!-- resourcemanager.admin，默认端口是8033 --&gt;  &lt;property&gt;    &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;    &lt;value&gt;hadoop1:8033&lt;/value&gt;  &lt;/property&gt;</div><div class="line">  &lt;!-- 各个task的资源调度及运行状况通过通过该web界面访问，默认端口是8088 --&gt;  &lt;property&gt;    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;    &lt;value&gt;hadoop1:8088&lt;/value&gt;  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h5 id="配置slaves节点，修改HADOOP-HOME-etc-hadoop-slaves"><a href="#配置slaves节点，修改HADOOP-HOME-etc-hadoop-slaves" class="headerlink" title="配置slaves节点，修改HADOOP_HOME/etc/hadoop/slaves"></a>配置slaves节点，修改HADOOP_HOME/etc/hadoop/slaves</h5><p>如果slaves配置中也添加Hadoop1节点，那么Hadoop1节点就既是namenode，又是datanode，这里没有这么配置，所以Hadoop1节点只是namenode，所以下面启动Hadoop1的服务之后，jps查看只有namenode服务器而没有datanode服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Hadoop2</div><div class="line">Hadoop3</div></pre></td></tr></table></figure>
<h5 id="配置-etc-hostname"><a href="#配置-etc-hostname" class="headerlink" title="配置/etc/hostname"></a>配置/etc/hostname</h5><p>Hadoop1,2,3分别修改自己的/etc/hostname文件，如果这里不修改的话，后面使用Hive做离线查询会遇到问题，具体问题请参考《Hive学习（二）使用Hive进行离线分析日志》</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop1</div></pre></td></tr></table></figure>
<h5 id="配置主机名-etc-hosts"><a href="#配置主机名-etc-hosts" class="headerlink" title="配置主机名/etc/hosts"></a>配置主机名/etc/hosts</h5><p>这里Hadoop1是namenode，Hadoop2和Hadoop3是datanode</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">192.168.1.119   hadoop1192.168.1.150   hadoop2192.168.1.149   hadoop3</div></pre></td></tr></table></figure>
<h5 id="配置SSH免密码登录"><a href="#配置SSH免密码登录" class="headerlink" title="配置SSH免密码登录"></a>配置SSH免密码登录</h5><p>在Hadoop1节点中生成新的SSH Key，并且将新生成的SSH Key添加到Hadoop1，2，3的authorized_keys免密码访问的配置中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># 创建authorized_keys文件</div><div class="line">$ vi ~/.ssh/authorized_keys</div><div class="line"></div><div class="line"># 注意：这里authorized_keys文件的权限设置为600。（这点很重要，网没有设置600权限会导致登录失败）因为我这里用的root账户没有这个问题，但是如果用自己创建的其他hadoop账户，不设置600权限就会导致登录失败</div><div class="line"></div><div class="line"># Hadoop1中执行</div><div class="line">$ ssh-keygen -t dsa -P &apos;&apos; -f ~/.ssh/id_dsa</div><div class="line"># 将Hadoop1中的公钥复制进去</div><div class="line">$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</div><div class="line"></div><div class="line"># Hadoop2，3中执行</div><div class="line">$ scp root@Hadoop1:~/.ssh/id_dsa.pub  ~/.ssh/master_dsa.pub</div><div class="line"># 将Hadoop2，3中的公钥复制进去</div><div class="line">$ cat ~/.ssh/master_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</div><div class="line"></div><div class="line"># 在Hadoop1中测试是否可以免密码登录Hadoop1，2，3（第一次应该只需要输入yes）</div><div class="line">$ ssh root@Hadoop1</div><div class="line">$ ssh root@Hadoop2</div><div class="line">$ ssh root@Hadoop3</div></pre></td></tr></table></figure>
<h5 id="配置好Hadoop1之后，将Hadoop1的配置copy到Hadoop2和Hadoop3"><a href="#配置好Hadoop1之后，将Hadoop1的配置copy到Hadoop2和Hadoop3" class="headerlink" title="配置好Hadoop1之后，将Hadoop1的配置copy到Hadoop2和Hadoop3"></a>配置好Hadoop1之后，将Hadoop1的配置copy到Hadoop2和Hadoop3</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 在Hadoop1中执行</div><div class="line">$ scp -r /data/hadoop-2.7.1 root@Hadoop2:/data/</div><div class="line">$ scp -r /data/hadoop-2.7.1 root@Hadoop3:/data/</div></pre></td></tr></table></figure>
<h5 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"># 初始化namenode</div><div class="line">$ ./bin/hdfs namenode -format</div><div class="line"></div><div class="line"># 初始化好namenode后，hadoop会自动建好对应hdfs-site.xml的namenode配置的文件路径</div><div class="line">$ ll /data/hdfs/name/current/total 24drwxrwxr-x 2 yunyu yunyu 4096 Sep 10 18:07 ./drwxrwxr-x 3 yunyu yunyu 4096 Sep 10 18:07 ../-rw-rw-r-- 1 yunyu yunyu  352 Sep 10 18:07 fsimage_0000000000000000000-rw-rw-r-- 1 yunyu yunyu   62 Sep 10 18:07 fsimage_0000000000000000000.md5-rw-rw-r-- 1 yunyu yunyu    2 Sep 10 18:07 seen_txid-rw-rw-r-- 1 yunyu yunyu  202 Sep 10 18:07 VERSION</div><div class="line"></div><div class="line"># 启动hdfs服务</div><div class="line">$ ./sbin/start-dfs.sh</div><div class="line">Starting namenodes on [hadoop1]hadoop1: starting namenode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-namenode-ubuntu.outhadoop2: starting datanode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-datanode-ubuntu.outhadoop3: starting datanode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-datanode-ubuntu.outStarting secondary namenodes [hadoop1]hadoop1: starting secondarynamenode, logging to /data/hadoop-2.7.1/logs/hadoop-yunyu-secondarynamenode-ubuntu.out</div><div class="line"></div><div class="line"># 使用jps检查启动的服务，可以看到NameNode和SecondaryNameNode已经启动</div><div class="line">$ jps20379 SecondaryNameNode20570 Jps20106 NameNode</div><div class="line"></div><div class="line"># 这时候在Hadoop2和Hadoop3节点上使用jps查看，DataNode已经启动</div><div class="line">$ jps16392 Jps16024 DataNode</div><div class="line"></div><div class="line"># 在Hadoop2和Hadoop3节点上，也会自动建好对应hdfs-site.xml的datanode配置的文件路径</div><div class="line">$ ll /data/hdfs/data/current/total 16drwxrwxr-x 3 yunyu yunyu 4096 Sep 10 18:10 ./drwx------ 3 yunyu yunyu 4096 Sep 10 18:10 ../drwx------ 4 yunyu yunyu 4096 Sep 10 18:10 BP-1965589257-127.0.1.1-1473502067891/-rw-rw-r-- 1 yunyu yunyu  229 Sep 10 18:10 VERSION</div><div class="line"></div><div class="line"># 启动yarn服务</div><div class="line">$ ./sbin/start-yarn.sh</div><div class="line">starting yarn daemonsstarting resourcemanager, logging to /data/hadoop-2.7.1/logs/yarn-yunyu-resourcemanager-ubuntu.outhadoop3: starting nodemanager, logging to /data/hadoop-2.7.1/logs/yarn-yunyu-nodemanager-ubuntu.outhadoop2: starting nodemanager, logging to /data/hadoop-2.7.1/logs/yarn-yunyu-nodemanager-ubuntu.out</div><div class="line"></div><div class="line"># 使用jps检查启动的服务，可以看到ResourceManager已经启动</div><div class="line">$ jps21653 Jps20379 SecondaryNameNode20106 NameNode21310 ResourceManager</div><div class="line"></div><div class="line"># 这时候在Hadoop2和Hadoop3节点上使用jps查看，NodeManager已经启动</div><div class="line">$ jps16946 NodeManager17235 Jps16024 DataNode</div><div class="line"></div><div class="line"># 启动jobhistory服务，默认jobhistory在使用start-all.sh是不启动的，所以即使使用start-all.sh也要手动启动jobhistory服务</div><div class="line">$ ./sbin/mr-jobhistory-daemon.sh start historyserver</div><div class="line">starting historyserver, logging to /data/hadoop-2.7.1/logs/mapred-yunyu-historyserver-ubuntu.out</div><div class="line"></div><div class="line"># 使用jps检查启动的服务，可以看到JobHistoryServer已经启动</div><div class="line">$ jps21937 Jps20379 SecondaryNameNode20106 NameNode21863 JobHistoryServer21310 ResourceManager</div></pre></td></tr></table></figure>
<p>注意：使用start-all.sh启动已经不再被推荐使用，所以这里使用的是Hadoop推荐的分开启动，分别启动start-dfs.sh和start-yarn.sh，所以看一些比较就的Hadoop版本安装的文章可能会用start-all.sh启动</p>
<h5 id="停止服务"><a href="#停止服务" class="headerlink" title="停止服务"></a>停止服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 停止hdfs服务</div><div class="line">$ ./sbin/stop-dfs.sh</div><div class="line"></div><div class="line"># 停止yarn服务</div><div class="line">$ ./sbin/stop-yarn.sh</div><div class="line"></div><div class="line"># 停止jobhistory服务</div><div class="line">$ ./sbin/mr-jobhistory-daemon.sh stop historyserver</div></pre></td></tr></table></figure>
<h5 id="验证Hadoop集群的Web服务"><a href="#验证Hadoop集群的Web服务" class="headerlink" title="验证Hadoop集群的Web服务"></a>验证Hadoop集群的Web服务</h5><ul>
<li><p>验证NameNode的Web服务能访问，浏览器访问<a href="http://192.168.1.119:50070" target="_blank" rel="external">http://192.168.1.119:50070</a><br><img src="http://img.blog.csdn.net/20160910184234348?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
</li>
<li><p>验证ResourceManager的Web服务能访问，浏览器访问<a href="http://192.168.1.119:8088" target="_blank" rel="external">http://192.168.1.119:8088</a><br><img src="http://img.blog.csdn.net/20160910184156566?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
</li>
<li><p>验证MapReduce JobHistory Server的Web服务能访问，浏览器访问<a href="http://192.168.1.119:19888" target="_blank" rel="external">http://192.168.1.119:19888</a><br><img src="http://img.blog.csdn.net/20160910184251708?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
</li>
</ul>
<h5 id="验证HDFS文件系统"><a href="#验证HDFS文件系统" class="headerlink" title="验证HDFS文件系统"></a>验证HDFS文件系统</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 查看根目录下的文件</div><div class="line">$ hdfs dfs -ls /Found 1 itemsdrwxrwx---   - yunyu supergroup          0 2016-09-10 03:15 /data</div><div class="line"></div><div class="line"># 创建temp目录</div><div class="line">$ hdfs dfs -mkdir /temp</div><div class="line"></div><div class="line"># 再次查看根目录下的文件，可以看到temp目录</div><div class="line">$ hdfs dfs -ls /Found 2 itemsdrwxrwx---   - yunyu supergroup          0 2016-09-10 03:15 /datadrwxr-xr-x   - yunyu supergroup          0 2016-09-10 03:45 /temp</div><div class="line"></div><div class="line"># 可以查看之前mapred-site.xml中配置的mapreduce作业执行中的目录和作业已完成的目录</div><div class="line">$ hdfs dfs -ls /data/history/Found 2 itemsdrwxrwx---   - yunyu supergroup          0 2016-09-10 03:15 /data/history/donedrwxrwxrwt   - yunyu supergroup          0 2016-09-10 03:15 /data/history/done_intermediate</div></pre></td></tr></table></figure>
<h3 id="需要注意的地方"><a href="#需要注意的地方" class="headerlink" title="需要注意的地方"></a>需要注意的地方</h3><p>网上一些Hadoop集群安装相关文章中，有一部分还是Hadoop老版本的配置，所以有些迷惑，像JobTracker，TaskTracker这些概念是Hadoop老版本才有的，新版本中使用ResourceManager和NodeManager替代了他们。后续的章节会详细的介绍Hadoop的相关原理以及新老版本的区别。</p>
<h3 id="使用HDFS默认端口号8020配置"><a href="#使用HDFS默认端口号8020配置" class="headerlink" title="使用HDFS默认端口号8020配置"></a>使用HDFS默认端口号8020配置</h3><p>修改core-site.xml配置文件如下（即把端口号去掉）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://hadoop1&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p>启动HDFS服务之后，分别在Hadoop1，2，3三台服务器上查看8020端口，发现HDFS默认使用的是8020端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 启动HDFS服务</div><div class="line">$ ./sbin/start-dfs.sh</div><div class="line"></div><div class="line"># Hadoop1中查看8020端口</div><div class="line">$ lsof -i:8020COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAMEjava    5112 yunyu  197u  IPv4  26041      0t0  TCP hadoop1:8020 (LISTEN)java    5112 yunyu  207u  IPv4  27568      0t0  TCP hadoop1:8020-&gt;hadoop2:34867 (ESTABLISHED)java    5112 yunyu  208u  IPv4  26096      0t0  TCP hadoop1:8020-&gt;hadoop3:59852 (ESTABLISHED)java    5112 yunyu  209u  IPv4  29792      0t0  TCP hadoop1:8020-&gt;hadoop1:45542 (ESTABLISHED)java    5383 yunyu  196u  IPv4  28826      0t0  TCP hadoop1:45542-&gt;hadoop1:8020 (ESTABLISHED)</div><div class="line"></div><div class="line"># Hadoop2中查看8020端口</div><div class="line">$ lsof -i:8020COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAMEjava    4609 yunyu  234u  IPv4  24013      0t0  TCP hadoop2:34867-&gt;hadoop1:8020 (ESTABLISHED)</div><div class="line"></div><div class="line"># Hadoop3中查看8020端口</div><div class="line">$ lsof -i:8020COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAMEjava    4452 yunyu  234u  IPv4  23413      0t0  TCP hadoop3:59852-&gt;hadoop1:8020 (ESTABLISHED)</div></pre></td></tr></table></figure>
<p>访问HDFS集群的方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 访问本机的HDFS集群</div><div class="line">hdfs dfs -ls /</div><div class="line"></div><div class="line"># 可以指定host和port访问远程的HDFS集群（这里使用hostname和port访问本地集群）</div><div class="line">hdfs dfs -ls hdfs://Hadoop1:8020/</div><div class="line"></div><div class="line"># 如果使用的默认端口号8020，也可以不指定端口号访问</div><div class="line">hdfs dfs -ls hdfs://Hadoop1/</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<ul>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/ClusterSetup.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/ClusterSetup.html</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/core-default.xml" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/core-default.xml</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#secondarynamenode" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#secondarynamenode</a></li>
<li><a href="http://www.cnblogs.com/luogankun/p/4019303.html" target="_blank" rel="external">http://www.cnblogs.com/luogankun/p/4019303.html</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/#_3.1_hadoop_0.23.0" target="_blank" rel="external">http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/#_3.1_hadoop_0.23.0</a></li>
<li><a href="http://blog.chinaunix.net/uid-25266990-id-3900239.html" target="_blank" rel="external">http://blog.chinaunix.net/uid-25266990-id-3900239.html</a></li>
<li><a href="http://blog.csdn.net/jxnu_xiaobing/article/details/46931693" target="_blank" rel="external">http://blog.csdn.net/jxnu_xiaobing/article/details/46931693</a></li>
<li><a href="http://www.cnblogs.com/liuling/archive/2013/06/16/2013-6-16-01.html" target="_blank" rel="external">http://www.cnblogs.com/liuling/archive/2013/06/16/2013-6-16-01.html</a></li>
<li><a href="http://www.cnblogs.com/luogankun/p/4019303.html" target="_blank" rel="external">http://www.cnblogs.com/luogankun/p/4019303.html</a></li>
<li><a href="http://jacoxu.com/?p=961" target="_blank" rel="external">http://jacoxu.com/?p=961</a></li>
</ul>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/09/11/Git/Git的SSH-Key用法/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption"><</strong>
      <div class="article-nav-title">
        
          Git的SSH-Key用法
        
      </div>
    </a>
  
  
    <a href="/2016/09/10/Hadoop/Hadoop学习（四）MapReduce清洗数据实例/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Hadoop学习（四）MapReduce清洗数据实例</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>








</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 birdben
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>



<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82900755-1', 'auto');
  ga('send', 'pageview');

</script>
<!-- End Google Analytics -->




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>